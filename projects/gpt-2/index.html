<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><style>:root{--accent-color:#FF4D4D}</style><title>gpt-2</title><meta name=description content="Software Developer & Student"><meta name=keywords content><meta property="og:url" content="https://jr0.org/projects/gpt-2/"><meta property="og:type" content="website"><meta property="og:title" content="gpt-2"><meta property="og:description" content="Software Developer & Student"><meta property="og:image" content="/images/35516367.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="gpt-2"><meta name=twitter:description content="Software Developer & Student"><meta property="twitter:domain" content="https://jr0.org/projects/gpt-2/"><meta property="twitter:url" content="https://jr0.org/projects/gpt-2/"><meta name=twitter:image content="/images/35516367.jpg"><link rel=canonical href=https://jr0.org/projects/gpt-2/><link rel=stylesheet type=text/css href=https://jr0.org/css/normalize.min.css media=print onload='this.media="all"'><link rel=stylesheet type=text/css href=https://jr0.org/css/main.css><link disabled id=dark-theme rel=stylesheet href=https://jr0.org/css/dark.css><script src=https://jr0.org/js/svg-injector.min.js></script>
<script src=https://jr0.org/js/feather-icons.min.js></script>
<script src=https://jr0.org/js/main.js></script></head><body><script type=text/javascript>setThemeByUserPref()</script><header class=header><nav class=header-nav><div class=avatar><a href=https://jr0.org><img src=https://jr0.org/images/35516367.jpg alt=avatar></a></div><div class=nav-title><a class=nav-brand href=https://jr0.org>JR0.org</a></div><div class=nav-links><div class=nav-link><a href=https://jr0.org/><span data-feather=home></span> Home</a></div><div class=nav-link><a href=https://jr0.org/projects/><span data-feather=code></span> Projects</a></div><div class=nav-link><a href=https://jr0.org/devlogs/><span data-feather=book></span> Devlog</a></div><div class=nav-link><a href=https://github.com/jakeroggenbuck/JakeRoggenbuck.github.io><span data-feather=github></span></a></div><span class=nav-icons-divider></span><div class="nav-link dark-theme-toggle"><a><span id=theme-toggle-icon data-feather=moon></span></a></div><div class=nav-link id=hamburger-menu-toggle><a><span data-feather=menu></span></a></div><ul class="nav-hamburger-list visibility-hidden"><li class=nav-item><a href=https://jr0.org/><span data-feather=home></span> Home</a></li><li class=nav-item><a href=https://jr0.org/projects/><span data-feather=code></span> Projects</a></li><li class=nav-item><a href=https://jr0.org/devlogs/><span data-feather=book></span> Devlog</a></li><li class=nav-item><a href=https://github.com/jakeroggenbuck/JakeRoggenbuck.github.io><span data-feather=github></span></a></li><li class="nav-item dark-theme-toggle"><a><span id=theme-toggle-icon data-feather=moon></span></a></li></ul></div></nav></header><main id=content><div class="post container"><div class=post-header-section><h1>gpt-2</h1></div><div class=post-content><p><h1 id=gpt-2>gpt-2</h1><p>Code from the paper <a href=https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf>&ldquo;Language Models are Unsupervised Multitask Learners&rdquo;</a>.</p><p>We have currently released small (117M parameter) and medium (345M parameter) versions of GPT-2. While we have not released the larger models, we have <a href=https://github.com/openai/gpt-2-output-dataset>released a dataset</a> for researchers to study their behaviors.</p><p>See more details in our <a href=https://blog.openai.com/better-language-models/>blog post</a>.</p><h2 id=usage>Usage</h2><p>This repository is meant to be a starting point for researchers and engineers to experiment with GPT-2.</p><h3 id=some-caveats>Some caveats</h3><ul><li>GPT-2 models&rsquo; robustness and worst case behaviors are not well-understood. As with any machine-learned model, carefully evaluate GPT-2 for your use case, especially if used without fine-tuning or in safety-critical applications where reliability is important.</li><li>The dataset our GPT-2 models were trained on contains many texts with <a href=https://twitter.com/TomerUllman/status/1101485289720242177>biases</a> and factual inaccuracies, and thus GPT-2 models are likely to be biased and inaccurate as well.</li><li>To avoid having samples mistaken as human-written, we recommend clearly labeling samples as synthetic before wide dissemination. Our models are often incoherent or inaccurate in subtle ways, which takes more than a quick read for a human to notice.</li></ul><h3 id=work-with-us>Work with us</h3><p>Please <a href=mailto:languagequestions@openai.com>let us know</a> if you’re doing interesting research with or working on applications of GPT-2! We’re especially interested in hearing from and potentially working with those who are studying</p><ul><li>Potential malicious use cases and defenses against them (e.g. the detectability of synthetic text)</li><li>The extent of problematic content (e.g. bias) being baked into the models and effective mitigations</li></ul><h2 id=development>Development</h2><p>See <a href=./DEVELOPERS.md>DEVELOPERS.md</a></p><h2 id=contributors>Contributors</h2><p>See <a href=./CONTRIBUTORS.md>CONTRIBUTORS.md</a></p><h2 id=citation>Citation</h2><p>Please use the following bibtex entry:</p><pre tabindex=0><code>@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}
</code></pre><h2 id=future-work>Future work</h2><p>We may release code for evaluating the models on various benchmarks.</p><p>We are still considering release of the larger models.</p><h2 id=license>License</h2><p><a href=./LICENSE>MIT</a></p></p></div></div></main><footer class=footer><span>&copy; 2022 Jake Roggenbuck</span></footer></body></html>