[{"content":" Favorite languages: C, Rust, Go, C++, Python # Topics of interest: Math, Space, Computer Science, Machine Learning # Recent Projects # JakeRoggenbuck/regolith A server-side TypeScript and JavaScript library immune to Regular Expression Denial of Service (ReDoS) attacks by using Rust and linear RegEx under the hood. Regolith has a linear worst case time complexity, compared to the default RegExp found in TypeScript and JavaScript, which has an exponential worst case. JavaScript 24 3 JakeRoggenbuck/auto-clock-speed A utility to check stats about your CPU, and auto regulate clock speeds to help with either performance or battery life. Rust 39 10 JakeRoggenbuck/RedoxQL 🦀 RedoxQL is an L-Store database written in Rust and Python 🚀 and 🥇 Fastest database speed in the class for milestone 2 (ECS165A Winter 2025) ⚡ Rust 3 0 More Projects # Open Source # I also contribute to open source, most notably: # onlook-dev/onlook (YC W25), pretzelai/pretzelai (YC W24), microsoft/RD-Agent, rocky-linux/rocky, rust-lang/miri, grafana/pyroscope, fastly/fastly-py, pathwaycom/pathway, argosopentech/argos-translate, python-mechanize/mechanize, and more\nContact # Email: me@jr0.org\nBug Reports: bug@jr0.org\nPublic GPG Key: 309BBC9\n","date":"19 July 2025","permalink":"/","section":"","summary":"Favorite languages: C, Rust, Go, C++, Python # Topics of interest: Math, Space, Computer Science, Machine Learning # Recent Projects # JakeRoggenbuck/regolith A server-side TypeScript and JavaScript library immune to Regular Expression Denial of Service (ReDoS) attacks by using Rust and linear RegEx under the hood.","title":""},{"content":"","date":"19 July 2025","permalink":"/tags/api/","section":"Tags","summary":"","title":"api"},{"content":"","date":"19 July 2025","permalink":"/tags/optimization/","section":"Tags","summary":"","title":"optimization"},{"content":"This is a overview of optimizations done to a course search app. We noticed that as our traffic increased and we added more computationally expensive features like vector search, our API got slower. After noticing this, we started running optimization tests and found places for improvements. Here are a few of those improvements.\nThe initial time of the API to send the 2.2MB of data was 5.209 seconds.\nOptimization 1. # This first optimization was to write it in Rust (opposed to Python), and the first thing I thought of was to actually load then entire JSON entire a global string to save load time later. We just return the JSON as RawJson, so no serialization has to be done [ 1]. In future, I would like to separate all of these changes to test them individually like use the Json type as a return and see how much different it is than RawJson.\nThis version is also running on a Linode VPS in US West opposed to running on Render.\nAt a high level, we are loading the file into memory as a string on startup and saving it globally. When the route is hit, we send by the data as is in a RawJson struct that doesn\u0026rsquo;t do serialization. This is faster than doing serialization with the Json struct. Further benchmarks can be done to show how much faster that is.\nstatic GLOBAL_STRING: OnceCell\u0026lt;String\u0026gt; = OnceCell::new(); #[get(\u0026#34;/courses\u0026#34;)] async fn get_all_courses() -\u0026gt; RawJson\u0026lt;String\u0026gt; { if let Some(s) = GLOBAL_STRING.get() { RawJson(s.to_string()) } else { RawJson(r#\u0026#34;{\u0026#34;message\u0026#34;: \u0026#34;Not initialized yet...\u0026#34;}\u0026#34;#.to_string()) } } async fn startup() { let json_string: String = fs::read_to_string(\u0026#34;./all_courses.json\u0026#34;).expect(\u0026#34;Should have read json file.\u0026#34;); GLOBAL_STRING.set(json_string).unwrap(); } There are also a few free optimizations with Rust like building with --release that give a big speed improvement. I benchmarked this speed improvement in RedoxQL.\nIt took 0.951 seconds to run. This results in a 5.48x speed up from the Python version.\nOptimization 2. # The next optimization is to use gzip to compress the file before it gets sent. Because it\u0026rsquo;s sending less data, we can expect some amount of time improvement. Furthermore, we can compress the data at startup and save it as a file and just load from a file and return it. I also optimized for the compression ratio compared to speed of compression. I previously found that using gzip -6 was optimal and thus picked it for this experiment. More experimentation can be done for this specific use case, but I think it\u0026rsquo;s rather optimal and any change won\u0026rsquo;t have significant difference, but that\u0026rsquo;s just speculation and needs to be tested.\nAt a high level, when the route gets hit, we are loading the gzipped version of our data into memory as a Vec\u0026lt;u8\u0026gt;, which is how you can represent bytes in Rust. We then use our Responder to return the bytes as well as setting the Header Content-Encoding to gzip so the service that fetches this data knows that it needs to be decompressed before it can be used. It\u0026rsquo;s important to test the time it takes to decompress the data as well as the time it takes to send the data, as both impact real world performance.\nstruct GzippedJson(Vec\u0026lt;u8\u0026gt;); #[rocket::async_trait] impl\u0026lt;\u0026#39;r\u0026gt; Responder\u0026lt;\u0026#39;r, \u0026#39;static\u0026gt; for GzippedJson { fn respond_to(self, _: \u0026amp;\u0026#39;r Request\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; rocket::response::Result\u0026lt;\u0026#39;static\u0026gt; { Response::build() .header(ContentType::JSON) .header(Header::new(\u0026#34;Content-Encoding\u0026#34;, \u0026#34;gzip\u0026#34;)) .sized_body(self.0.len(), Cursor::new(self.0)) .ok() } } #[get(\u0026#34;/courses-gzip-6\u0026#34;)] async fn get_all_courses_gzip_six() -\u0026gt; Result\u0026lt;GzippedJson, Status\u0026gt; { let mut file = File::open(\u0026#34;./all_courses.json.6.gz\u0026#34;) .await .map_err(|_| Status::InternalServerError)?; let mut contents = Vec::new(); file.read_to_end(\u0026amp;mut contents) .await .map_err(|_| Status::InternalServerError)?; Ok(GzippedJson(contents)) } It took 0.669 seconds to run. This resulted in a 7.79x speedup from the original, but we can do better!\nOptimization 3. # We can use the technique developed in Optimization 1. to preload the file and keep it in memory with a Global OnceCell. This saves us the time of fetching from disk and loading the data into memory.\nAt a high level, we are loading the gzipped data into memory as bytes at the startup of the server. When the route gets hit, we use our Responder from before to return the data. This loads the gzip at startup and not when the route gets hit.\nstatic GZIPPED_COURSE_SIX: OnceCell\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt; = OnceCell::new(); #[get(\u0026#34;/courses-gzip-6-preload\u0026#34;)] async fn get_all_courses_gzip_six_preload() -\u0026gt; Result\u0026lt;GzippedJson, Status\u0026gt; { match GZIPPED_COURSE_SIX.get() { Some(contents) =\u0026gt; Ok(GzippedJson(contents.clone())), None =\u0026gt; Err(Status::InternalServerError), } } async fn startup() { // -- snip -- let mut gzip_file = File::open(\u0026#34;./all_courses.json.6.gz\u0026#34;) .await .expect(\u0026#34;Failed to open gzip-6 file\u0026#34;); let mut gzip_contents = Vec::new(); gzip_file .read_to_end(\u0026amp;mut gzip_contents) .await .expect(\u0026#34;Failed to read gzip-6 file\u0026#34;); GZIPPED_COURSE_SIX.set(gzip_contents).unwrap(); } It took 0.651 to run. This results in an 8x speed improvement overall.\nAttempt 1. # I tried to improve the speed by not cloning the data, but this didn\u0026rsquo;t have any significant speedup.\n#[get(\u0026#34;/courses-gzip-6-preload-own\u0026#34;)] async fn get_all_courses_gzip_six_preload_own() -\u0026gt; Result\u0026lt;GzippedJson, Status\u0026gt; { match GZIPPED_COURSE_SIX.get() { Some(contents) =\u0026gt; Ok(GzippedJson(contents.to_owned())), None =\u0026gt; Err(Status::InternalServerError), } } This route swaps contents.clone() with contents.to_owned(), which should cause it speed up because it does not have to copy memory, but copying memory is so quick, it\u0026rsquo;s lost in the noise of the speed tests and is not significant here.\nNot all hypothetical optimizations show scientific results.\nAttempt 2. # Using compiler flags can optimize Rust and was effective in RedoxQL but when the performance of an API has a lot of noise due to sending the data over the wire, the optimizations have to be rather large to show significant and consistent results. I couldn\u0026rsquo;t find any speed improvements with better compiler flags, but there also wasn\u0026rsquo;t any regression.\n[profile.release] codegen-units = 1 lto = \u0026#34;fat\u0026#34; opt-level = 3 Conclusion # By changing how you solve a problem, you get get significant speed improvements. We improved the speed of fetching all courses by 8x. Going from 5.209 seconds to 0.651 seconds. 5.209 / 0.651 = 8.002.\nAppendix # Raw runtime using time curl and sending to /dev/null:\ntimes = { \u0026#34;Render Python (All Courses)\u0026#34;: 5.209, \u0026#34;Linode Rust (All Courses)\u0026#34;: 0.951, \u0026#34;Linode Rust (All Courses) GZIP-6\u0026#34;: 0.669, \u0026#34;Linode Rust (All Courses) GZIP-6 Preload\u0026#34;: 0.651, } The first time is the \u0026ldquo;before\u0026rdquo; and each subsequent time is each optimization applied.\n","date":"19 July 2025","permalink":"/posts/optimizing-course-api/","section":"Posts","summary":"This is a overview of optimizations done to a course search app.","title":"Optimizing Course Search by 8x"},{"content":"","date":"19 July 2025","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"19 July 2025","permalink":"/tags/rust/","section":"Tags","summary":"","title":"rust"},{"content":"","date":"19 July 2025","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"This is a list of resources I used to have on my website, or things that do not have any specific page.\nDev Logs # You can still find my Dev Logs. However, I no longer update these.\nSTPRQ - 02/24/2025 # Subjective Test for Pull Request Quality\nCircular Development # Circular Development ","date":"17 July 2025","permalink":"/archive/","section":"","summary":"This is a list of resources I used to have on my website, or things that do not have any specific page.","title":"Archive"},{"content":"","date":"12 July 2025","permalink":"/tags/backend/","section":"Tags","summary":"","title":"backend"},{"content":"","date":"12 July 2025","permalink":"/tags/express/","section":"Tags","summary":"","title":"express"},{"content":"","date":"12 July 2025","permalink":"/tags/frontend/","section":"Tags","summary":"","title":"frontend"},{"content":"","date":"12 July 2025","permalink":"/tags/javascript/","section":"Tags","summary":"","title":"javascript"},{"content":"","date":"12 July 2025","permalink":"/tags/regex/","section":"Tags","summary":"","title":"regex"},{"content":"","date":"12 July 2025","permalink":"/tags/regolith/","section":"Tags","summary":"","title":"regolith"},{"content":"","date":"12 July 2025","permalink":"/tags/typescript/","section":"Tags","summary":"","title":"typescript"},{"content":" JakeRoggenbuck/regolith A server-side TypeScript and JavaScript library immune to Regular Expression Denial of Service (ReDoS) attacks by using Rust and linear RegEx under the hood. Regolith has a linear worst case time complexity, compared to the default RegExp found in TypeScript and JavaScript, which has an exponential worst case. JavaScript 24 3 This is a brief introduction to using Regex validation in an Express backend. We will be using the new Regex library called Regolith that prevents Regular Expression Denial of Service Attacks when creating Regex patterns. More information about this library is shared below or on the GitHub page.\nProject Setup # Start by creating a project in your terminal:\nmkdir express-example Navigate in to the newly created project:\ncd express-example Initialize the JavaScript Project:\nnpm init Fill in the details for the new project. You can also just press enter for every field for the default (which is fine).\nInstall Express and Regolith:\nnpm i express @regolithjs/regolith Edit the contents of package.json and change type: \u0026quot;commonjs\u0026quot; to type: \u0026quot;module\u0026quot; to allow us to use the module import syntax.\nIt should end up looking something like this:\n{ \u0026#34;name\u0026#34;: \u0026#34;express-example\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;ISC\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;module\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;echo \\\u0026#34;Error: no test specified\\\u0026#34; \u0026amp;\u0026amp; exit 1\u0026#34; }, \u0026#34;dependencies\u0026#34;: { \u0026#34;@regolithjs/regolith\u0026#34;: \u0026#34;^0.1.4\u0026#34;, \u0026#34;express\u0026#34;: \u0026#34;^5.1.0\u0026#34; } } Writing the code # Create a file that will contain all of our code called index.js in your favorite editor.\nOpen the file and then add the following code:\nimport express from \u0026#34;express\u0026#34;; console.log(express); Now when we run the code with node index.js, we should see the following:\n[Function: createApplication] { application: { init: [Function: init], defaultConfiguration: [Function: defaultConfiguration], // ... text: [Function: text], urlencoded: [Function: urlencoded] } This shows us that Express was imported and installed correctly.\nWe can now add some basic Express code to our index.js:\nimport express from \u0026#34;express\u0026#34;; import { Regolith } from \u0026#34;@regolithjs/regolith\u0026#34;; const app = express(); const port = 3000; console.log(\u0026#34;Show Regolith is installed correctly:\u0026#34;, Regolith); app.listen(port, () =\u0026gt; { console.log(`Example app listening on port ${port}`); }); Run the code with node index.js and we should see the server start up:\nShow Regolith is installed correctly. [Function: Regolith] Example app listening on port 3000 This also shows us that Regolith is installed correctly.\nNow we can create Regex patterns with Regolith that check if a provided input matches either the int pattern or the float pattern. Imagine we wanted to create a basic server that helps us validate different types of numbers.\nimport express from \u0026#34;express\u0026#34;; import { Regolith } from \u0026#34;@regolithjs/regolith\u0026#34;; const app = express(); const port = 3000; // Create Regolith Regex patterns const intPattern = new Regolith(\u0026#34;^\\\\d+$\u0026#34;); const floatPattern = new Regolith(\u0026#34;^\\\\d*\\\\.\\\\d+$\u0026#34;); app.listen(port, () =\u0026gt; { console.log(`Example app listening on port ${port}`); }); Now we can test to see if our new Regex patterns work:\nimport express from \u0026#34;express\u0026#34;; import { Regolith } from \u0026#34;@regolithjs/regolith\u0026#34;; const app = express(); const port = 3000; // Create Regolith Regex patterns const intPattern = new Regolith(\u0026#34;^\\\\d+$\u0026#34;); const floatPattern = new Regolith(\u0026#34;^\\\\d*\\\\.\\\\d+$\u0026#34;); console.log(\u0026#34;Should be true:\u0026#34;, intPattern.test(\u0026#34;10\u0026#34;)); console.log(\u0026#34;Should be true:\u0026#34;, floatPattern.test(\u0026#34;10.0\u0026#34;)); console.log(\u0026#34;Should be false:\u0026#34;, intPattern.test(\u0026#34;hello\u0026#34;)); console.log(\u0026#34;Should be false:\u0026#34;, floatPattern.test(\u0026#34;world\u0026#34;)); app.listen(port, () =\u0026gt; { console.log(`Example app listening on port ${port}`); }); We added some console.log statements that test different inputs.\nWhen we run this with node index.js, we should see true, true, false, false:\nShould be true: true Should be true: true Should be false: false Should be false: false Example app listening on port 3000 We can trade the logging code with a route, that will allow us to call our backend:\napp.get(\u0026#34;/check\u0026#34;, (req, res) =\u0026gt; { // Get the input value that we will test const value = req.query.value; // Run the test with Regolith pattern const isInt = intPattern.test(value); const isFloat = floatPattern.test(value); // Return out test output res.json({isInt, isFloat}); }); This will respond when we call the /check route when we give a value like so /check?value=10. When we validate that this works, we can run node index.js and then see the URL (http://localhost:3000/check?value=10) in our browser.\nThis is what the browser page should look like:\nThe full code after that should look like this:\nimport express from \u0026#34;express\u0026#34;; import { Regolith } from \u0026#34;@regolithjs/regolith\u0026#34;; const app = express(); const port = 3000; // Create Regolith Regex patterns const intPattern = new Regolith(\u0026#34;^\\\\d+$\u0026#34;); const floatPattern = new Regolith(\u0026#34;^\\\\d*\\\\.\\\\d+$\u0026#34;); app.get(\u0026#34;/check\u0026#34;, (req, res) =\u0026gt; { // Get the input value that we will test const value = req.query.value; // Run the test with Regolith pattern const isInt = intPattern.test(value); const isFloat = floatPattern.test(value); // Return out test output res.json({isInt, isFloat}); }); app.listen(port, () =\u0026gt; { console.log(`Example app listening on port ${port}`); }); We can add input validation to check that we provided the value URL parameter:\n// Make sure that we provide a `value` in the request if (!value) { return res.status(400).send(\u0026#34;Please provide a value query parameter\u0026#34;); } We can also include the input value in the final output:\nres.json({ value, isInt, isFloat, }); The final code should be:\nimport express from \u0026#34;express\u0026#34;; import { Regolith } from \u0026#34;@regolithjs/regolith\u0026#34;; const app = express(); const port = 3000; // Create Regolith Regex patterns const intPattern = new Regolith(\u0026#34;^\\\\d+$\u0026#34;); const floatPattern = new Regolith(\u0026#34;^\\\\d*\\\\.\\\\d+$\u0026#34;); app.get(\u0026#34;/check\u0026#34;, (req, res) =\u0026gt; { // Get the input value that we will test const value = req.query.value; // Make sure that we provide a `value` in the request if (!value) { return res.status(400).send(\u0026#34;Please provide a value query parameter\u0026#34;); } // Run the test with Regolith pattern const isInt = intPattern.test(value); const isFloat = floatPattern.test(value); // Return out test output res.json({ value, isInt, isFloat, }); }); app.listen(port, () =\u0026gt; { console.log(`Example app listening on port ${port}`); }); We can even test this using the curl command if you have that installed. If not, browser works fine as well.\nCheck with a float value\ncurl http://localhost:3000/check?value=10.2 Output from float test\n{\u0026#34;value\u0026#34;:\u0026#34;10.2\u0026#34;,\u0026#34;isInt\u0026#34;:false,\u0026#34;isFloat\u0026#34;:true} Check with an int value\ncurl http://localhost:3000/check?value=10 Output from int test\n{\u0026#34;value\u0026#34;:\u0026#34;10\u0026#34;,\u0026#34;isInt\u0026#34;:true,\u0026#34;isFloat\u0026#34;:false} What are ReDoS attacks? # Regular Expression Denial of Service (ReDoS) attacks occur when vulnerable Regex patterns are executed with specifically constructed inputs that result in an inefficient execution. This can be exploited to cause services to become unavailable because the services are stuck trying to compute the inefficient Regex.\nHow does Regolith prevent ReDoS attacks? # Regolith prevents ReDoS attacks by disallowing features that make Regex vulnerable to ReDoS attacks. Two of these features are backreferences and look-around, which aren\u0026rsquo;t available in Regolith. When they features are invoked in other Regex libraries, they can take exponentially long to execute, crashing your server and causing a denial of service. We want to prevent this, so we use Regolith, which has a linear worst case time.\nFeedback # If you have any feedback or questions, please add a comment here or ask in the Regolith discussion form.\n⭐ If you found this article valuable, consider supporting us by giving the Regolith GitHub a star.\n","date":"12 July 2025","permalink":"/posts/using-regex-with-express/","section":"Posts","summary":"JakeRoggenbuck/regolith A server-side TypeScript and JavaScript library immune to Regular Expression Denial of Service (ReDoS) attacks by using Rust and linear RegEx under the hood.","title":"Using Regex with Express"},{"content":" JakeRoggenbuck/regolith A server-side TypeScript and JavaScript library immune to Regular Expression Denial of Service (ReDoS) attacks by using Rust and linear RegEx under the hood. Regolith has a linear worst case time complexity, compared to the default RegExp found in TypeScript and JavaScript, which has an exponential worst case. JavaScript 24 3 Description # A server-side TypeScript and JavaScript library immune to Regular Expression Denial of Service (ReDoS) attacks by using Rust and linear Regex under the hood. Regolith has a linear worst case time complexity, compared to the default RegExp found in TypeScript and JavaScript, which has an exponential worst case.\nInstall Regolith with NPM # npm i @regolithjs/regolith Try it out # import { Regolith } from \u0026#39;@regolithjs/regolith\u0026#39;; const pattern = new Regolith(\u0026#34;^\\\\d+$\u0026#34;); pattern.test(\u0026#34;12345\u0026#34;); // true pattern.test(\u0026#34;Hello\u0026#34;); // false What are ReDoS attacks? # Regular Expression Denial of Service (ReDoS) attacks occur when vulnerable Regex patterns are executed with specifically constructed inputs that result in an inefficient execution. This can be exploited to cause services to become unavailable because the services are stuck trying to compute the inefficient Regex.\nLinear vs Exponential Regex Libraries # This table shows popular languages and if their Regex library has a linear worst case or an exponential worst case. It also includes experimental results for how long execution took for a vulnerable Regex pattern that can be attacked with ReDoS and an input of size 30.\nDrop-in Replacement # Regolith attempts to be a drop-in replacement for RegExp and requires minimal (to no) changes to be used instead. The goal of Regolith is to allow developers to easily build software that is immune to ReDoS attacks.\nImpact # These vulnerabilities happen relatively often in popular libraries. It\u0026rsquo;s no one\u0026rsquo;s fault specifically, it just comes down to the fact that the language allows for these things to happen.\nA recent example of a ReDoS vulnerability is CVE-2025-5889 from brace-expansion. Again, this isn\u0026rsquo;t any fault of that project, it\u0026rsquo;s simply an issue with the language allowing this to happen. Measures can be put into place to reduce the risk of this, but it\u0026rsquo;s hard to spot and test for these issues.\nThe brace-expansion project is used by 42.5 million other projects on GitHub. Meaning if everyone were to patch their software (which the hopefully will), that would be 42.5 million pull requests, roughly 42.5 million build minutes, and probably more than 42 million engineering minutes as well. All of that for a single vulnerability, and that\u0026rsquo;s just a lower bound of effort spent on this if everyone were to keep their software patched.\nOther versions of brace-expansion had these patches backported to them, needing updates for versions 1, 2, 3, and the current version 4.\nHaving a library or project that is immune to these vulnerabilities would save this effort for each project that adopted it, and would save the whole package ecosystem that effort if widely adopted. Adoption of libraries is difficult, especially when they aren\u0026rsquo;t very flashy, but helping library maintainers and engineers not worry about ReDoS for one library, one project at a time, is our goal.\nTrade-off # The Rust Regex library purposefully excludes features that make Regex engines particularly vulnerable to ReDoS attacks. Those features are backreferences and look-around. Excluding those features allow Regex to guarantee linear time execution.\nSince Regolith uses Rust bindings to implement the Rust Regex library to achieve linear time worst case, this means that backreferences and look-around aren\u0026rsquo;t available in Regolith either.\nThis trade-off has proven to be worth it for the Rust community of libraries and projects.\nResults # Since ReDoS vulnerabilities are hard to spot, there are rather frequent CVEs that get submitted. Having a Regex library that has a linear worst case time would completely prevent all of these potential issues for downstream projects.\nClosing # View more info on GitHub\n","date":"3 July 2025","permalink":"/posts/preventing-redos-attacks-with-regolith/","section":"Posts","summary":"JakeRoggenbuck/regolith A server-side TypeScript and JavaScript library immune to Regular Expression Denial of Service (ReDoS) attacks by using Rust and linear RegEx under the hood.","title":"Preventing ReDoS Attacks with Regolith"},{"content":"","date":"3 July 2025","permalink":"/tags/redos/","section":"Tags","summary":"","title":"redos"},{"content":"","date":"2 July 2025","permalink":"/projects/","section":"Projects","summary":"","title":"Projects"},{"content":" Regolith Landing Page # A modern, responsive landing page for Regolith - a secure, Rust-backed regex engine for JavaScript and TypeScript.\nAbout Regolith # Regolith is a drop-in RegExp replacement that\u0026rsquo;s immune to ReDoS (Regular Expression Denial of Service) attacks, with guaranteed linear worst-case performance thanks to its Rust implementation.\nFeatures # 🛡️ ReDoS Protection - Complete immunity to Regular Expression Denial of Service attacks ⚡ Linear Performance - Guaranteed O(n+m) time complexity for all operations 🔄 Drop-in Replacement - Compatible API with JavaScript\u0026rsquo;s native RegExp 🦀 Rust-Powered - Built on Rust\u0026rsquo;s memory-safe foundation with WebAssembly bindings 📱 Responsive Design - Mobile-first, modern UI built with Tailwind CSS 🌙 Dark Mode - Optional dark theme support Tech Stack # Next.js 14 - React framework with App Router TypeScript - Type-safe development Tailwind CSS - Utility-first CSS framework Radix UI - Accessible component primitives Lucide React - Beautiful icons shadcn/ui - Re-usable component library Getting Started # Install dependencies:\nbun install Run the development server:\nbun run dev Open your browser: Navigate to http://localhost:8084\nProject Structure # ├── app/ │ ├── globals.css # Global styles and CSS variables │ └── page.tsx # Main landing page ├── components/ │ ├── ui/ # Reusable UI components │ ├── Header.tsx # Navigation header │ ├── Hero.tsx # Hero section │ ├── Overview.tsx # Problem/solution overview │ ├── Features.tsx # Feature highlights │ ├── Installation.tsx # Installation guide │ ├── Examples.tsx # Code examples │ ├── Contribute.tsx # Contribution section │ ├── Footer.tsx # Site footer │ └── CodeBlock.tsx # Syntax-highlighted code blocks ├── lib/ │ └── utils.ts # Utility functions └── public/ # Static assets Sections # The landing page includes the following sections:\nHero - Introduction and key value propositions Overview - Detailed explanation of the ReDoS problem and Regolith\u0026rsquo;s solution Features - Key benefits and capabilities Installation - Package manager installation commands Examples - Real-world code examples in JavaScript and TypeScript Contribute - Ways to contribute to the open source project Contributing # This is the landing page for the Regolith project. For contributing to the core Regolith library, visit:\nRegolith GitHub Repository Contributing Guidelines License # MIT License - see the LICENSE file for details.\n","date":"2 July 2025","permalink":"/projects/regolith-website/","section":"Projects","summary":"Regolith Landing Page # A modern, responsive landing page for Regolith - a secure, Rust-backed regex engine for JavaScript and TypeScript.","title":"regolith-website"},{"content":" The Go Programming Language # Go is an open source programming language that makes it easy to build simple, reliable, and efficient software.\nGopher image by Renee French, licensed under Creative Commons 4.0 Attribution license.\nOur canonical Git repository is located at https://go.googlesource.com/go. There is a mirror of the repository at https://github.com/golang/go.\nUnless otherwise noted, the Go source files are distributed under the BSD-style license found in the LICENSE file.\nDownload and Install # Binary Distributions # Official binary distributions are available at https://go.dev/dl/.\nAfter downloading a binary release, visit https://go.dev/doc/install for installation instructions.\nInstall From Source # If a binary distribution is not available for your combination of operating system and architecture, visit https://go.dev/doc/install/source for source installation instructions.\nContributing # Go is the work of thousands of contributors. We appreciate your help!\nTo contribute, please read the contribution guidelines at https://go.dev/doc/contribute.\nNote that the Go project uses the issue tracker for bug reports and proposals only. See https://go.dev/wiki/Questions for a list of places to ask questions about the Go language.\n","date":"18 June 2025","permalink":"/projects/go/","section":"Projects","summary":"The Go Programming Language # Go is an open source programming language that makes it easy to build simple, reliable, and efficient software.","title":"go"},{"content":"","date":"18 June 2025","permalink":"/tags/none/","section":"Tags","summary":"","title":"None"},{"content":" Regolith # A server-side TypeScript and JavaScript library immune to Regular Expression Denial of Service (ReDoS) attacks by using Rust and linear Regex under the hood. Regolith has a linear worst case time complexity, compared to the default RegExp found in TypeScript and JavaScript, which has an exponential worst case.\nMotivation: I wanted a Regex library for TypeScript and JavaScript where I didn\u0026rsquo;t have to worry about ReDoS attacks.\n[!IMPORTANT] Regolith is still early in development! We need help building and getting developer adoption!\nDrop-in Replacement # Regolith attempts to be a drop-in replacement for RegExp and requires minimal (to no) changes to be used instead. The goal of Regolith is to allow developers to easily build software that is immune to ReDoS attacks.\nPreventing ReDoS Attacks # What are ReDoS attacks? # Regular Expression Denial of Service (ReDoS) attacks occur when vulnerable Regex patterns are executed with specifically constructed inputs that result in an inefficient execution. This can be exploited to cause services to become unavailable because the services are stuck trying to compute the inefficient Regex.\nRead more: owasp.org \u0026amp; learn.snyk.io\nExponential Worst Case # Python has an exponential increase in execution time for the worst case.\nThis is the same case for TypeScript and JavaScript. Both having RegExp, which has an exponential worst case.\nLinear vs Exponential Regex Libraries # This table shows popular languages and if their Regex library has a linear worst case or an exponential worst case. It also includes experimental results for how long execution took for a vulnerable Regex pattern that can be attacked with ReDoS and an input of size 30.\nNote that TypeScript and JavaScript do not have a linear worst case for Regex, making them vulnerable to these types of attacks.\nMore information and images: Jake Roggenbuck - Preventing ReDoS Attacks - 2025\nImpact # [!NOTE]\nReDoS attacks happen relatively frequently to popular libraries costing millions of hours of work. This can be prevented with linear regex engines like Regolith.\nThese vulnerabilities happen relatively often in popular libraries. It\u0026rsquo;s no one\u0026rsquo;s fault specifically, it just comes down to the fact that the language allows for these things to happen.\nA recent example of a ReDoS vulnerability is CVE-2025-5889 from brace-expansion. Again, this isn\u0026rsquo;t any fault of that project, it\u0026rsquo;s simply an issue with the language allowing this to happen. Measures can be put into place to reduce the risk of this, but it\u0026rsquo;s hard to spot and test for these issues.\nThe brace-expansion project is used by 42.5 million other projects on GitHub. Meaning if everyone were to patch their software (which the hopefully will), that would be 42.5 million pull requests, roughly 42.5 million build minutes, and probably more than 42 million engineering minutes as well. All of that for a single vulnerability, and that\u0026rsquo;s just a lower bound of effort spent on this if everyone were to keep their software patched.\nOther versions of brace-expansion had these patches backported to them, needing updates for versions 1, 2, 3, and the current version 4.\nHaving a library or project that is immune to these vulnerabilities would save this effort for each project that adopted it, and would save the whole package ecosystem that effort if widely adopted. Adoption of libraries is difficult, especially when they aren\u0026rsquo;t very flashy, but helping library maintainers and engineers not worry about ReDoS for one library, one project at a time, is our goal.\nTrade-off # The Rust Regex library purposefully excludes features that make Regex engines particularly vulnerable to ReDoS attacks. Those features are backreferences and look-around. Excluding those features allow Regex to guarantee linear time execution.\nSince Regolith uses Rust bindings to implement the Rust Regex library to achieve linear time worst case, this means that backreferences and look-around aren\u0026rsquo;t available in Regolith either.\nThis trade-off has proven to be worth it for the Rust community of libraries and projects.\nResults # Since ReDoS vulnerabilities are hard to spot, there are rather frequent CVEs that get submitted. Having a Regex library that has a linear worst case time would completely prevent all of these potential issues for downstream projects.\nRust Regex under the hood # Regolith makes JavaScript bindings (using napi-rs) that implement the features of the very popular Regex library for Rust. Initially, when I had this idea for this library, I wanted to implement my own linear time regex engine. Now there is a chance I still end up doing that, I realized it\u0026rsquo;s better to not duplicate the work of the already excellent Regex library and focus on making these JavaScript and TypeScript bindings the best they can be. The focus of this project is to deliver the best linear time regex engine to TypeScript and JavaScript as a drop-in replacement for the default RegExp.\nMy full appreciation goes out to the developers of the Rust Regex library, who enabled this project to exist.\nLimitations # I\u0026rsquo;m still working to get this to be able to link to client side run applications like when you use react with \u0026ldquo;use client\u0026rdquo;. This may result in either finding a new way to link Rust (possible with WASM) or making my own Regex engine. Currently, everything works for server side JavaScript and TypeScript, which is the main focus of this, because servers are what usually get attacked in ReDoS attacks instead of clients.\nYou might get a message link: No loader is configured for \u0026quot;.node\u0026quot; files: node_modules/@regolithjs/regolith-linux-x64-gnu/regolith.linux-x64-gnu.node.\nI will actively be working to add this functionality and it will be tracked as issue #40. For now, I recommend using Regolith for server side and systems applications to prevent ReDoS attacks for servers, as this is what commonly gets Denial of Service attacks.\nUsage (Quick Start) # 1. Install # npm i @regolithjs/regolith 2. Try it out # import { Regolith } from \u0026#39;@regolithjs/regolith\u0026#39;; const pattern = new Regolith(\u0026#34;^\\\\d+$\u0026#34;); pattern.test(\u0026#34;12345\u0026#34;); // true pattern.test(\u0026#34;Hello\u0026#34;); // false Examples # Simple pattern matching example to match crab in our sentence my crab ferris.\nimport { Regolith } from \u0026#39;@regolithjs/regolith\u0026#39;; const pattern = new Regolith(\u0026#39;crab\u0026#39;, \u0026#39;g\u0026#39;); console.log(pattern.test(\u0026#39;my crab ferris\u0026#39;)); // true Here we use \u0026lsquo;g\u0026rsquo; in the Regolith constructor to mean a global.\nMatch method # const sentence = \u0026#39;crab, snail, crab\u0026#39;; const crabPattern = new Regolith(\u0026#39;crab\u0026#39;, \u0026#39;g\u0026#39;); // Find all matches console.log(crabPattern.match(sentence)); // Output: [\u0026#39;crab\u0026#39;, \u0026#39;crab\u0026#39;] Replace method # const sentence = \u0026#39;crab, snail, crab\u0026#39;; const crabPattern = new Regolith(\u0026#39;crab\u0026#39;, \u0026#39;g\u0026#39;); // Replace all occurrences console.log(crabPattern.replace(sentence, \u0026#39;snake\u0026#39;)); // Output: \u0026#39;snake, snail, snake\u0026#39; Search method # const sentence = \u0026#39;crab, snail, crab\u0026#39;; const snailPattern = new Regolith(\u0026#39;snail\u0026#39;); console.log(snailPattern.search(sentence)); // Output: 6 (index where \u0026#39;snail\u0026#39; is found) Split method # const splitPattern = new Regolith(\u0026#39;[,\\\\|]\u0026#39;); console.log(splitPattern.split(\u0026#39;apple,banana|orange\u0026#39;)); // Output: [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;orange\u0026#39;] Express Example # Make a backend express app that will tell you if a number is an int or a float. View the complete guide for the Express example.\nimport express from \u0026#34;express\u0026#34;; import { Regolith } from \u0026#34;@regolithjs/regolith\u0026#34;; const app = express(); const port = 3000; // Create Regolith Regex patterns const intPattern = new Regolith(\u0026#34;^\\\\d+$\u0026#34;); const floatPattern = new Regolith(\u0026#34;^\\\\d*\\\\.\\\\d+$\u0026#34;); app.get(\u0026#34;/check\u0026#34;, (req, res) =\u0026gt; { const value = req.query.value; if (!value) { return res.status(400).send(\u0026#34;Please provide a value query parameter\u0026#34;); } // Run the test with Regolith pattern const isInt = intPattern.test(value); const isFloat = floatPattern.test(value); res.json({ value, isInt, isFloat, }); }); app.listen(port, () =\u0026gt; { console.log(`Example app listening on port ${port}`); }); Development # These are instructions only if you want to build this library yourself (e.g. for development).\nBuilding # Before you build, you will need to have yarn installed. Here is a guide for installing yarn. You will also need Rust, and you can install Rust with rustup. To build the project, use this command:\nyarn build Running yarn build will build the Rust package, and you should see the Rust compiler complete the build process.\nRunning # Now we can test to see if Regolith was built correctly. We can open the node REPL and load the library.\nnode After opening the shell, you can load the library with:\nconst { Regolith } = await import(\u0026#34;./index.js\u0026#34;); After that, you can use Regolith as normal.\nconst integerPattern = new Regolith(\u0026#34;^\\\\d+$\u0026#34;); integerPattern.test(\u0026#34;123\u0026#34;); Here is an example of running Regolith in the REPL to test if it built correctly.\nTesting # Testing the TS/JS library # yarn test You should see the tests complete. Currently, there are 93 tests that get run.\nThese tests can be found in the __test__ directory.\nTesting the Rust bindings # cargo test Here is what the output should look like:\nThese tests can be found in the source files in src/lib.rs.\nPublishing Checklist # Increment the version in package.json All changes are merged into main Run the tests with yarn test Run npm login Run npm publish Docs # 1. Important Files # name purpose docs build.rs Runs the setup for napi-rs Cargo.lock Automatically generated by Cargo to keep track of Rust package versions Cargo.toml Contains information about the Rust crate; like the name, version, and dependencies index.d.ts Type information automatically generated by napi-rs index.js The main entry point for the library that is automatically generated by napi-rs package.json Information about the Regolith package rustfmt.toml A config for the Rust formatter yarn.lock Keeps track of the dependency version for yarn and it is automatically generated .npmignore Keeps files and directories out of what is shipped in the library Link .yarnrc.yml Configure yarn settings 2. Formatting # 2.1 Rust Format # Use cargo fmt. This is actually checked in the automated tests when you create a pull request. You can also see rustfmt.toml for the config for cargo fmt.\n2.2 TypeScript / JavaScript Format # Use prettier with prettier --write \u0026lt;file\u0026gt; or prettier --write . to format all .ts and .js files.\n3. Website # The source code for the Regolith website can be found at github.com/JakeRoggenbuck/regolith-website. The URL for the website is regolithjs.com.\n4. Platforms Tested # These are the platforms that Regolith has been tested on. These checks happen automatically in the CI.\nPlatform Status Arm 64 Apple Darwin Working Arm 64 Linux Android Working Arm 64 Linux GNU Working Arm 64 Linux MUSL Working Arm 64 PC Windows MSVC Working Arm v7 Linux GNU Working Arm v7 Linux MUSL Working x86-64 Linux MUSL Working x86-64 FreeBSD Not Tested i686 PC Windows MSVC Working Arm v7 Linux Andriod Working Universal Apple Darwin Working RISC-V 64 GC Linux GNU Working Report a Bug # If you find a bug, please send me an email at bug at jr0 dot org and or open an issue.\nName Origin # When trying to think of words that started with reg, I thought of the word regolith, which describes top layer of a planet that\u0026rsquo;s made of dust and rock. I likely got familiar with this word in a class I took about dinosaurs.\nMotivation and Background # I was initially inspired to build this library after doing undergraduate research to learn more about why certain languages have problems with ReDoS and others don\u0026rsquo;t. This led me to a question I couldn\u0026rsquo;t answer: \u0026ldquo;Why isn\u0026rsquo;t the most popular Regex library a linear time engine for languages like TypeScript, JavaScript, and Python?\u0026rdquo; You\u0026rsquo;d think that having a library that cannot get attacked (in a common way software gets attacked) would be more commonly used. I found an example called regexy in Python, but there hasn\u0026rsquo;t been an update in 8 years, and it was archived in 2024. There is also rure-python but this has not been updated in 6 years either. JavaScript has some more popular libraries that address this issue too. One is called re2js, which wraps Google\u0026rsquo;s RE2 library written in C++. re2js has a different API as the default RegExp from JavaScript, requiring some reworking on code that needs to be migrated over. The other is called node-re2, and this library also provides bindings for Google\u0026rsquo;s RE2 library. node-re2 does have an API similar to JavaScript\u0026rsquo;s RegExp. Even with these libraries, anecdotally it feels like the vast majority of projects still use the default regex for their respective languages; libraries that are vulnerable to ReDoS attacks. I could not find an exact percentage for how many projects use linear time engines vs exponential engines so this should be something to either find out from literature or maybe even try to answer this question directly, by reviewing packages published and trying to calculate a percentage.\nUltimately, I wanted a Regex library that is a drop-in replacement for RegExp in TypeScript and JavaScript where I didn\u0026rsquo;t have to worry about ReDoS attacks. My hope is that this library brings value to your software as well.\n","date":"16 June 2025","permalink":"/projects/regolith/","section":"Projects","summary":"Regolith # A server-side TypeScript and JavaScript library immune to Regular Expression Denial of Service (ReDoS) attacks by using Rust and linear Regex under the hood.","title":"regolith"},{"content":" Docs - Community - Roadmap - Why PostHog? - Changelog - Bug reports PostHog is an all-in-one, open source platform for building successful products # PostHog provides every tool you need to build a successful product including:\nProduct analytics: Autocapture or manually instrument event-based analytics to understand user behavior and analyze data with visualization or SQL. Web analytics: Monitor web traffic and user sessions with a GA-like dashboard. Easily monitor conversion, web vitals, and revenue. Session replays: Watch real user sessions of interactions with your website or mobile app to diagnose issues and understand user behavior. Feature flags: Safely roll out features to select users or cohorts with feature flags. Experiments: Test changes and measure their statistical impact on goal metrics. Set up experiments with no-code too. Error tracking: Track errors, get alerts, and resolve issues to improve your product. Surveys: Ask anything with our collection of no-code survey templates, or build custom surveys with our survey builder. Data warehouse: Sync data from external tools like Stripe, Hubspot, your data warehouse, and more. Query it alongside your product data. Data pipelines: Run custom filters and transformations on your incoming data. Send it to 25+ tools or any webhook in real time or batch export large amounts to your warehouse. LLM observability: Capture traces, generations, latency, and cost for your LLM-powered app. Best of all, all of this is free to use with a generous monthly free tier for each product. Get started by signing up for PostHog Cloud US or PostHog Cloud EU.\nTable of Contents # Getting started with PostHog Cloud Self-hosting the open-source hobby deploy Setting up PostHog Learning more about PostHog Contributing Open-source vs paid Getting started with PostHog Cloud (Recommended) # The fastest and most reliable way to get started with PostHog is signing up for free to PostHog Cloud or PostHog Cloud EU. Your first 1 million events, 5k recordings, 1M flag requests, 100k exceptions, and 250 survey responses are free every month, after which you pay based on usage.\nSelf-hosting the open-source hobby deploy (Advanced) # If you want to self-host PostHog, you can deploy a hobby instance in one line on Linux with Docker (recommended 4GB memory):\n/bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)\u0026#34; Open source deployments should scale to approximately 100k events per month, after which we recommend migrating to a PostHog Cloud.\nWe do not provide customer support or offer guarantees for open source deployments. See our self-hosting docs, troubleshooting guide, and disclaimer for more info.\nSetting up PostHog # Once you\u0026rsquo;ve got a PostHog instance, you can set it up by installing our JavaScript web snippet, one of our SDKs, or by using our API.\nWe have SDKs and libraries for popular languages and frameworks like:\nFrontend Mobile Backend JavaScript React Native Python Next.js Android Node React iOS PHP Vue Flutter Ruby Beyond this, we have docs and guides for Go, .NET/C#, Django, Angular, WordPress, Webflow, and more.\nOnce you\u0026rsquo;ve installed PostHog, see our product docs for more information on how to set up product analytics, web analytics, session replays, feature flags, experiments, error tracking, surveys, data warehouse, and more.\nLearning more about PostHog # Our code isn\u0026rsquo;t the only thing that\u0026rsquo;s open source 😳. We also open source our company handbook which details our strategy, ways of working, and processes.\nCurious about how to make the most of PostHog? We wrote a guide to winning with PostHog which walks you through the basics of measuring activation, tracking retention, and capturing revenue.\nContributing # We \u0026lt;3 contributions big and small:\nVote on features or get early access to beta functionality in our roadmap Open a PR (see our instructions on developing PostHog locally) Submit a feature request or bug report Open-source vs. paid # This repo is available under the MIT expat license, except for the ee directory (which has its license here) if applicable.\nNeed absolutely 💯% FOSS? Check out our posthog-foss repository, which is purged of all proprietary code and features.\nThe pricing for our paid plan is completely transparent and available on our pricing page.\nWe’re hiring! # Hey! If you\u0026rsquo;re reading this, you\u0026rsquo;ve proven yourself as a dedicated README reader.\nYou might also make a great addition to our team. We\u0026rsquo;re growing fast and would love for you to join us.\nContributors 🦸 # And 200+ more\u0026hellip; Thank you!\n","date":"6 June 2025","permalink":"/projects/posthog/","section":"Projects","summary":"Docs - Community - Roadmap - Why PostHog?","title":"posthog"},{"content":" Assured Confidential Execution (ACE) for RISC-V # ACE-RISCV is an open-source project, whose goal is to deliver a confidential computing framework with a formally proven security monitor. It is based on the canonical architecture and targets RISC-V with the goal of being portable to other architectures. The formal verification efforts focus on the security monitor implementation. We invite collaborators to work with us to push the boundaries of provable confidential computing technology.\nFormal verification: This project implements the RISC-V CoVE spec\u0026rsquo;s deployment model 3 referenced in Appendix D. The formal specification is embedded in the security monitor\u0026rsquo;s source code and the proofs are in the verification/ folder. Please read our paper1 and paper2 to learn about the approach and goals.\nPost-Quantum Cryptography (PQC) and Attestation: ACE supports local attestation, a mechanism to authenticate confidential VMs intended for embedded systems with limited or no network connectivity. We already support PQC, specifically we use ML-KEM, SHA-384, and AES-GCM-256 cryptography.\nHardware requirements # We are currently building on RISC-V 64-bit with integer (I), atomic (A) and hypervisor extentions (H), physical memory protection (PMP), memory management unit (MMU), IOPMP, core-local interrupt controller (CLINT), and supervisor timecmp extension (Sstc).\nRISC-V hardware to run ACE:\nSiFive P550 evaluation board, see instructions. Quick Start # Follow instructions to run one of the sample confidential workloads under an untrusted Linux KVM hypervisor in an emulated RISC-V environment.\nRequirements # Full compilation of the framework takes a long time because many tools are built from sources. Our toolchain currently includes: hypervisor kernel (Linux kernel), confidential guest kernel (Linux kernel) and firmware (security monitor with OpenSBI firmware). Make sure to build this project on a machine with at least 4 cores, 4GB RAM, and 50GB disk space for reasonable (~30min) build time.\nDependencies # You must install build dependencies specific to the operating system you use AND install the Rust toolchain. You can also look at the reproducible build configuration of the continous integration (CI) system.\nDependencies for Ubuntu 22.04\nsudo apt update # riscv-gnu-toolchain dependencies: sudo apt -qq -y install autoconf automake autotools-dev curl python3 libmpc-dev libmpfr-dev libgmp-dev gawk build-essential bison flex texinfo gperf libtool patchutils bc zlib1g-dev libexpat-dev xz-utils # OpenSBI sudo apt -qq -y install clang # Qemu 8.2 sudo apt -qq -y install git libglib2.0-dev libfdt-dev libpixman-1-dev zlib1g-dev ninja-build python3-venv libslirp-dev # Buildroot sudo apt -qq -y install unzip sed binutils diffutils build-essential bash patch gzip bzip2 perl tar cpio unzip rsync file bc findutils # utilities sudo apt install -y sshpass Install the latest Rust:\ncurl --proto \u0026#39;=https\u0026#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y source \u0026#34;$HOME/.cargo/env\u0026#34; rustup default nightly rustup target add riscv64gc-unknown-none-elf rustup component add rustfmt cargo install cargo-binutils # check that the below lines are in the ~/.bashrc . \u0026#34;$HOME/.cargo/env\u0026#34; Sources # Checkout this repository with submodules:\ngit clone --recurse-submodules git@github.com:IBM/ACE-RISCV.git Compilation # Prerequisites # Run the following commands from the directory containing this README file.\nMake sure once again that all submodules are fetched:\ngit submodule update --init --recursive Set up the ACE_DIR variable to point to the location where the project will build. Default is the build/ subdirectory of the location where you will execute the make command.\nexport ACE_DIR=/your/path/to/build/ace Build everything # The following command will build the entire framework. Set -j flag to the number of processor cores you have in the system. Below command assumes that you have 4 cores.\nMAKEFLAGS=\u0026#34;--silent -j4\u0026#34; make Build individual components # Alternativly, you can build individual components to avoid long builds that can lead to \u0026lsquo;ssh disconnections\u0026rsquo;, \u0026lsquo;hangups\u0026rsquo;, and similar issues.\nInstall all develoment tools required to compile code for the RISC-V architecture:\nmake devtools Build the host OS \u0026ndash; a Linux KVM hypervisor:\nmake hypervisor Build the low level firmware responsible for the boot process. This command will also build the security monitor (SM):\nmake firmware Build sample confidential workloads:\nmake confidential_vms Build the RISC-V emulator and utility tools that simplify running the test environment:\nmake emulator Run and Test # Make sure you have the ACE_DIR environmental variable set and it points to the location of your build. Check the \u0026lsquo;Compilation\u0026rsquo; section in case this variable is not set.\necho $ACE_DIR To run the test environment on a RISC-V emulator run:\n${ACE_DIR}/tools/ace run You should see the output from the boot process and a promt to login to the hypervisor:\n# login: root, password: passwd To run the sample Linux OS as a confidential VM (login: root, password: passwd) execute:\n./run_linux_vm_qemu.sh You should see the output indicating that local attestation suceeded:\n#ACE: Reference PCR4=Sha512=0x86774eec200ca6552cbc50211e4b32e7a4ba815c190d56b11ffabc8df1ebb6d9c41d04a64099d860b90c65729a28ded8 #ACE: Attestation succeeded, read 1 secret You can login now to the confidential VM:\n# login: root, password: passwd You can read the secret from the inside of the confidential VM:\n# if the root file system has not been mounted, then execute below: mount /dev/vda /root cd /root/root/ace_module insmod ace.ko You should see the secret:\n[ 203.051959] Requesting secret from the security monitor [ 203.107150] Secret=0xc0ffee Integrating local attestation with dm-crypt/LUKS is work in progress. When finished, you will be able to encrypt your rootfs and pass the decryption key via TAP. A script in initrd will then retrieve the decryption key from TAP and decrypt the rootfs.\nLicense # This repository is distributed under the terms of the Apache 2.0 License, see LICENSE.\nThis is an active research project, without warranties of any kind.\nCitation # Our newest full paper on ACE:\n@misc{ozga2025ace, author = {Ozga, Wojciech and Hunt, Guerney D. H. and Le, Michael V. and Gaeher Lennard and Shinnar, Avraham and Palmer, Elaine R. and Jamjoom, Hani and Dragone, Silvio}, title = {ACE: Confidential Computing for Embedded RISC-V Systems}, year = 2025, howpublished = {\\url{https://arxiv.org/pdf/2505.12995}} } Our workshop paper on ACE:\n@inproceedings{ozga2023riscvtee, title={Towards a Formally Verified Security Monitor for VM-based Confidential Computing}, author={Ozga, Wojciech and Hunt, Guerney D. H. and Le, Michael V. and Palmer, Elaine R. and Shinnar, Avraham}, booktitle = {Proceedings of the 12th International Workshop on Hardware and Architectural Support for Security and Privacy}, series = {HASP2023}, year={2023} } Our paper on context switch validation:\n@misc{kalani2025sailor, author = {Kalani, Neelu and Bourgeat, Thomas and Hunt, Guerney D.H. and Ozga, Wojciech}, title = {Save what must be saved: Secure context switching with Sailor}, year = 2025, howpublished = {\\url{https://arxiv.org/pdf/2502.06609}} } ","date":"21 May 2025","permalink":"/projects/ace-riscv/","section":"Projects","summary":"Assured Confidential Execution (ACE) for RISC-V # ACE-RISCV is an open-source project, whose goal is to deliver a confidential computing framework with a formally proven security monitor.","title":"ACE-RISCV"},{"content":" 🚀 Assemblr - HackDavis 2025 # A revolutionary web-based IDE with a custom assembly-like language for robotics programming!\n🌟 Overview # Assemblr is a cutting-edge development environment that combines the power of modern web technologies with a custom-built compiler for robotics programming. Whether you\u0026rsquo;re a robotics enthusiast, student, or professional developer, our platform provides an intuitive interface for writing and executing robot control code.\nKey Features # 🎯 Real-time Compilation: Instant feedback on your code 🔄 Live Preview: See your robot\u0026rsquo;s movements in a simulated environment 📝 Intelligent Code Editor: Syntax highlighting and autocompletion 🧪 Debugging Tools: Step-through execution and variable inspection 📱 Responsive Design: Works seamlessly on desktop and mobile 🌐 Cloud Integration: Save and share your projects 💡 Pro Tips \u0026amp; Tricks # Code Organization # Use meaningful labels and variable names Group related code into functions Add comments to explain complex logic Keep functions small and focused Performance Optimization # Use variables for repeated values Minimize wait times between movements Optimize movement patterns Use loops for repetitive actions Debugging Techniques # Add strategic mov wait commands to slow down execution Use variables to track state Print debug information to the terminal Test functions in isolation 📊 Architecture # graph TD A[Frontend - Next.js] --\u0026gt;|HTTP POST| B[Backend Server - Rust] B --\u0026gt;|Library Call| C[Compiler - Rust] C --\u0026gt;|IR| D[Code Generation] C --\u0026gt;|C++| E[Arduino Code] subgraph Frontend A --\u0026gt;|Monaco Editor| F[Code Editor] A --\u0026gt;|XTerm.js| G[Terminal] A --\u0026gt;|Three.js| H[3D Preview] end subgraph Backend B --\u0026gt;|/api/compile| I[IR Compilation] B --\u0026gt;|/api/compile/arduino| J[Arduino Compilation] B --\u0026gt;|/api/simulate| K[Robot Simulation] end 🛠️ Tech Stack # Frontend Powerhouse # ⚛️ Next.js 15 - React framework for production 📝 TypeScript - Type-safe development 🎨 Tailwind CSS - Utility-first styling 📊 Monaco Editor - VS Code-like editing experience 🖥️ XTerm.js - Terminal emulation 🎯 React Icons - Beautiful iconography 🎮 Three.js - 3D robot visualization 📱 Responsive Design - Mobile-first approach Robust Backend # 🦀 Rust - Systems programming language 🔧 Custom Compiler - Purpose-built for robotics 🌐 Unicode Support - International character compatibility 📦 Module System - Organized code structure 🔄 Async Runtime - High-performance concurrency 🧪 Unit Testing - Comprehensive test coverage 📡 API Documentation # Compilation Endpoints # 1. Compile to IR # POST /api/compile Content-Type: application/json { \u0026#34;code\u0026#34;: \u0026#34;your assembly code here\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;optimize\u0026#34;: true, \u0026#34;debug\u0026#34;: false } } Response:\n{ \u0026#34;output\u0026#34;: { \u0026#34;ir\u0026#34;: \u0026#34;compiled IR in JSON format\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;executionTime\u0026#34;: \u0026#34;0.123s\u0026#34;, \u0026#34;optimizationLevel\u0026#34;: \u0026#34;high\u0026#34;, \u0026#34;warnings\u0026#34;: [] } } } Error Response:\n{ \u0026#34;error\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;error message\u0026#34;, \u0026#34;line\u0026#34;: 42, \u0026#34;column\u0026#34;: 10, \u0026#34;suggestion\u0026#34;: \u0026#34;Did you mean \u0026#39;forward\u0026#39; instead of \u0026#39;foward\u0026#39;?\u0026#34; } } 2. Compile to Arduino # POST /api/compile/arduino Content-Type: application/json { \u0026#34;code\u0026#34;: \u0026#34;your assembly code here\u0026#34;, \u0026#34;board\u0026#34;: \u0026#34;arduino_uno\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;includeLibraries\u0026#34;: [\u0026#34;Servo.h\u0026#34;, \u0026#34;Wire.h\u0026#34;] } } Response:\n{ \u0026#34;output\u0026#34;: { \u0026#34;code\u0026#34;: \u0026#34;generated Arduino C++ code\u0026#34;, \u0026#34;dependencies\u0026#34;: [\u0026#34;Servo.h\u0026#34;, \u0026#34;Wire.h\u0026#34;], \u0026#34;memoryUsage\u0026#34;: { \u0026#34;flash\u0026#34;: \u0026#34;1234 bytes\u0026#34;, \u0026#34;sram\u0026#34;: \u0026#34;567 bytes\u0026#34; } } } Assembly Language Syntax # Basic Syntax # # Comments start with # # Labels end with : # Instructions are lowercase # Values can be decimal or hex (0x prefix) # Example program - Simple Square Movement square: mov forward, 10 # Move forward 10 units mov direction, 1 # Turn left 90 degrees mov forward, 10 # Move forward 10 units mov direction, 1 # Turn left 90 degrees mov forward, 10 # Move forward 10 units mov direction, 1 # Turn left 90 degrees mov forward, 10 # Move forward 10 units ret # Return to caller main: jal square # Execute square movement mov wait, 1 # Wait 1 second jal square # Repeat square movement Advanced Features # # Example: Complex Pattern with Variables and Loops var speed = 5 var pattern_size = 4 var angle = 45 # Function to draw a pattern draw_pattern: param size param angle var i = 0 pattern_loop: mov forward, size mov direction, angle add i, 1 blt i, pattern_size, pattern_loop ret main: mov speed, speed # Set movement speed jal draw_pattern # Draw first pattern mov wait, 0.5 # Pause mov direction, 180 # Turn around jal draw_pattern # Draw second pattern More Examples # 1. Spiral Pattern # # Draw a spiral pattern spiral: var radius = 1 var angle = 0 spiral_loop: mov forward, radius mov direction, 1 add radius, 1 add angle, 1 blt angle, 36, spiral_loop ret main: jal spiral 2. Obstacle Avoidance # # Simple obstacle avoidance routine avoid_obstacle: mov backward, 5 # Back up mov direction, 1 # Turn left mov forward, 10 # Move forward mov direction, 2 # Turn right mov forward, 10 # Move forward ret main: var obstacle_detected = 0 loop: mov forward, 1 # Simulate obstacle detection add obstacle_detected, 1 beq obstacle_detected, 5, avoid j loop avoid: jal avoid_obstacle mov obstacle_detected, 0 j loop 3. Zigzag Pattern # # Create a zigzag pattern zigzag: var steps = 5 var i = 0 zigzag_loop: mov forward, 10 mov direction, 1 mov forward, 10 mov direction, 2 add i, 1 blt i, steps, zigzag_loop ret main: jal zigzag Available Commands # Command Description Example Notes mov direction, N Set movement direction mov direction, 1 0=straight, 1=left, 2=right mov forward, N Move forward N units mov forward, 4 Units are in centimeters mov backward, N Move backward N units mov backward, 2 Negative values not allowed mov wait, N Wait N seconds mov wait, 1 Supports decimal values jal label Jump to label jal circle Supports nested calls ret Return from subroutine ret Must be in function var name = value Declare variable var speed = 5 Global scope param name Function parameter param distance Must be in function add var, value Add to variable add counter, 1 Supports variables sub var, value Subtract from variable sub counter, 1 Supports variables blt var, value, label Branch if less than blt i, 10, loop Supports variables beq var, value, label Branch if equal beq x, 0, end Supports variables j label Unconditional jump j loop Direct jump print var Print variable value print counter Debugging tool 🚀 Getting Started # Prerequisites # Node.js 18+ Rust and Cargo Git Arduino IDE (optional, for hardware deployment) Quick Start # Clone \u0026amp; Install\ngit clone https://github.com/yourusername/hackdavis-2025.git cd hackdavis-2025 Frontend Setup\ncd client npm install npm run dev Backend Setup\ncd server cargo run Compiler Setup\ncd compiler cargo build cargo test Development Workflow # Writing Code\nOpen the IDE in your browser (default: http://localhost:3000) Write your robot control code in the editor Use the live preview to see the robot\u0026rsquo;s movements Testing\nRun unit tests: cargo test Check compiler output: cargo run -- compile examples/robot.asm Test Arduino output: cargo run -- compile-arduino examples/robot.asm Deployment\nBuild production version: npm run build Deploy to server: cargo build --release Upload to Arduino: Use the generated .ino file 📁 Project Structure # hackdavis-2025/ ├── client/ # Frontend application │ ├── app/ # Next.js app directory │ │ ├── components/ # React components │ │ │ ├── Editor/ # Code editor component │ │ │ ├── Preview/ # 3D preview component │ │ │ └── Terminal/ # Terminal component │ │ ├── lib/ # Utility functions │ │ ├── page.tsx # Main page │ │ └── globals.css # Global styles │ └── package.json # Frontend dependencies │ ├── server/ # HTTP backend │ ├── src/ # Server source │ │ ├── main.rs # API endpoints │ │ ├── routes/ # Route handlers │ │ └── models/ # Data models │ └── Cargo.toml # Server dependencies │ ├── compiler/ # Rust compiler backend │ ├── src/ # Source code │ │ ├── lexer.rs # Token analysis │ │ ├── parser.rs # AST generation │ │ ├── codegen.rs # Code generation │ │ └── lib.rs # Core functionality │ ├── tests/ # Test cases │ └── Cargo.toml # Rust dependencies │ └── README.md # Project documentation 🧪 Testing # Frontend Tests # cd client npm test Backend Tests # cd server cargo test Compiler Tests # cd compiler cargo test 🤝 Contributing # We welcome contributions! Here\u0026rsquo;s how you can help:\nFork the repository Create your feature branch (git checkout -b feature/AmazingFeature) Commit your changes (git commit -m 'Add some AmazingFeature') Push to the branch (git push origin feature/AmazingFeature) Open a Pull Request Development Guidelines # Follow the Rust style guide for backend code Use TypeScript for all frontend code Write tests for new features Update documentation as needed Keep commits focused and atomic 📝 License # This project is licensed under the MIT License - see the LICENSE file for details.\n🙏 Acknowledgments # HackDavis 2025 organizers and mentors The amazing Next.js and Rust communities All contributors and supporters The open-source community for their invaluable tools and libraries Built with ❤️ at HackDavis 2025\nReport Bug · Request Feature\n","date":"19 April 2025","permalink":"/projects/hackdavis-2025/","section":"Projects","summary":"🚀 Assemblr - HackDavis 2025 # A revolutionary web-based IDE with a custom assembly-like language for robotics programming!","title":"hackdavis-2025"},{"content":" 🖥️ Live Demo | 🎥 Demo Video ▶️YouTube | 📖 Documentation | 📃 Papers 📰 News # 🗞️ News 📝 Description Kaggle Scenario release We release Kaggle Agent, try the new features! Official WeChat group release We created a WeChat group, welcome to join! (🗪 QR Code) Official Discord release We launch our first chatting channel in Discord (🗪 ) First release RDAgent is released on GitHub 🌟 Introduction # RDAgent aims to automate the most critical and valuable aspects of the industrial R\u0026amp;D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data. Methodologically, we have identified a framework with two key components: \u0026lsquo;R\u0026rsquo; for proposing new ideas and \u0026lsquo;D\u0026rsquo; for implementing them. We believe that the automatic evolution of R\u0026amp;D will lead to solutions of significant industrial value.\nR\u0026amp;D is a very general scenario. The advent of RDAgent can be your\n💰 Automatic Quant Factory ( 🎥Demo Video| ▶️YouTube) 🤖 Data Mining Agent: Iteratively proposing data \u0026amp; models ( 🎥Demo Video 1| ▶️YouTube) ( 🎥Demo Video 2| ▶️YouTube) and implementing them by gaining knowledge from data. 🦾 Research Copilot: Auto read research papers ( 🎥Demo Video| ▶️YouTube) / financial reports ( 🎥Demo Video| ▶️YouTube) and implement model structures or building datasets. 🤖 Kaggle Agent: Auto Model Tuning and Feature Engineering( 🎥Demo Video Coming Soon\u0026hellip;) and implementing them to achieve more in competitions. \u0026hellip; You can click the links above to view the demo. We\u0026rsquo;re continuously adding more methods and scenarios to the project to enhance your R\u0026amp;D processes and boost productivity.\nAdditionally, you can take a closer look at the examples in our 🖥️ Live Demo.\n⚡ Quick start # You can try above demos by running the following command:\n🐳 Docker installation. # Users must ensure Docker is installed before attempting most scenarios. Please refer to the official 🐳Docker page for installation instructions.\n🐍 Create a Conda Environment # Create a new conda environment with Python (3.10 and 3.11 are well-tested in our CI): conda create -n rdagent python=3.10 Activate the environment: conda activate rdagent 🛠️ Install the RDAgent # You can directly install the RDAgent package from PyPI: pip install rdagent 💊 Health check # rdagent provides a health check that currently checks two things. whether the docker installation was successful. whether the default port used by the rdagent ui is occupied. rdagent health_check ⚙️ Configuration # The demos requires following ability:\nChatCompletion json_mode embedding query For example: If you are using the OpenAI API, you have to configure your GPT model in the .env file like this.\ncat \u0026lt;\u0026lt; EOF \u0026gt; .env OPENAI_API_KEY=\u0026lt;replace_with_your_openai_api_key\u0026gt; # EMBEDDING_MODEL=text-embedding-3-small CHAT_MODEL=gpt-4-turbo EOF However, not every API services support these features by devault. For example: AZURE OpenAI, you have to configure your GPT model in the .env file like this.\ncat \u0026lt;\u0026lt; EOF \u0026gt; .env USE_AZURE=True EMBEDDING_OPENAI_API_KEY=\u0026lt;replace_with_your_azure_openai_api_key\u0026gt; EMBEDDING_AZURE_API_BASE=\u0026lt;replace_with_your_azure_endpoint\u0026gt; EMBEDDING_AZURE_API_VERSION=\u0026lt;replace_with_the_version_of_your_azure_openai_api\u0026gt; EMBEDDING_MODEL=text-embedding-3-small CHAT_OPENAI_API_KEY=\u0026lt;replace_with_your_azure_openai_api_key\u0026gt; CHAT_AZURE_API_BASE=\u0026lt;replace_with_your_azure_endpoint\u0026gt; CHAT_AZURE_API_VERSION=\u0026lt;replace_with_the_version_of_your_azure_openai_api\u0026gt; CHAT_MODEL=\u0026lt;replace_it_with_the_name_of_your_azure_chat_model\u0026gt; EOF For more configuration information, please refer to the documentation.\n🚀 Run the Application # The 🖥️ Live Demo is implemented by the following commands(each item represents one demo, you can select the one you prefer):\nRun the Automated Quantitative Trading \u0026amp; Iterative Factors Evolution: Qlib self-loop factor proposal and implementation application\nrdagent fin_factor Run the Automated Quantitative Trading \u0026amp; Iterative Model Evolution: Qlib self-loop model proposal and implementation application\nrdagent fin_model Run the Automated Medical Prediction Model Evolution: Medical self-loop model proposal and implementation application\n(1) Apply for an account at PhysioNet. (2) Request access to FIDDLE preprocessed data: FIDDLE Dataset. (3) Place your username and password in .env.\ncat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; .env DM_USERNAME=\u0026lt;your_username\u0026gt; DM_PASSWORD=\u0026lt;your_password\u0026gt; EOF rdagent med_model Run the Automated Quantitative Trading \u0026amp; Factors Extraction from Financial Reports: Run the Qlib factor extraction and implementation application based on financial reports\n# 1. Generally, you can run this scenario using the following command: rdagent fin_factor_report --report_folder=\u0026lt;Your financial reports folder path\u0026gt; # 2. Specifically, you need to prepare some financial reports first. You can follow this concrete example: wget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip unzip all_reports.zip -d git_ignore_folder/reports rdagent fin_factor_report --report_folder=git_ignore_folder/reports Run the Automated Model Research \u0026amp; Development Copilot: model extraction and implementation application\n# 1. Generally, you can run your own papers/reports with the following command: rdagent general_model \u0026lt;Your paper URL\u0026gt; # 2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`: rdagent general_model \u0026#34;https://arxiv.org/pdf/2210.09789\u0026#34; Run the Automated Kaggle Model Tuning \u0026amp; Feature Engineering: self-loop model proposal and feature engineering implementation application Using sf-crime (San Francisco Crime Classification) as an example. Register and login on the Kaggle website. Configuring the Kaggle API. (1) Click on the avatar (usually in the top right corner of the page) -\u0026gt; Settings -\u0026gt; Create New Token, A file called kaggle.json will be downloaded. (2) Move kaggle.json to ~/.config/kaggle/ (3) Modify the permissions of the kaggle.json file. Reference command: chmod 600 ~/.config/kaggle/kaggle.json Join the competition: Click Join the competition -\u0026gt; I Understand and Accept at the bottom of the competition details page. # Generally, you can run the Kaggle competition program with the following command: rdagent kaggle --competition \u0026lt;your competition name\u0026gt; # Specifically, you will need to first prepare some competition description files and configure the competition description file path, which you can follow for this specific example: # 1. Prepare the competition description files wget https://github.com/SunsetWolf/rdagent_resource/releases/download/kaggle_data/kaggle_data.zip unzip kaggle_data.zip -d git_ignore_folder/kaggle_data # 2. Add the competition description file path to the `.env` file. dotenv set KG_LOCAL_DATA_PATH \u0026#34;$(pwd)/git_ignore_folder/kaggle_data\u0026#34; # 3. run the application rdagent kaggle --competition sf-crime Description of the above example: Kaggle competition data, contains two parts: competition description file (json file) and competition dataset (zip file). We prepare the competition description file for you, the competition dataset will be downloaded automatically when you run the program, as in the example. If you want to download the competition description file automatically, you need to install chromedriver, The instructions for installing chromedriver can be found in the documentation. The Competition List Available can be found here. 🖥️ Monitor the Application Results # You can run the following command for our demo program to see the run logs.\nrdagent ui --port 19899 --log_dir \u0026lt;your log folder like \u0026#34;log/\u0026#34;\u0026gt; Note: Although port 19899 is not commonly used, but before you run this demo, you need to check if port 19899 is occupied. If it is, please change it to another port that is not occupied.\nYou can check if a port is occupied by running the following command.\nrdagent health_check 🏭 Scenarios # We have applied RD-Agent to multiple valuable data-driven industrial scenarios.\n🎯 Goal: Agent for Data-driven R\u0026amp;D # In this project, we are aiming to build an Agent to automate Data-Driven R\u0026amp;D that can\n📄 Read real-world material (reports, papers, etc.) and extract key formulas, descriptions of interested features and models, which are the key components of data-driven R\u0026amp;D . 🛠️ Implement the extracted formulas (e.g., features, factors, and models) in runnable codes. Due to the limited ability of LLM in implementing at once, build an evolving process for the agent to improve performance by learning from feedback and knowledge. 💡 Propose new ideas based on current knowledge and observations. 📈 Scenarios/Demos # In the two key areas of data-driven scenarios, model implementation and data building, our system aims to serve two main roles: 🦾Copilot and 🤖Agent.\nThe 🦾Copilot follows human instructions to automate repetitive tasks. The 🤖Agent, being more autonomous, actively proposes ideas for better results in the future. The supported scenarios are listed below:\nScenario/Target Model Implementation Data Building 💹 Finance 🤖 Iteratively Proposing Ideas \u0026amp; Evolving ▶️YouTube 🤖 Iteratively Proposing Ideas \u0026amp; Evolving ▶️YouTube 🦾 Auto reports reading \u0026amp; implementation ▶️YouTube 🩺 Medical 🤖 Iteratively Proposing Ideas \u0026amp; Evolving ▶️YouTube - 🏭 General 🦾 Auto paper reading \u0026amp; implementation ▶️YouTube 🤖 Auto Kaggle Model Tuning 🤖Auto Kaggle feature Engineering RoadMap: Currently, we are working hard to add new features to the Kaggle scenario. Different scenarios vary in entrance and configuration. Please check the detailed setup tutorial in the scenarios documents.\nHere is a gallery of successful explorations (5 traces showed in 🖥️ Live Demo). You can download and view the execution trace using this command from the documentation.\nPlease refer to 📖readthedocs_scen for more details of the scenarios.\n⚙️ Framework # Automating the R\u0026amp;D process in data science is a highly valuable yet underexplored area in industry. We propose a framework to push the boundaries of this important research field.\nThe research questions within this framework can be divided into three main categories:\nResearch Area Paper/Work List Benchmark the R\u0026amp;D abilities Benchmark Idea proposal: Explore new ideas or refine existing ones Research Ability to realize ideas: Implement and execute ideas Development We believe that the key to delivering high-quality solutions lies in the ability to evolve R\u0026amp;D capabilities. Agents should learn like human experts, continuously improving their R\u0026amp;D skills.\nMore documents can be found in the 📖 readthedocs.\n📃 Paper/Work list # 📊 Benchmark # Towards Data-Centric Automatic R\u0026amp;D @misc{chen2024datacentric, title={Towards Data-Centric Automatic R\u0026amp;D}, author={Haotian Chen and Xinjie Shen and Zeqi Ye and Wenjun Feng and Haoxue Wang and Xiao Yang and Xu Yang and Weiqing Liu and Jiang Bian}, year={2024}, eprint={2404.11276}, archivePrefix={arXiv}, primaryClass={cs.AI} } 🔍 Research # In a data mining expert\u0026rsquo;s daily research and development process, they propose a hypothesis (e.g., a model structure like RNN can capture patterns in time-series data), design experiments (e.g., finance data contains time-series and we can verify the hypothesis in this scenario), implement the experiment as code (e.g., Pytorch model structure), and then execute the code to get feedback (e.g., metrics, loss curve, etc.). The experts learn from the feedback and improve in the next iteration.\nBased on the principles above, we have established a basic method framework that continuously proposes hypotheses, verifies them, and gets feedback from the real-world practice. This is the first scientific research automation framework that supports linking with real-world verification.\nFor more detail, please refer to our 🖥️ Live Demo page.\n🛠️ Development # Collaborative Evolving Strategy for Automatic Data-Centric Development @misc{yang2024collaborative, title={Collaborative Evolving Strategy for Automatic Data-Centric Development}, author={Xu Yang and Haotian Chen and Wenjun Feng and Haoxue Wang and Zeqi Ye and Xinjie Shen and Xiao Yang and Shizhao Sun and Weiqing Liu and Jiang Bian}, year={2024}, eprint={2407.18690}, archivePrefix={arXiv}, primaryClass={cs.AI} } 🤝 Contributing # 📝 Guidelines # This project welcomes contributions and suggestions. Contributing to this project is straightforward and rewarding. Whether it\u0026rsquo;s solving an issue, addressing a bug, enhancing documentation, or even correcting a typo, every contribution is valuable and helps improve RDAgent.\nTo get started, you can explore the issues list, or search for TODO: comments in the codebase by running the command grep -r \u0026quot;TODO:\u0026quot;.\nBefore we released RD-Agent as an open-source project on GitHub, it was an internal project within our group. Unfortunately, the internal commit history was not preserved when we removed some confidential code. As a result, some contributions from our group members, including Haotian Chen, Wenjun Feng, Haoxue Wang, Zeqi Ye, Xinjie Shen, and Jinhui Li, were not included in the public commits.\n⚖️ Legal disclaimer # The RD-agent is provided “as is”, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. The RD-agent is aimed to facilitate research and development process in the financial industry and not ready-to-use for any financial investment or advice. Users shall independently assess and test the risks of the RD-agent in a specific use scenario, ensure the responsible use of AI technology, including but not limited to developing and integrating risk mitigation measures, and comply with all applicable laws and regulations in all applicable jurisdictions. The RD-agent does not provide financial opinions or reflect the opinions of Microsoft, nor is it designed to replace the role of qualified financial professionals in formulating, assessing, and approving finance products. The inputs and outputs of the RD-agent belong to the users and users shall assume all liability under any theory of liability, whether in contract, torts, regulatory, negligence, products liability, or otherwise, associated with use of the RD-agent and any inputs and outputs thereof.\n","date":"16 March 2025","permalink":"/projects/rd-agent/","section":"Projects","summary":"🖥️ Live Demo | 🎥 Demo Video ▶️YouTube | 📖 Documentation | 📃 Papers 📰 News # 🗞️ News 📝 Description Kaggle Scenario release We release Kaggle Agent, try the new features!","title":"RD-Agent"},{"content":"","date":"11 March 2025","permalink":"/tags/python/","section":"Tags","summary":"","title":"Python"},{"content":" Chess in Python # Final project for ECS 198F\nGroup members: Joe Vogel, Cait Chau, Jash Patel, and Jake Roggenbuck\nOutline: # Create breakdown of project, decomposition Assign tasks to group members Create a virtual environment for Python development using Docker Work on assigned tasks ","date":"11 March 2025","permalink":"/projects/python-chess/","section":"Projects","summary":"Chess in Python # Final project for ECS 198F","title":"Python-Chess"},{"content":"404: Not Found\n","date":"21 February 2025","permalink":"/projects/ecs198f_hw3_template/","section":"Projects","summary":"404: Not Found","title":"ecs198f_hw3_template"},{"content":" x86 Instruction Set Reference # Introduction # The reference web\nA web book📖 of x86 Instruction set reference converted from Mirror of: Into the Void: x86 Instruction Set Reference using a dumb script.\nInstruction syntax of this reference book is Intel Syntax which written as Instruction, Destination, Source.\n🔋 by Vuepress\n","date":"16 February 2025","permalink":"/projects/x86_ref_book/","section":"Projects","summary":"x86 Instruction Set Reference # Introduction # The reference web","title":"x86_ref_book"},{"content":"","date":"8 February 2025","permalink":"/tags/go/","section":"Tags","summary":"","title":"Go"},{"content":" Objx # Objx - Go package for dealing with maps, slices, JSON and other data.\nGet started:\nInstall Objx with one line of code, or update it with another Check out the API Documentation http://pkg.go.dev/github.com/stretchr/objx Overview # Objx provides the objx.Map type, which is a map[string]interface{} that exposes a powerful Get method (among others) that allows you to easily and quickly get access to data within the map, without having to worry too much about type assertions, missing data, default values etc.\nPattern # Objx uses a predictable pattern to make access data from within map[string]interface{} easy. Call one of the objx. functions to create your objx.Map to get going:\nm, err := objx.FromJSON(json) NOTE: Any methods or functions with the Must prefix will panic if something goes wrong, the rest will be optimistic and try to figure things out without panicking.\nUse Get to access the value you\u0026rsquo;re interested in. You can use dot and array notation too:\nm.Get(\u0026#34;places[0].latlng\u0026#34;) Once you have sought the Value you\u0026rsquo;re interested in, you can use the Is* methods to determine its type.\nif m.Get(\u0026#34;code\u0026#34;).IsStr() { // Your code... } Or you can just assume the type, and use one of the strong type methods to extract the real value:\nm.Get(\u0026#34;code\u0026#34;).Int() If there\u0026rsquo;s no value there (or if it\u0026rsquo;s the wrong type) then a default value will be returned, or you can be explicit about the default value.\nGet(\u0026#34;code\u0026#34;).Int(-1) If you\u0026rsquo;re dealing with a slice of data as a value, Objx provides many useful methods for iterating, manipulating and selecting that data. You can find out more by exploring the index below.\nReading data # A simple example of how to use Objx:\n// Use MustFromJSON to make an objx.Map from some JSON m := objx.MustFromJSON(`{\u0026#34;name\u0026#34;: \u0026#34;Mat\u0026#34;, \u0026#34;age\u0026#34;: 30}`) // Get the details name := m.Get(\u0026#34;name\u0026#34;).Str() age := m.Get(\u0026#34;age\u0026#34;).Int() // Get their nickname (or use their name if they don\u0026#39;t have one) nickname := m.Get(\u0026#34;nickname\u0026#34;).Str(name) Ranging # Since objx.Map is a map[string]interface{} you can treat it as such. For example, to range the data, do what you would expect:\nm := objx.MustFromJSON(json) for key, value := range m { // Your code... } Installation # To install Objx, use go get:\ngo get github.com/stretchr/objx Staying up to date # To update Objx to the latest version, run:\ngo get -u github.com/stretchr/objx Supported go versions # We currently support the three recent major Go versions.\nContributing # Please feel free to submit issues, fork the repository and send pull requests!\n","date":"8 February 2025","permalink":"/projects/objx/","section":"Projects","summary":"Objx # Objx - Go package for dealing with maps, slices, JSON and other data.","title":"objx"},{"content":" Onlook Cursor for Designers Explore the docs » 👨‍💻👩‍💻👨‍💻 We're hiring engineers in SF! 👩‍💻👨‍💻👩‍💻 View Demo · Report Bug · Request Feature 中文 | Español | Deutsch | français | Português | Русский | 日本語 | 한국어\nThe Cursor for Designers – an Open-Source, Visual-First Code Editor # Craft websites, prototypes, and designs with AI in Next.js + TailwindCSS. Make edits directly in the browser DOM with a visual editor. Design in realtime with code. An open-source alternative to Bolt.new, Lovable, V0, Replit Agent, Figma Make, Webflow, etc.\n🚧 🚧 🚧 Onlook for Web is still under development 🚧 🚧 🚧 # We\u0026rsquo;re actively looking for contributors to help make Onlook for Web an incredible prompt-to-build experience. Check the open issues for a full list of proposed features (and known issues), and join our Discord to collaborate with hundreds of other builders.\nWhat you can do with Onlook: # Create Next.js app in seconds Start from text or image Use prebuilt templates Import from Figma Start from GitHub repo Visually edit your app Use Figma-like UI Preview your app in real-time Manage brand assets and tokens Create and navigate to Pages Browse layers – Previously in Onlook Desktop Detect and use Components – Previously in Onlook Desktop Manage project Images – Previously in Onlook Desktop Development Tools Real-time code editor Save and restore from checkpoints Run commands via CLI Connect with app marketplace Edit code locally – Previously in Onlook Desktop Deploy your app in seconds Generate sharable links Link your custom domain Collaborate with your team Real-time editing Leave comments Onlook for Desktop (aka Onlook Alpha) # We\u0026rsquo;re in early preview for Onlook Web. If you\u0026rsquo;re looking for the downloadable desktop electron app, it\u0026rsquo;s moved to Onlook Desktop.\nWhy are we moving to the web? Read about our decision: Migrating from Electron to Web\nGetting Started # Available soon with a hosted app or run locally.\nUsage # Onlook will run on any Next.js + TailwindCSS project, import your project into Onlook or start from scratch within the editor.\nUse the AI chat to create or edit a project you\u0026rsquo;re working on. At any time, you can always right-click an element to open up the exact location of the element in code.\nDraw-in new divs and re-arrange them within their parent containers by dragging-and-dropping.\nPreview the code side-by-side with your site design.\nUse Onlook\u0026rsquo;s editor toolbar to adjust Tailwind styles, directly manipulate objects, and experiment with layouts.\nDocumentation # For full documentation, visit docs.onlook.com\nTo see how to Contribute, visit Contributing to Onlook in our docs.\nHow it works # When you create an app, we load the code into a web container The container runs and serves the code Our editor receives the preview link and displays it in an iFrame Our editor reads and indexes the code from the container We instrument the code in order to map elements to their place in code When the element is edited, we edit the element in our iFrame, then in code Our AI chat also has code access and tools to understand and edit the code This architecture can theoretically scale to any language or framework that displays DOM elements declaratively (e.g. jsx/tsx/html). We are focused on making it work well with Next.js and TailwindCSS for now.\nFor a full walkthrough, check out our Architecture Docs.\nOur Tech Stack # Front-end # Next.js - Full stack TailwindCSS - Styling tRPC - Server interface Database # Supabase - Auth, Database, Storage Drizzle - ORM AI # AI SDK - LLM client Anthropic - LLM model provider Morph Fast Apply - Fast apply model provider Relace - Fast apply model provider Sandbox and hosting # CodeSandboxSDK - Dev sandbox Freestyle - Hosting Runtime # Bun - Monorepo, runtime, bundler Docker - Container management Contributing # If you have a suggestion that would make this better, please fork the repo and create a pull request. You can also open issues.\nSee the CONTRIBUTING.md for instructions and code of conduct.\nContributors # Contact # Team: Discord - Twitter - LinkedIn - Email Project: https://github.com/onlook-dev/onlook Website: https://onlook.com License # Distributed under the Apache 2.0 License. See LICENSE.md for more information.\n","date":"8 February 2025","permalink":"/projects/onlook/","section":"Projects","summary":"Onlook Cursor for Designers Explore the docs » 👨‍💻👩‍💻👨‍💻 We're hiring engineers in SF!","title":"onlook"},{"content":"","date":"4 February 2025","permalink":"/tags/dockerfile/","section":"Tags","summary":"","title":"Dockerfile"},{"content":" ecs198f_hw2_template # ","date":"4 February 2025","permalink":"/projects/ecs198f_hw2_template/","section":"Projects","summary":" ecs198f_hw2_template # ","title":"ecs198f_hw2_template"},{"content":" 🦀 RedoxQL is an L-Store database written in Rust and Python 🚀\n🥇 Fastest database speed in the class for milestone 2 (ECS165A Winter 2025) ⚡\nRedoxQL is our implementation of an L-Store database.\n[!IMPORTANT] Read the Structure section — We use both Rust and Python and they go in different places\nSetup # Create a virtual environment\npython3 -m venv venv Source the virtual environment\nsource venv/bin/activate Install maturin\npip install -r requirements.txt Running # Build the Rust code\nmaturin build --release Install the module (Note: the version will change so check the exact filename in target/wheels/)\npip install target/wheels/lstore* --force-reinstall Run the database benchmark\npython3 __main__.py You should see this for milestone 1.\n(venv) ecs-165a-database (main) λ p __main__.py Inserting 10k records took: 0.0077650810000000035 Updating 10k records took: 0.020893269 Selecting 10k records took: 0.016048745000000003 Aggregate 10k of 100 record batch took:\t0.0039221569999999956 Deleting 10k records took: 0.002314741000000009 (venv) ecs-165a-database (main) λ Attribution # Keanu - Secondary indexes, page.rs and all the page stuff, index.rs and all of the index stuff Lucas \u0026amp; Andrew - update Lucas - Merge, select_version, sum_version, matching Abdulrasol - Merge, BaseContainer, TailContainer, PageDirectory, insert into containers, RecordAddress and Record Jake - Persistence, RTable, RDatabase, RQuery, new RIndex, python bindings, inserts and reads for all props Structure # File Structure # . ├── benches │ └── db_benchmark.rs ├── Cargo.lock ├── Cargo.toml ├── docs │ └── old-system-diagrams.md ├── LICENSE ├── main_checking.py ├── __main__.py ├── Makefile ├── pyproject.toml ├── python │ ├── benchmarks │ │ ├── graph_scale.py │ │ ├── scaling_tester.py │ │ ├── simple_example.py │ │ ├── simple_tester.py │ │ └── speedtests.py │ ├── lstore │ │ ├── db.py │ │ ├── __init__.py │ │ ├── query.py │ │ ├── transaction.py │ │ └── transaction_worker.py │ └── tests │ ├── __init__.py │ └── test_main.py ├── README.md ├── requirements.txt └── src ├── container.rs ├── database.rs ├── index.rs ├── lib.rs ├── pagerange.rs ├── page.rs ├── query.rs ├── record.rs ├── system.rs └── table.rs lstore # The lstore (./lstore) directory is where the Python code goes. This is what gets called by the tests in __main__.py. The lstore directory can be referred to as a Python module.\nsrc # The src (./src) directory is where the Rust code goes. This gets called by the code in the lstore module.\nsystem.rs - all the functions and structs related to getting information from the system machine database.rs - the database struct and the related functions\nDemo Project # In the demo project, we made an API that uses our database to store grades. Here we are sending a PUT and then doing a GET.\nResults # Milestone 1 # After a simple fix after we submitted, we were able to get everything working.\nMilestone 2 # Milestone 3 # exam_tester_m3_part1.py and exam_tester_m3_part2.py\nm3_tester_part_1.py and m3_tester_part_2.py\nUse in Python # This is a quickstart for how to use RedosQL as a library in Python. To follow along, run the setup instructions and then you can write the following code.\n# Import the database class from lstore.db import Database # Create an instance of the database db = Database() # Create a table in the database # This table has 5 columns and the primary key is the 0th column grades_table = db.create_table(\u0026#39;Grades\u0026#39;, 5, 0) # Import the query from lstore.query import Query # Insert a row with the primary key 0 and 4 grades query.insert([0, 91, 92, 93, 94]) # Update the 1st column in the database query.update(0, [90, None, None, None]) Testing # Rust testing # cargo test Here is what the correct output should look like. You should see multiple tests passing.\nUnit Tests # Rust unit tests are located in each Rust file and can be found in ./src\nIntegration Tests # The integration tests are located at ./tests and are also run with cargo test\nPython testing # pytest Python tests are located in a separate directory called tests located in ./python\nRust Docs # Rust has a way of making docs from the source code. Run cargo doc and view the produced HTML page in your browser. Adding comments to your code starting with /// will be put into these docs.\nSpeed Analysis # Using flamegraph to benchmark # You may need to install flamegraph with cargo install flamegraph\ncargo flamegraph --test update_tests # Open the svg (It\u0026#39;s nice to view in a browser) firefox flamegraph.svg Preview:\nRunning cargo benchmarks # This will take a long time but you can run benchmarks separately.\ncargo bench You can use cargo bench to see if your changes significantly affect performance.\nOften small changes can happen randomly. Like this has no change in the code. Try to run the bench as the only thing running on the system.\nUsing maturin in release mode # Maturin is what builds our Python package. Maturin has a mode called --release that significantly improves the speed. This is because release mode runs Cargo in release mode, and this does all of the optimization tricks to speed up the executable as much as possible.\nData for both graphs can be found here\nScaling of Insert Operation # Using release build settings # With compiler options\n(venv) redoxql (main) λ p __main__.py Inserting 10k records took: 0.006803145 Updating 10k records took: 0.018702902999999996 Selecting 10k records took: 0.016315803000000004 Aggregate 10k of 100 record batch took: 0.005981531999999998 Deleting 10k records took: 0.002332115999999995 (venv) redoxql (main) λ time p m1_tester.py Insert finished real 0m0.117s user 0m0.106s sys 0m0.010s user 0m0.106s sys 0m0.010s (venv) redoxql (main) λ time p exam_tester_m1.py Insert finished real 0m0.282s user 0m0.272s sys 0m0.010s (venv) redoxql (main) λ Without compiler options\n(venv) redoxql (main) λ p __main__.py Inserting 10k records took: 0.007401888000000002 Updating 10k records took: 0.018957811999999997 Selecting 10k records took: 0.015054888999999995 Aggregate 10k of 100 record batch took: 0.003300163000000002 Deleting 10k records took: 0.002181812999999991 (venv) redoxql (main) λ time p m1_tester.py Insert finished real 0m0.112s user 0m0.108s sys 0m0.004s (venv) redoxql (main) λ time p exam_tester_m1.py Insert finished real 0m0.268s user 0m0.254s sys 0m0.014s Running with debug or info logging # To use the logger, import the debug, error, or info macro from the log crate. Then you can add the macros to code like debug!(\u0026quot;Start database!\u0026quot;);. When you go to run the code, you can set the env var RUST_LOG=debug. Docs: https://docs.rs/env_logger/latest/env_logger/.\nWe tried Pypy # We started to try using Pypy which is a runtime for Python that is supposedly faster. Because of Amdahl\u0026rsquo;s law, we actually can\u0026rsquo;t get all that much performance out of it. We also found that the newest version of Pypy cannot use the newest version of Pyo3, so future work is needed to get them to run together.\nFuture questions:\nHow much time does the Python part take up? How do we measure the improvement from Python to Pypy How do we downgrade Pypy to work with Py03 We opted to go for C Python because it\u0026rsquo;s much faster in this use case. We show the results of some tests here.\nProfiling Python # We use py-spy to profile Python and we got expected results. It shows that the main use of Python is just the update function when testM2.py is run.\nHere is how we ran it:\npy-spy record -o profile.svg -- python3 testM2.py profile\nMoving everything possible to Rust # We used to make a ReturnRecord object for every single row! We also would turn the result of rquery into a list, wrap each in ReturnRecord and then make that back into a new list. ReturnRecord would also do list truncation each time it was initialized (for each row). We moved all of this logic into Rust can simply return what the Rust function returns. This change improved the performance by nearly 30%. The speed of testM2.py went from 1.30 seconds to 0.92 seconds. These results we consistent across 3 different runs each done in an interwoven fashion (with change, then without change, then with change again, etc.) for a total of 6 runs. The change can be seen in PR#162.\n- class ReturnRecord: - def __init__(self, columns: List[int]): - self.columns = columns[4:] - - def __str__(self): - return f\u0026#34;Record({self.columns})\u0026#34; - return [ - ReturnRecord( - list( - self.rquery.select_version( - search_key, - search_key_index, - projected_columns_index, - relative_version, - ) - ) - ) - ] + return self.rquery.select_version( + search_key, + search_key_index, + projected_columns_index, + relative_version, + ) This includes insert and update.\nUsing FxHashMap instead of default HashMap # The Rust default HashMap is a great general purpose HashMap implementation and is very fast but FxHashMap is a decent bit faster. After changing the page directory to use FxHashMap in PR#186, the speed of many reads and writes improved by over 28% and the overall speed of all the benchmarks improved by over 26%.\nuse crate::container::{ReservedColumns, NUM_RESERVED_COLUMNS}; use crate::index::RIndexHandle; use pyo3::prelude::*; use rustc_hash::FxHashMap; use serde::{Deserialize, Serialize}; use std::collections::HashMap; use std::sync::{Arc, Mutex, RwLock}; type RedoxQLHasher\u0026lt;K, V\u0026gt; = FxHashMap\u0026lt;K, V\u0026gt;; #[derive(Default, Clone, Serialize, Deserialize)] pub struct PageDirectoryMetadata { pub directory: HashMap\u0026lt;i64, RecordMetadata\u0026gt;, pub directory: RedoxQLHasher\u0026lt;i64, RecordMetadata\u0026gt;, } #[derive(Default, Clone)] pub struct PageDirectory { pub directory: HashMap\u0026lt;i64, Record\u0026gt;, pub directory: RedoxQLHasher\u0026lt;i64, Record\u0026gt;, } // -- snip -- Using Rust compilation flags # You can set specific build flags that improve the optimization.\n[profile.release] lto = \u0026#34;fat\u0026#34; codegen-units = 1 opt-level = 3 lto = \u0026quot;fat\u0026quot; is the max setting for \u0026ldquo;Link time optimization\u0026rdquo;, meaning that when it links the binary together, it does the maximum amount of optimizations.\ncodegen-units = 1 means that rustc will not split up the source code into different parts to compile separately on different threads. Because of this, it\u0026rsquo;s able to apply optimization tricks like inlining between libraries and our project source code.\nopt-level = 3 is just setting the optimization level. The reason you might want it lower is compilation speed.\nUsing snakeviz to profile # After reading bits from the O\u0026rsquo;Reilly High Performance Python, 3rd Edition, I found out about profiling with snakeviz.\npip install snakeviz python3 -m cProfile -o profile.prof testM2.py snakeviz profile.prof Here is the output from testM2.py\nPython dis library and bytecode # Python gets parsed into bytecode and we can see it using the dis library. This can be useful to see what byte might be unneeded.\nSpeed Improvement PRs # Improve hashing speed for page directory by over 26% ~70% speed improvement with lazy select (Not used because the API expects an iterable) Move select return record logic all to Rust to achieve 30% speed improvement ","date":"13 January 2025","permalink":"/projects/redoxql/","section":"Projects","summary":"🦀 RedoxQL is an L-Store database written in Rust and Python 🚀","title":"RedoxQL"},{"content":" ecs198f_hw1_template # ","date":"12 January 2025","permalink":"/projects/ecs198f_hw1_template/","section":"Projects","summary":" ecs198f_hw1_template # ","title":"ecs198f_hw1_template"},{"content":" JOS # JSON Object Store - quickly send JSON to a server and retrieve it with a hash\nUsage # export ADMIN_PASSWORD=\u0026lt;password\u0026gt; Upload a file # curl -u \u0026#34;Admin:$ADMIN_PASSWORD\u0026#34; -F \u0026#34;myFile=@path_to_your_file\u0026#34; \\ http://localhost:8080/api/v1/upload-file { \u0026#34;hash\u0026#34;: \u0026#34;\u0026lt;hash\u0026gt;\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;File uploaded and saved successfully!\u0026#34; } Get a file # curl -u \u0026#34;Admin:$ADMIN_PASSWORD\u0026#34; http://localhost:8080/api/v1/get-file?hash=\u0026lt;hash\u0026gt; Upload JSON # curl -u \u0026#34;Admin:$ADMIN_PASSWORD\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;key\u0026#34;: \u0026#34;value\u0026#34;}\u0026#39; http://localhost:8080/api/v1/upload-json { \u0026#34;hash\u0026#34;: \u0026#34;\u0026lt;hash\u0026gt;\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;JSON uploaded and saved successfully!\u0026#34; } Get JSON # curl -u \u0026#34;Admin:$ADMIN_PASSWORD\u0026#34; http://localhost:8080/api/v1/get-json?hash=\u0026lt;hash\u0026gt; Setup # Build the container\ndocker build -t jos-server . Run the container\ndocker run -d --name jos_server -p 8080:8080 jos-server Check that it\u0026rsquo;s running\ndocker ps ","date":"12 January 2025","permalink":"/projects/jos/","section":"Projects","summary":"JOS # JSON Object Store - quickly send JSON to a server and retrieve it with a hash","title":"JOS"},{"content":"","date":"12 January 2025","permalink":"/tags/shell/","section":"Tags","summary":"","title":"Shell"},{"content":" ResilientDB: Global-Scale Sustainable Blockchain Fabric # ResilientDB is a High Throughput Yielding Permissioned Blockchain Fabric founded by ExpoLab at UC Davis in 2018. ResilientDB advocates a system-centric design by adopting a multi-threaded architecture that encompasses deep pipelines. Further, ResilientDB separates the ordering of client transactions from their execution, which allows it to process messages out-of-order.\nDownloads: # Download address for run-directly software package: https://downloads.apache.org/incubator/resilientdb/\nQuick Facts on ResilientDB # ResilientDB orders client transactions through a highly optimized implementation of the PBFT [Castro and Liskov, 1998] protocol, which helps to achieve consensus among its replicas. ResilientDB also supports deploying other state-of-the-art consensus protocols [release are planned] such as GeoBFT [ blog, released], PoE, RCC, RingBFT, PoC, SpotLess, HotStuff, and DAG. ResilientDB requires deploying at least 3f+1 replicas, where f (f \u0026gt; 0) is the maximum number of arbitrary (or malicious) replicas. ResilientDB supports primary-backup architecture, which designates one of the replicas as the primary (replica with identifier 0). The primary replica initiates consensus on a client transaction, while backups agree to follow a non-malicious primary. ResilientDB exposes a wide range of interfaces such as a Key-Value store, Smart Contracts, UTXO, and Python SDK. Following are some of the decentralized applications (DApps) built on top of ResilientDB: NFT Marketplace and Debitable. To persist blockchain, chain state, and metadata, ResilientDB provides durability through LevelDB. ResilientDB provides access to a seamless GUI display for deployment and maintenance, and supports Grafana for plotting monitoring data. [Historial Facts] The ResilientDB project was founded by Mohammad Sadoghi along with his students ( Suyash Gupta as the lead Architect, Sajjad Rahnama as the lead System Designer, and Jelle Hellings) at UC Davis in 2018 and was open-sourced in late 2019. On September 30, 2021, we released ResilientDB v-3.0. In 2022, ResilientDB was completely re-written and re-architected ( Junchao Chen as the lead Architect, Dakai Kang as the lead Recovery Architect along with the entire NexRes Team), paving the way for a new sustainable foundation, referred to as NexRes (Next Generation ResilientDB). Thus, on September 30, 2022, NexRes-v1.0.0 was born, marking a new beginning for ResilientDB. On October 21, 2023, ResilientDB was officially accepted into Apache Incubation. Online Documentation: # The latest ResilientDB documentation, including a programming guide, is available on our blog repository. This README file provides basic setup instructions.\nTable of Contents # Software Stack Architecture SDK, Interface/API, Platform, Execution, and Chain Layers Detailed API Documentation: Core and SDK SDK Layer: Python SDK and Wallet - ResVault Interface Layer: Key-Value, Solidity Smart Contract, Unspent Transaction Output (UTXO) Model, ResilientDB Database Connectivity (RDBC) API Platform Layer: Consensus Manager Architecture (ordering, recovery, network, chain management) Recovery \u0026amp; Checkpoint Design Execution Layer: Transaction Manager Design (Runtime) Chain Layer: Chain State \u0026amp; Storage Manager Design ( durability) Installing \u0026amp; Deploying ResilientDB Build Your First Application: KV Service, UTXO Dashboard: Monitoring, Deployment, Data Pipeline System Parameters \u0026amp; Configuration Continuous Integration \u0026amp; Testing OS Requirements # Ubuntu 20+\nBuild and Deploy ResilientDB # Next, we show how to quickly build ResilientDB and deploy 4 replicas and 1 client proxy on your local machine. The proxy acts as an interface for all the clients. It batches client requests and forwards these batches to the replica designated as the leader. The 4 replicas participate in the PBFT consensus to order and execute these batches. Post execution, they return the response to the leader.\nInstall dependencies:\n./INSTALL.sh For non-root users, see INSTALL/README.md\nRun ResilientDB (Providing a Key-Value Service):\n./service/tools/kv/server_tools/start_kv_service.sh This script starts 4 replicas and 1 client. Each replica instantiates a key-value store. Build Interactive Tools:\nbazel build service/tools/kv/api_tools/kv_service_tools Issues # If you cannot build the project successfully, try to reduce the bazel jobs here.\nFunctions # ResilientDB supports two types of functions: version-based and non-version-based. Version-based functions will leverage versions to protect each update, versions must be obtained before updating a key.\nNote: Version-based functions are not compatible with non-version-based functions. Do not use both in your applications.\nWe show the functions below and show how to use kv_service_tools to test the function.\nVersion-Based Functions # Get # Obtain the value of key with a specific version v.\nkv_service_tools --config config_file --cmd get_with_version --key key --version v parameters descriptions config the path of the client config which points to the db entrance cmd get_with_version key the key you want to obtain version the version you want to obtain. (If the v is 0, it will return the latest version Example:\nbazel-bin/service/tools/kv/api_tools/kv_service_tools --config service/tools/config/interface/service.config --cmd get_with_version --key key1 --version 0 Results:\nget key = key1, value = value: \u0026ldquo;v2\u0026rdquo; version: 2\nSet # Set value to the key key based on version v.\nkv_service_tools --config config_file --cmd set_with_version --key key --version v --value value parameters descriptions config the path of the client config which points to the db entrance cmd set_with_version key the key you want to set version the version you have obtained. (If the version has been changed during the update, the transaction will be ignored) value the new value Example:\nbazel-bin/service/tools/kv/api_tools/kv_service_tools --config service/tools/config/interface/service.config --cmd set_with_version --key key1 --version 0 --value v1 Results:\nset key = key1, value = v3, version = 2 done, ret = 0\ncurrent value = value: \u0026ldquo;v3\u0026rdquo; version: 3\nGet Key History # Obtain the update history of key key within the versions [v1, v2].\nkv_service_tools --config config_file --cmd get_history --key key --min_version v1 --max_version v2 parameters descriptions config the path of the client config which points to the db entrance cmd get_history key the key you want to obtain min_version the minimum version you want to obtain max_version the maximum version you want to obtain Example:\nbazel-bin/service/tools/kv/api_tools/kv_service_tools --config service/tools/config/interface/service.config --cmd get_history --key key1 --min_version 1 --max_version 2 Results:\nget history key = key1, min version = 1, max version = 2 value = item { key: \u0026ldquo;key1\u0026rdquo; value_info { value: \u0026ldquo;v1\u0026rdquo; version: 2 } } item { key: \u0026ldquo;key1\u0026rdquo; value_info { value: \u0026ldquo;v0\u0026rdquo; version: 1 } }\nGet Top # Obtain the recent top_number history of the key key.\nkv_service_tools --config config_path --cmd get_top --key key --top top_number parameters descriptions config the path of the client config which points to the db entrance cmd get_top key the key you want to obtain top the number of the recent updates Example:\nbazel-bin/service/tools/kv/api_tools/kv_service_tools --config service/tools/config/interface/service.config --cmd get_top --key key1 --top 1 Results:\nkey = key1, top 1 value = item { key: \u0026ldquo;key1\u0026rdquo; value_info { value: \u0026ldquo;v2\u0026rdquo; version: 3 } }\nGet Key Range # Obtain the values of the keys in the ranges [key1, key2]. Do not use this function in your practice code\nkv_service_tools --config config_file --cmd get_key_range_with_version --min_key key1 --max_key key2 parameters descriptions config the path of the client config which points to the db entrance cmd get_key_range_with_version min_key the minimum key max_key the maximum key Example:\nbazel-bin/service/tools/kv/api_tools/kv_service_tools --config service/tools/config/interface/service.config --cmd get_key_range_with_version --min_key key1 --max_key key3 Results:\nmin key = key1 max key = key2 getrange value = item { key: \u0026ldquo;key1\u0026rdquo; value_info { value: \u0026ldquo;v0\u0026rdquo; version: 1 } } item { key: \u0026ldquo;key2\u0026rdquo; value_info { value: \u0026ldquo;v1\u0026rdquo; version: 1 } }\nNon-Version-Based Function # Set # Set value to the key key.\nkv_service_tools --config config_file --cmd set --key key --value value parameters descriptions config the path of the client config which points to the db entrance cmd set key the key you want to set value the new value Example:\nbazel-bin/service/tools/kv/api_tools/kv_service_tools --config service/tools/config/interface/service.config --cmd set --key key1 --value value1 Results:\nset key = key1, value = v1, done, ret = 0\nGet # Obtain the value of key.\nkv_service_tools --config config_file --cmd get --key key parameters descriptions config the path of the client config which points to the db entrance cmd get key the key you want to obtain Example:\nbazel-bin/service/tools/kv/api_tools/kv_service_tools --config service/tools/config/interface/service.config --cmd get --key key1 Results:\nget key = key1, value = \u0026ldquo;v2\u0026rdquo;\nGet Key Range # Obtain the values of the keys in the ranges [key1, key2]. Do not use this function in your practice code\nkv_service_tools --config config_path --cmd get_key_range --min_key key1 --max_key key2 parameters descriptions config the path of the client config which points to the db entrance cmd get_key_range min_key the minimum key max_key the maximum key Example:\nbazel-bin/service/tools/kv/api_tools/kv_service_tools --config service/tools/config/interface/service.config --cmd get_key_range --min_key key1 --max_key key3 Results:\ngetrange min key = key1, max key = key3 value = [v3,v2,v1]\nDeployment Script # We also provide access to a deployment script that allows deployment on distinct machines.\nDeploy via Docker # Install Docker\nBefore getting started, make sure you have Docker installed on your system. If you don\u0026rsquo;t have Docker already, you can download and install it from the official Docker website.\nPull the Latest ResilientDB Image\nChoose the appropriate ResilientDB image for your machine\u0026rsquo;s architecture:\nFor amd architecture, run:\ndocker pull expolab/resdb:amd64 For Apple Silicon (M1/M2) architecture, run:\ndocker pull expolab/resdb:arm64 Run a Container with the Pulled Image\nLaunch a Docker container using the ResilientDB image you just pulled:\nFor amd architecture, run:\ndocker run -d --name myserver expolab/resdb:amd64 For Apple Silicon (M1/M2) architecture, run:\ndocker run -d --name myserver expolab/resdb:arm64 Test with Set and Get Commands Exec into the running server:\ndocker exec -it myserver bash NOTE: If you encounter a Connection Refused error\nRun the following command within the container:\n./service/tools/kv/server_tools/start_kv_service.sh Verify the functionality of the service by performing set and get operations provided above functions.\nCustom Ports # When starting the service locally, current services are running on 10000 port-base with 5 services where the server config is located here\nIf you want to change the setting, you need to generate the certificates.\nGo the the workspace where the resilientdb repo is localted.\nChange the setting parameters here and run the script:\n./service/tools/kv/server_tools/generate_config.sh Then re-run the start script:\n./service/tools/kv/server_tools/start_kv_service.sh ","date":"7 January 2025","permalink":"/projects/incubator-resilientdb/","section":"Projects","summary":"ResilientDB: Global-Scale Sustainable Blockchain Fabric # ResilientDB is a High Throughput Yielding Permissioned Blockchain Fabric founded by ExpoLab at UC Davis in 2018.","title":"incubator-resilientdb"},{"content":" Vibe Check # ⭐ Do a quick vibe check of a meeting or get some feedback after a presentation\nUsage # Navigate to a url with the parameter event to submit a response.\nhttps://the-vibe-check.vercel.app/?event=Test\nTODO: # Create a state that is the vibe context (e.g. A meeting) so that it can be displayed like vibe for today's meeting so these responses can all get aggregated in the same place.\nCreate a button to create a new page or vibe context, so say the GitHub OAuth user JakeRoggenbuck can create a new page called General Meeting 1 with the random code 867976 that others can visit from the url bar\nCreate a backend for different pages, their questions, and a place to submit the feedback\nCreate a frontend prompt when creating a new page that allows you to make multiple questions that can either be answered with stars or a comment box\nLearning # You can use row-level security to only enable insertions and disable deletions\n","date":"4 January 2025","permalink":"/projects/vibe-check/","section":"Projects","summary":"Vibe Check # ⭐ Do a quick vibe check of a meeting or get some feedback after a presentation","title":"vibe-check"},{"content":" FLUX # by Black Forest Labs: https://blackforestlabs.ai. Documentation for our API can be found here: docs.bfl.ml.\nThis repo contains minimal inference code to run image generation \u0026amp; editing with our Flux models.\nLocal installation # cd $HOME \u0026amp;\u0026amp; git clone https://github.com/black-forest-labs/flux cd $HOME/flux python3.10 -m venv .venv source .venv/bin/activate pip install -e \u0026#34;.[all]\u0026#34; Models # We are offering an extensive suite of models. For more information about the invidual models, please refer to the link under Usage.\nName Usage HuggingFace repo License FLUX.1 [schnell] Text to Image https://huggingface.co/black-forest-labs/FLUX.1-schnell apache-2.0 FLUX.1 [dev] Text to Image https://huggingface.co/black-forest-labs/FLUX.1-dev FLUX.1-dev Non-Commercial License FLUX.1 Fill [dev] In/Out-painting https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev FLUX.1-dev Non-Commercial License FLUX.1 Canny [dev] Structural Conditioning https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev FLUX.1-dev Non-Commercial License FLUX.1 Depth [dev] Structural Conditioning https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev FLUX.1-dev Non-Commercial License FLUX.1 Canny [dev] LoRA Structural Conditioning https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora FLUX.1-dev Non-Commercial License FLUX.1 Depth [dev] LoRA Structural Conditioning https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora FLUX.1-dev Non-Commercial License FLUX.1 Redux [dev] Image variation https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev FLUX.1-dev Non-Commercial License FLUX.1 [pro] Text to Image Available in our API. FLUX1.1 [pro] Text to Image Available in our API. FLUX1.1 [pro] Ultra/raw Text to Image Available in our API. FLUX.1 Fill [pro] In/Out-painting Available in our API. FLUX.1 Canny [pro] Structural Conditioning Available in our API. FLUX.1 Depth [pro] Structural Conditioning Available in our API. FLUX1.1 Redux [pro] Image variation Available in our API. FLUX1.1 Redux [pro] Ultra Image variation Available in our API. The weights of the autoencoder are also released under apache-2.0 and can be found in the HuggingFace repos above.\nAPI usage # Our API offers access to our models. It is documented here: docs.bfl.ml.\nIn this repository we also offer an easy python interface. To use this, you first need to register with the API on api.bfl.ml, and create a new API key.\nTo use the API key either run export BFL_API_KEY=\u0026lt;your_key_here\u0026gt; or provide it via the api_key=\u0026lt;your_key_here\u0026gt; parameter. It is also expected that you have installed the package as above.\nUsage from python:\nfrom flux.api import ImageRequest # this will create an api request directly but not block until the generation is finished request = ImageRequest(\u0026#34;A beautiful beach\u0026#34;, name=\u0026#34;flux.1.1-pro\u0026#34;) # or: request = ImageRequest(\u0026#34;A beautiful beach\u0026#34;, name=\u0026#34;flux.1.1-pro\u0026#34;, api_key=\u0026#34;your_key_here\u0026#34;) # any of the following will block until the generation is finished request.url # -\u0026gt; https:\u0026lt;...\u0026gt;/sample.jpg request.bytes # -\u0026gt; b\u0026#34;...\u0026#34; bytes for the generated image request.save(\u0026#34;outputs/api.jpg\u0026#34;) # saves the sample to local storage request.image # -\u0026gt; a PIL image Usage from the command line:\n$ python -m flux.api --prompt=\u0026#34;A beautiful beach\u0026#34; url https:\u0026lt;...\u0026gt;/sample.jpg # generate and save the result $ python -m flux.api --prompt=\u0026#34;A beautiful beach\u0026#34; save outputs/api # open the image directly $ python -m flux.api --prompt=\u0026#34;A beautiful beach\u0026#34; image show Citation # If you find the provided code or models useful for your research, consider citing them as:\n@misc{flux2023, author={Black Forest Labs}, title={FLUX}, year={2023}, howpublished={\\url{https://github.com/black-forest-labs/flux}}, } ","date":"3 January 2025","permalink":"/projects/black-forest-labs-flux/","section":"Projects","summary":"FLUX # by Black Forest Labs: https://blackforestlabs.","title":"black-forest-labs-flux"},{"content":" PromptWizard 🧙 # Blog Post Project Website PromptWizard: Task-Aware Prompt Optimization Framework\nEshaan Agarwal, Joykirat Singh, Vivek Dani, Raghav Magazine, Tanuja Ganu, Akshay Nambi Overview 🌟 # Overview of the PromptWizard framework\nPromptWizard is a discrete prompt optimization framework that employs a self-evolving mechanism where the LLM generates, critiques, and refines its own prompts and examples, continuously improving through iterative feedback and synthesis. This self-adaptive approach ensures holistic optimization by evolving both the instructions and in-context learning examples for better task performance.\nThree key components of PromptWizard are te following :\nFeedback-driven Refinement: LLM generates, critiques, and refines its own prompts and examples, continuously improving through iterative feedback and synthesis​ Critique and Synthesize diverse examples: Generates synthetic examples that are robust, diverse and task-aware. Also it optimizes both prompt and examples in tandem​ Self generated Chain of Thought (CoT) steps with combination of positive, negative and synthetic examples Stage 1: Iterative optimization of instructions\nStage 2: Sequential optimization of instruction and examples\nInstallation ⬇️ # Follow these steps to set up the development environment and install the package:\nClone the repository\ngit clone https://github.com/microsoft/PromptWizard cd PromptWizard Create and activate a virtual environment\nOn Windows\npython -m venv venv venv\\Scripts\\activate On macOS/Linux:\npython -m venv venv source venv/bin/activate Install the package in development mode:\npip install -e . Quickstart 🏃 # There are three main ways to use PromptWizard:\nScenario 1 : Optimizing prompts without examples Scenario 2 : Generating synthetic examples and using them to optimize prompts Scenario 3 : Optimizing prompts with training data NOTE : Refer this notebook to get a detailed understanding of the usage for each of the scenarios. This serves as a starting point to understand the usage of PromptWizard\nHigh level overview of using PromptWizard # Decide your scenario Fix the configuration and environmental varibles for API calling Use promptopt_config.yaml to set configurations. For example for GSM8k this file can be used Use .env to set environmental varibles. For GSM8k this file can be used USE_OPENAI_API_KEY=\u0026#34;XXXX\u0026#34; # Replace with True/False based on whether or not to use OPENAI API key # If the first variable is set to True then fill the following two OPENAI_API_KEY=\u0026#34;XXXX\u0026#34; OPENAI_MODEL_NAME =\u0026#34;XXXX\u0026#34; # If the first variable is set to False then fill the following three AZURE_OPENAI_ENDPOINT=\u0026#34;XXXXX\u0026#34; # Replace with your Azure OpenAI Endpoint OPENAI_API_VERSION=\u0026#34;XXXX\u0026#34; # Replace with the version of your API AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=\u0026#34;XXXXX\u0026#34; # Create a deployment for the model and place the deployment name here. Run the code To run PromptWizard on your custom dataset please jump here Running PromptWizard with training data (Scenario 3) # We support GSM8k, SVAMP, AQUARAT and Instruction_Induction(BBII) datasets Please note that time taken for prompt optimzation is dependent on the dataset. In our experiments for the above mentioned datasets, it took around 20 - 30 minutes on average. Running on GSM8k (AQUARAT/SVAMP) # Please note that this code requires access to LLMs via API calling for which we support AZURE endpoints or OPENAI keys Set the AZURE endpoint configurations in .env Follow the steps in demo.ipynb to download the data, run the prompt optimization and carry out inference. Running on BBII # BBII has many datasets in it, based on the dataset set the configs here In configs task_description,base_instruction and answer_format need to be changed for different datasets in BBII, the rest of the configs remain the same A demo is presented in demo.ipynb Run on Custom Datasets 🗃️ # Create Custom Dataset # Our code expects the dataset to be in .jsonl file format Both the train and test set follow the same format Every sample in the .jsonl should have 2 fields : question : It should contain the complete question that is to asked to the LLM answer : It should contain the ground truth answer which can be verbose or consize Run on Custom Dataset # NOTE : Refer to demos folder for examples of folders for four datasets. The .ipynb in each of the folders shows how to run PromptWizard on that particular dataset. A similar procedure can be followed for a new dataset. Below is the explanation of each of the components of the .ipynb and the dataset specifc folder structure in detail\nSteps to be followed for custom datasets # Every new dataset needs to have the following\nconfigs folder to store files for defining optimization hyperparameters and setup configs data folder to store train.jsonl and test.jsonl as curated here (this is done in the notebooks) .env file for environment varibles to be used for API calling .py/.ipynb script to run the code Set the hyperparameters like number of mutations, refine steps, in-context examples etc.\nSet the following in promptopt_config.yaml : task_description : Desciption of the task at hand which will be fed into the prompt\nFor GSM8k a description like the following can be used You are a mathematics expert. You will be given a mathematics problem which you need to solve base_instruction : Base instruction in line with the dataset\nA commonly used base instruction could be Lets think step by step. answer_format : Instruction for specifying the answer format\nIt is crucial to set the answer_format properly to ensure correct extraction by def extract_final_answer() Answer format could be : At the end, wrap only your final option between \u0026lt;ANS_START\u0026gt; and \u0026lt;ANS_END\u0026gt; tags Then in def extract_final_answer() we can simply write code to extract string between the tags seen_set_size : The number of train samples to be used for prompt optimization\nIn our experiments we set this to be 25. In general any number between 20-50 would work few_shot_count : The number of in-context examples needed in the prompt\nThe value can be set to any positive integer based on the requirement For generating zero-shot prompts, set the values to a small number (i.e between 2-5) and after the final prompt is generated the in-context examples can be removed. We suggest using some in-context examples as during the optimization process the instructions in the prompt are refined using in-context examples hence setting it to a small number will give better zero-shot instructions in the prompt generate_reasoning : Whether or not to generate reasoning for the in-context examples\nIn our experiments we found it to improve the prompt overall as it provides a step-by-step approach to reach the final answer. However if there is a constraint on the prompt length or number of prompt tokens, it can be turned off to get smaller sized prompts generate_expert_identity and generate_intent_keywords : Having these helped improve the prompt as they help making the prompt relevant to the task\nRefer promptopt_config.yaml files in folders present here for the descriptions used for AQUARAT, SVAMP and GSM8k. For BBII refer description.py which has the meta instructions for each of the datasets Following are the global parameters which can be set based on the availability of the training data run_without_train_examples is a global hyperparameter which can be used when there are no training samples and in-context examples are not required in the final prompt generate_synthetic_examples is a global hyperparameter which can be used when there are no training samples and we want to generate synthetic data for training use_examples is a global hyperparameter which can be used to optimize prompts using training data Create a dataset specific class which inherits class DatasetSpecificProcessing similar to GSM8k(DatasetSpecificProcessing) in demo.ipynb and define the following functions in it\nIn def extract_answer_from_output() : This is a dataset specific function, given the answer from the dataset it should extract and return a consize form of the answer. Note that based on the dataset it can also simply return the answer as it is like in case of SVAMP and AQUARAT datasets def extract_final_answer() : This is a LLM output specific function, given the verbose answer from the LLM it should extract and return the consize final answer Define def access_answer() : This function takes an input the LLM output, then does the following: Extracts the consize answer using def extract_final_answer() from the LLM output as defined above Evaluates the extracted answer with the ground truth and retuns Extracted answer from LLM output Boolean value indicating if answer is correct or not The evaluation done here is dataset specific, for datasets like GSM8k, SVAMP and AQUARAT which have final answer as an number, we can do a direct match between the numbers generated and the ground truth, while for datasets where the answer is a sentence or paragraph it would be better to do evaluation with llm-as-a-judge, to compare the generated and ground truth paragraph/sentence. An example is available in def access_answer() in this notebook How PromptWizard Works 🔍 # Using the problem description and initial prompt instruction, PW generates variations of the instruction by prompting LLMs to mutate it. Based on performance, the best prompt is selected. PW incorporates a critique component that provides feedback, thus guiding and refining the prompt over multiple iterations. PW also optimizes in-context examples. PW selects a diverse set of examples from the training data, identifying positive and negative examples based on their performance with the modified prompt. Negative examples help inform further prompt refinements. Examples and instructions are sequentially optimized, using the critique to generate synthetic examples that address the current prompt’s weaknesses. These examples are integrated to further refine the prompt. PW generates detailed reasoning chains via Chain-of-Thought (CoT), enriching the prompt’s capacity for problem-solving. PW aligns prompts with human reasoning by integrating task intent and expert personas, enhancing both model performance and interpretability. Configurations ⚙️ # Here we define the various hyperparameters used in prompt optimization process found in promptopt_config.yaml\nmutate_refine_iterations: Number of iterations for conducting mutation of task description followed by refinement of instructions mutation_rounds: Number of rounds of mutation to be performed when generating different styles refine_task_eg_iterations: Number of iterations for refining task description and in context examples style_variation: Number of thinking style variations to be used in prompt mutation questions_batch_size: Number of questions to be asked to LLM in a single batch, during training step min_correct_count: Minimum number of batches of questions to correctly answered, for a prompt to be considered as performing good max_eval_batches: Maximum number of mini-batches on which we should evaluate the prompt top_n: Number of top best prompts to be considered from scoring stage for the next stage seen_set_size: Number of samples from trainset to be used for training few_shot_count: Number of in-context examples required in final prompt Best Practices 💡 # Following are some of best pracitices we followed during are experiments\nRegarding the parameters in promptopt_config.yaml We found the best performing values for mutate_refine_iterations,mutation_rounds,refine_task_eg_iterations to be 3 or 5 Other parameters have been set to their ideal values. seen_set_size can be increased to 50 and few_shot_count can be set based on the use case The prompts generated at the end of the training process are usually very detailed, however user supervision can help tune it further for the task at hand Trying both configurations of having synthetic in-context examples or in-context examples from the train set can be tried to find the best prompt based on use case. Results 📈 # PromptWizard consistently outperforms other methods across various thresholds, maintaining the highest p(τ) values, indicating that it consistently performs near the best possible accuracy across all tasks\nThe fiqure shows the performance profile curve for the instruction induction tasks. The performance profile curve visualizes how frequently different approaches’ performance is within a given distance of the best performance. In this curve, the x-axis (τ) represents the performance ratio relative to the best-performing method, and the y-axis (p(τ )) reflects the fraction of tasks where a method’s performance is within this ratio. So for a given method, the curve tells what percentage of the tasks are within τ distance to the best performance. How to contribute: ✋ # This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com. When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our CLA. This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.\nCitation 📝 # If you make use of our work, please cite our paper:\n@misc{agarwal2024promptwizardtaskawarepromptoptimization, title={PromptWizard: Task-Aware Prompt Optimization Framework}, author={Eshaan Agarwal and Joykirat Singh and Vivek Dani and Raghav Magazine and Tanuja Ganu and Akshay Nambi}, year={2024}, eprint={2405.18369}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2405.18369}, } Responsible AI Considerations # For guidelines and best practices related to Responsible AI, please refer to our Responsible AI Guidelines.\n","date":"2 January 2025","permalink":"/projects/promptwizard/","section":"Projects","summary":"PromptWizard 🧙 # Blog Post Project Website PromptWizard: Task-Aware Prompt Optimization Framework","title":"PromptWizard"},{"content":" ml-compression # Automatically create compression schemes convolutional neural networks. This is also known as an Auto Encoder.\nDescription # Train a neural network to have its output reproduce the exact input. Why is this interesting? You can constrict the quantity of neurons in one of the intermediate layer and use this as a stand-in for the data that was input. This then means that you have effectively compressed this data.\nThese types of models can help us generate optimal compression schemes.\nSetup # pip install -r requirements.txt Running # python3 main.py Results # TODO: # Restrict pixels instead of just channels in the convolutions ","date":"27 December 2024","permalink":"/projects/ml-compression/","section":"Projects","summary":"ml-compression # Automatically create compression schemes convolutional neural networks.","title":"ml-compression"},{"content":" Self Referential Hashes # #️⃣ This program finds strings that include parts of hashes H, that when hashed, the output hash starts with H.\nMD5(Jake's MD5 hash = 109e5a72e) = 109e5a72eea4eaa2c1bf94edcecb2fb0\nHow # I am using a program in Rust that is multithreaded and tries all possibilities.\nWhy # This is kinda interesting and it\u0026rsquo;s a fun thing to have a hash like Jake's MD5 hash = 109e5a72e that makes people think. And the larger the hash, the less easily it can be found by others, so a hash with maybe 10+ hash characters might be almost impossible for most people to find, making it kinda like a trophy of multithreading and optimization.\n","date":"6 December 2024","permalink":"/projects/self-referential-hashes/","section":"Projects","summary":"Self Referential Hashes # #️⃣ This program finds strings that include parts of hashes H, that when hashed, the output hash starts with H.","title":"self-referential-hashes"},{"content":" PyTorch Examples # https://pytorch.org/examples/\npytorch/examples is a repository showcasing examples of using PyTorch. The goal is to have curated, short, few/no dependencies high quality examples that are substantially different from each other that can be emulated in your existing work.\nFor tutorials: https://github.com/pytorch/tutorials For changes to pytorch.org: https://github.com/pytorch/pytorch.github.io For a general model hub: https://pytorch.org/hub/ or https://huggingface.co/models For recipes on how to run PyTorch in production: https://github.com/facebookresearch/recipes For general Q\u0026amp;A and support: https://discuss.pytorch.org/ Available models # Image classification (MNIST) using Convnets Word-level Language Modeling using RNN and Transformer Training Imagenet Classifiers with Popular Networks Generative Adversarial Networks (DCGAN) Variational Auto-Encoders Superresolution using an efficient sub-pixel convolutional neural network Hogwild training of shared ConvNets across multiple processes on MNIST Training a CartPole to balance in OpenAI Gym with actor-critic Natural Language Inference (SNLI) with GloVe vectors, LSTMs, and torchtext Time sequence prediction - use an LSTM to learn Sine waves Implement the Neural Style Transfer algorithm on images Reinforcement Learning with Actor Critic and REINFORCE algorithms on OpenAI gym PyTorch Module Transformations using fx Distributed PyTorch examples with Distributed Data Parallel and RPC Several examples illustrating the C++ Frontend Image Classification Using Forward-Forward Language Translation using Transformers Additionally, a list of good examples hosted in their own repositories:\nNeural Machine Translation using sequence-to-sequence RNN with attention (OpenNMT) Contributing # If you\u0026rsquo;d like to contribute your own example or fix a bug please make sure to take a look at CONTRIBUTING.md.\n","date":"30 November 2024","permalink":"/projects/pytorch-examples/","section":"Projects","summary":"PyTorch Examples # https://pytorch.","title":"pytorch-examples"},{"content":" Cowculator - v2 # 🐮 Calculate the enteric methane emissions from cows based on their food\n🥇 1st Place Winner of UC Davis Climate Pitch Competition\nSecond version of cowculator\n","date":"9 November 2024","permalink":"/projects/cowculator-v2/","section":"Projects","summary":"Cowculator - v2 # 🐮 Calculate the enteric methane emissions from cows based on their food","title":"cowculator-v2"},{"content":" Simulate a CPU and its instructions\nDemo # ","date":"9 November 2024","permalink":"/projects/cpu-simulator/","section":"Projects","summary":"Simulate a CPU and its instructions","title":"cpu-simulator"},{"content":" Getting Started | Deployment | Documentation and Support | Blog | License Pathway # Pathway is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.\nPathway comes with an easy-to-use Python API, allowing you to seamlessly integrate your favorite Python ML libraries. Pathway code is versatile and robust: you can use it in both development and production environments, handling both batch and streaming data effectively. The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.\nPathway is powered by a scalable Rust engine based on Differential Dataflow and performs incremental computation. Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations. All the pipeline is kept in memory and can be easily deployed with Docker and Kubernetes.\nYou can install Pathway with pip:\npip install -U pathway For any questions, you will find the community and team behind the project on Discord.\nUse-cases and templates # Ready to see what Pathway can do?\nTry one of our easy-to-run examples!\nAvailable in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!\nEvent processing and real-time analytics pipelines # With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It\u0026rsquo;s the ideal solution for a wide range of data processing pipelines, including:\nShowcase: Real-time ETL. Showcase: Event-driven pipelines with alerting. Showcase: Realtime analytics. Docs: Switch from batch to streaming. Live LLM and RAG pipelines # Pathway provides dedicated LLM tooling to build LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our LLM xpack documentation.\nDon\u0026rsquo;t hesitate to try one of our runnable examples featuring LLM tooling. You can find such examples here.\nTemplate: Unstructured data to SQL on-the-fly. Template: Private RAG with Ollama and Mistral AI Template: Adaptive RAG Template: Multimodal RAG with gpt-4o Features # A wide range of connectors: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector. Stateless and stateful transformations: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data. Persistence: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway! Consistency: Pathway handles the time for you, making your all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the \u0026ldquo;at least once\u0026rdquo; consistency while the enterprise version provides the \u0026ldquo;exactly once\u0026rdquo; consistency. Scalable Rust engine: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations. LLM helpers: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents. Getting started # Installation # Pathway requires Python 3.10 or above.\nYou can install the current release of Pathway using pip:\n$ pip install -U pathway ⚠️ Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.\nExample: computing the sum of positive values in real time. # import pathway as pw # Define the schema of your data (Optional) class InputSchema(pw.Schema): value: int # Connect to your data using connectors input_table = pw.io.csv.read( \u0026#34;./input/\u0026#34;, schema=InputSchema ) #Define your operations on the data filtered_table = input_table.filter(input_table.value\u0026gt;=0) result_table = filtered_table.reduce( sum_value = pw.reducers.sum(filtered_table.value) ) # Load your results to external systems pw.io.jsonlines.write(result_table, \u0026#34;output.jsonl\u0026#34;) # Run the computation pw.run() Run Pathway in Google Colab.\nYou can find more examples here.\nDeployment # Locally # To use Pathway, you only need to import it:\nimport pathway as pw Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:\npw.run() You can then run your Pathway project (say, main.py) just like a normal Python script: $ python main.py. Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages.\nAlternatively, you can use the pathway\u0026rsquo;ish version:\n$ pathway spawn python main.py Pathway natively supports multithreading. To launch your application with 3 threads, you can do as follows:\n$ pathway spawn --threads 3 python main.py To jumpstart a Pathway project, you can use our cookiecutter template.\nDocker # You can easily run Pathway using docker.\nPathway image # You can use the Pathway docker image, using a Dockerfile:\nFROM pathwaycom/pathway:latest WORKDIR /app COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt COPY . . CMD [ \u0026#34;python\u0026#34;, \u0026#34;./your-script.py\u0026#34; ] You can then build and run the Docker image:\ndocker build -t my-pathway-app . docker run -it --rm --name my-pathway-app my-pathway-app Run a single Python script # When dealing with single-file projects, creating a full-fledged Dockerfile might seem unnecessary. In such scenarios, you can execute a Python script directly using the Pathway Docker image. For example:\ndocker run -it --rm --name my-pathway-app -v \u0026#34;$PWD\u0026#34;:/app pathwaycom/pathway:latest python my-pathway-app.py Python docker image # You can also use a standard Python image and install Pathway using pip with a Dockerfile:\nFROM --platform=linux/x86_64 python:3.10 RUN pip install -U pathway COPY ./pathway-script.py pathway-script.py CMD [\u0026#34;python\u0026#34;, \u0026#34;-u\u0026#34;, \u0026#34;pathway-script.py\u0026#34;] Kubernetes and cloud # Docker containers are ideally suited for deployment on the cloud with Kubernetes. If you want to scale your Pathway application, you may be interested in our Pathway for Enterprise. Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics. It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.\nYou can easily deploy Pathway using services like Render: see how to deploy Pathway in a few clicks.\nIf you are interested, don\u0026rsquo;t hesitate to contact us to learn more.\nPerformance # Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF\u0026rsquo;s in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).\nIf you are curious, here are some benchmarks to play with.\nDocumentation and Support # The entire documentation of Pathway is available at pathway.com/developers/, including the API Docs.\nIf you have any question, don\u0026rsquo;t hesitate to open an issue on GitHub, join us on Discord, or send us an email at contact@pathway.com.\nLicense # Pathway is distributed on a BSL 1.1 License which allows for unlimited non-commercial use, as well as use of the Pathway package for most commercial purposes, free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some public repos which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.\nContribution guidelines # If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license.\nFor all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don\u0026rsquo;t hesitate to engage with Pathway\u0026rsquo;s Discord community.\n","date":"6 November 2024","permalink":"/projects/pathway/","section":"Projects","summary":"Getting Started | Deployment | Documentation and Support | Blog | License Pathway # Pathway is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.","title":"pathway"},{"content":" Cowculator - v1 # 🐮 Calculate the enteric methane emissions from cows based on their food\n🥇 1st Place Winner of UC Davis Climate Pitch Competition\nEarly version of cowculator-v2\n","date":"1 November 2024","permalink":"/projects/cowculator/","section":"Projects","summary":"Cowculator - v1 # 🐮 Calculate the enteric methane emissions from cows based on their food","title":"cowculator"},{"content":"\n🎉 Announcement: The new Explore Profiles UI is here! # We are thrilled to announce the launch of the Explore Profiles UI, a brand-new way to explore and analyze your profiling data—now available as part of the Grafana Explore Apps suite! This new app brings you a queryless, intuitive experience for visualizing your profiling data, simplifying the entire process without the need to write complex queries.\nhttps://github.com/user-attachments/assets/4db19ec7-86f3-4701-8f5f-9b7ffcebd49c\nWhat is Grafana Pyroscope? # Grafana Pyroscope is a continuous profiling platform designed to surface performance insights from your applications, helping you optimize resource usage such as CPU, memory, and I/O operations. With Pyroscope, you can both proactively and reactively address performance bottlenecks across your system.\nThe typical use cases are:\nProactive: Reducing resource consumption, improving application performance, or preventing latency issues. Reactive: Quickly resolving incidents with line-level detail and debugging active CPU, memory, or I/O bottlenecks. Pyroscope provides powerful tools to give you a comprehensive view of your application\u0026rsquo;s behavior while allowing you to drill down into specific services for more targeted root cause analysis.\nHow Does Pyroscope Work? # Pyroscope consists of three main components:\nPyroscope Server: The server component that stores and processes profiling data. Pyroscope SDKs(push) or Grafana alloy(pull) : The client-side part of Pyroscope that collects profiling data from your applications and sends it to the server. Explore Profiles UI: A queryless, intuitive UI for visualizing and analyzing profiling data. Pyroscope Live Demo # Quick Start: Run Pyroscope server locally # Homebrew # brew install pyroscope-io/brew/pyroscope brew services start pyroscope Docker # docker run -it -p 4040:4040 grafana/pyroscope For more documentation on how to configure Pyroscope server, see our server documentation.\nQuick Start: Run Explore Profiles UI in Grafana # Grafana Cloud # The app UI and server are both installed and running auomatically \u0026ndash; just start sending data!\nGrafana OSS # You can run the Explore profiles UI in Grafana by installing the plugin from the Grafana Plugin Directory\nFor more information, check out the Explore Profiles README\nDocumentation # For more information on how to use Pyroscope with other programming languages, install it on Linux, or use it in production environment, check out our documentation:\nGetting Started Deployment Guide Pyroscope Architecture Send data to server via Pyroscope agent (language specific) # For more documentation on how to add the Pyroscope agent to your code, see the agent documentation on our website or find language specific examples and documentation below:\nGolang\nDocumentation\nExamples Java\nDocumentation\nExamples Python\nDocumentation\nExamples Ruby\nDocumentation\nExamples Node.js\nDocumentation\nExamples Dotnet\nDocumentation\nExamples eBPF\nDocumentation\nExamples Rust\nDocumentation\nExamples Supported Languages # Our documentation contains the most recent list of supported languages and also an overview over what profiling types are supported per language.\nLet us know what other integrations you want to see in our issues or in our slack.\nCredits # Pyroscope is possible thanks to the excellent work of many people, including but not limited to:\nBrendan Gregg — inventor of Flame Graphs Julia Evans — creator of rbspy — sampling profiler for Ruby Vladimir Agafonkin — creator of flamebearer — fast flame graph renderer Ben Frederickson — creator of py-spy — sampling profiler for Python Adam Saponara — creator of phpspy — sampling profiler for PHP Alexei Starovoitov, Brendan Gregg, and many others who made BPF based profiling in Linux kernel possible Jamie Wong — creator of speedscope — interactive flame graph visualizer Contributing # To start contributing, check out our Contributing Guide\nThanks to the contributors of Pyroscope! # ","date":"25 October 2024","permalink":"/projects/pyroscope/","section":"Projects","summary":"","title":"pyroscope"},{"content":" event-attendance # Match provided email on an application with ones who filled out an event attendance form\nInstall Deno # https://docs.deno.com/runtime/getting_started/installation/\nUsage # deno run -A main.ts --event_path=\u0026lt;event.csv\u0026gt; --applications_path=\u0026lt;app.csv\u0026gt; About # For this project, I used TypeScript and the Deno runtime\nTesting # deno test What did I learn from this project? # \u0026ldquo;top-level async\u0026rdquo; is when you have an async in main Using the .split() method is the same as Python You can .pop() an array, and it\u0026rsquo;ll remove the last item like in Rust Using a forEach is useful All async functions return a Promise\u0026lt;Type\u0026gt; You need to --allow-read={filename} to allow reads to files You can use assertThrows to test if a function correctly throws and Error Use slice to get a piece of an array like you would with [i:j] in Python Sets can contain strings but you need to create them with new Set\u0026lt;string\u0026gt;([]); Sets with strings to not have the .intersection method unfortunately ","date":"15 October 2024","permalink":"/projects/event-attendance/","section":"Projects","summary":"event-attendance # Match provided email on an application with ones who filled out an event attendance form","title":"event-attendance"},{"content":" A small programming language with postfix notation using TypeScript and Deno.\nSyntax # Postfix # In postfix for an operation of two arguments, you add two numbers to the stack, then the operator will pop the arguments off the stack. So adding 1 to 2 is the following.\n1 2 + Operators # + addition - subtraction * multiply / divide Built-in Functions # sin sine function cos cosine function tan tangent function Built-in Constants # e Euler\u0026rsquo;s number pi Pi Equations # Unit-language can evaluate more complicated expressions too.\n2 3 + 9 - 8 * sin 5 - Running # deno run dev Running Tests # deno test Running list # deno lint Testings Running # ","date":"12 October 2024","permalink":"/projects/unit-language/","section":"Projects","summary":"A small programming language with postfix notation using TypeScript and Deno.","title":"unit-language"},{"content":" Miri # Miri is an Undefined Behavior detection tool for Rust. It can run binaries and test suites of cargo projects and detect unsafe code that fails to uphold its safety requirements. For instance:\nOut-of-bounds memory accesses and use-after-free Invalid use of uninitialized data Violation of intrinsic preconditions (an unreachable_unchecked being reached, calling copy_nonoverlapping with overlapping ranges, \u0026hellip;) Not sufficiently aligned memory accesses and references Violation of basic type invariants (a bool that is not 0 or 1, for example, or an invalid enum discriminant) Experimental: Violations of the Stacked Borrows rules governing aliasing for reference types Experimental: Violations of the Tree Borrows aliasing rules, as an optional alternative to Stacked Borrows Experimental: Data races and emulation of weak memory effects, i.e., atomic reads can return outdated values. On top of that, Miri will also tell you about memory leaks: when there is memory still allocated at the end of the execution, and that memory is not reachable from a global static, Miri will raise an error.\nYou can use Miri to emulate programs on other targets, e.g. to ensure that byte-level data manipulation works correctly both on little-endian and big-endian systems. See cross-interpretation below.\nMiri has already discovered many real-world bugs. If you found a bug with Miri, we\u0026rsquo;d appreciate if you tell us and we\u0026rsquo;ll add it to the list!\nBy default, Miri ensures a fully deterministic execution and isolates the program from the host system. Some APIs that would usually access the host, such as gathering entropy for random number generators, environment variables, and clocks, are replaced by deterministic \u0026ldquo;fake\u0026rdquo; implementations. Set MIRIFLAGS=\u0026quot;-Zmiri-disable-isolation\u0026quot; to access the real system APIs instead. (In particular, the \u0026ldquo;fake\u0026rdquo; system RNG APIs make Miri not suited for cryptographic use! Do not generate keys using Miri.)\nAll that said, be aware that Miri does not catch every violation of the Rust specification in your program, not least because there is no such specification. Miri uses its own approximation of what is and is not Undefined Behavior in Rust. To the best of our knowledge, all Undefined Behavior that has the potential to affect a program\u0026rsquo;s correctness is being detected by Miri (modulo bugs), but you should consult the Reference for the official definition of Undefined Behavior. Miri will be updated with the Rust compiler to protect against UB as it is understood by the current compiler, but it makes no promises about future versions of rustc.\nFurther caveats that Miri users should be aware of:\nIf the program relies on unspecified details of how data is laid out, it will still run fine in Miri \u0026ndash; but might break (including causing UB) on different compiler versions or different platforms. (You can use -Zrandomize-layout to detect some of these cases.) Program execution is non-deterministic when it depends, for example, on where exactly in memory allocations end up, or on the exact interleaving of concurrent threads. Miri tests one of many possible executions of your program, but it will miss bugs that only occur in a different possible execution. You can alleviate this to some extent by running Miri with different values for -Zmiri-seed, but that will still by far not explore all possible executions. Miri runs the program as a platform-independent interpreter, so the program has no access to most platform-specific APIs or FFI. A few APIs have been implemented (such as printing to stdout, accessing environment variables, and basic file system access) but most have not: for example, Miri currently does not support networking. System API support varies between targets; if you run on Windows it is a good idea to use --target x86_64-unknown-linux-gnu to get better support. Weak memory emulation is not complete: there are legal behaviors that Miri will never produce. However, Miri produces many behaviors that are hard to observe on real hardware, so it can help quite a bit in finding weak memory concurrency bugs. To be really sure about complicated atomic code, use specialized tools such as loom. Moreover, Miri fundamentally cannot ensure that your code is sound. Soundness is the property of never causing undefined behavior when invoked from arbitrary safe code, even in combination with other sound code. In contrast, Miri can just tell you if a particular way of interacting with your code (e.g., a test suite) causes any undefined behavior in a particular execution (of which there may be many, e.g. when concurrency or other forms of non-determinism are involved). When Miri finds UB, your code is definitely unsound, but when Miri does not find UB, then you may just have to test more inputs or more possible non-deterministic choices.\nUsing Miri # Install Miri on Rust nightly via rustup:\nrustup +nightly component add miri All the following commands assume the nightly toolchain is pinned via rustup override set nightly. Alternatively, use cargo +nightly for each of the following commands.\nNow you can run your project in Miri:\nTo run all tests in your project through Miri, use cargo miri test. If you have a binary project, you can run it through Miri using cargo miri run. The first time you run Miri, it will perform some extra setup and install some dependencies. It will ask you for confirmation before installing anything.\ncargo miri run/test supports the exact same flags as cargo run/test. For example, cargo miri test filter only runs the tests containing filter in their name.\nYou can pass flags to Miri via MIRIFLAGS. For example, MIRIFLAGS=\u0026quot;-Zmiri-disable-stacked-borrows\u0026quot; cargo miri run runs the program without checking the aliasing of references.\nWhen compiling code via cargo miri, the cfg(miri) config flag is set for code that will be interpreted under Miri. You can use this to ignore test cases that fail under Miri because they do things Miri does not support:\n#[test] #[cfg_attr(miri, ignore)] fn does_not_work_on_miri() { tokio::run(futures::future:🆗:\u0026lt;_, ()\u0026gt;(())); } There is no way to list all the infinite things Miri cannot do, but the interpreter will explicitly tell you when it finds something unsupported:\nerror: unsupported operation: can\u0026#39;t call foreign function: bind ... = help: this is likely not a bug in the program; it indicates that the program \\ performed an operation that Miri does not support Cross-interpretation: running for different targets # Miri can not only run a binary or test suite for your host target, it can also perform cross-interpretation for arbitrary foreign targets: cargo miri run --target x86_64-unknown-linux-gnu will run your program as if it was a Linux program, no matter your host OS. This is particularly useful if you are using Windows, as the Linux target is much better supported than Windows targets.\nYou can also use this to test platforms with different properties than your host platform. For example cargo miri test --target s390x-unknown-linux-gnu will run your test suite on a big-endian target, which is useful for testing endian-sensitive code.\nTesting multiple different executions # Certain parts of the execution are picked randomly by Miri, such as the exact base address allocations are stored at and the interleaving of concurrently executing threads. Sometimes, it can be useful to explore multiple different execution, e.g. to make sure that your code does not depend on incidental \u0026ldquo;super-alignment\u0026rdquo; of new allocations and to test different thread interleavings. This can be done with the -Zmiri-many-seeds flag:\nMIRIFLAGS=\u0026#34;-Zmiri-many-seeds\u0026#34; cargo miri test # tries the seeds in 0..64 MIRIFLAGS=\u0026#34;-Zmiri-many-seeds=0..16\u0026#34; cargo miri test The default of 64 different seeds can be quite slow, so you often want to specify a smaller range.\nRunning Miri on CI # When running Miri on CI, use the following snippet to install a nightly toolchain with the Miri component:\nrustup toolchain install nightly --component miri rustup override set nightly cargo miri test Here is an example job for GitHub Actions:\nmiri: name: \u0026#34;Miri\u0026#34; runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Install Miri run: | rustup toolchain install nightly --component miri rustup override set nightly cargo miri setup - name: Test with Miri run: cargo miri test The explicit cargo miri setup helps to keep the output of the actual test step clean.\nSupported targets # Miri does not support all targets supported by Rust. The good news, however, is that no matter your host OS/platform, it is easy to run code for any target using --target!\nThe following targets are tested on CI and thus should always work (to the degree documented below):\nAll Rust Tier 1 targets are supported by Miri. They are all checked on Miri\u0026rsquo;s CI, and some (at least one per OS) are even checked on every Rust PR, so the shipped Miri should always work on these targets. s390x-unknown-linux-gnu is supported as our \u0026ldquo;big-endian target of choice\u0026rdquo;. For every other target with OS linux, macos, or windows, Miri should generally work, but we make no promises and we don\u0026rsquo;t run tests for such targets. We have unofficial support (not maintained by the Miri team itself) for some further operating systems. solaris / illumos: maintained by @devnexen. Supports the entire test suite. freebsd: maintained by @YohDeadfall. Supports std::env and parts of std::{thread, fs}, but not std::sync. android: maintainer wanted. Support very incomplete, but a basic \u0026ldquo;hello world\u0026rdquo; works. wasi: maintainer wanted. Support very incomplete, not even standard output works, but an empty main function works. For targets on other operating systems, Miri might fail before even reaching the main function. However, even for targets that we do support, the degree of support for accessing platform APIs (such as the file system) differs between targets: generally, Linux targets have the best support, and macOS targets are usually on par. Windows is supported less well.\nRunning tests in parallel # Though it implements Rust threading, Miri itself is a single-threaded interpreter. This means that when running cargo miri test, you will probably see a dramatic increase in the amount of time it takes to run your whole test suite due to the inherent interpreter slowdown and a loss of parallelism.\nYou can get your test suite\u0026rsquo;s parallelism back by running cargo miri nextest run -jN (note that you will need cargo-nextest installed). This works because cargo-nextest collects a list of all tests then launches a separate cargo miri run for each test. You will need to specify a -j or --test-threads; by default cargo miri nextest run runs one test at a time. For more details, see the cargo-nextest Miri documentation.\nNote: This one-test-per-process model means that cargo miri test is able to detect data races where two tests race on a shared resource, but cargo miri nextest run will not detect such races.\nNote: cargo-nextest does not support doctests, see https://github.com/nextest-rs/nextest/issues/16\nCommon Problems # When using the above instructions, you may encounter a number of confusing compiler errors.\n\u0026ldquo;note: run with RUST_BACKTRACE=1 environment variable to display a backtrace\u0026rdquo; # You may see this when trying to get Miri to display a backtrace. By default, Miri doesn\u0026rsquo;t expose any environment to the program, so running RUST_BACKTRACE=1 cargo miri test will not do what you expect.\nTo get a backtrace, you need to disable isolation using -Zmiri-disable-isolation:\nRUST_BACKTRACE=1 MIRIFLAGS=\u0026#34;-Zmiri-disable-isolation\u0026#34; cargo miri test \u0026ldquo;found crate std compiled by an incompatible version of rustc\u0026rdquo; # You may be running cargo miri with a different compiler version than the one used to build the custom libstd that Miri uses, and Miri failed to detect that. Try running cargo miri clean.\nMiri -Z flags and environment variables # Miri adds its own set of -Z flags, which are usually set via the MIRIFLAGS environment variable. We first document the most relevant and most commonly used flags:\n-Zmiri-address-reuse-rate=\u0026lt;rate\u0026gt; changes the probability that a freed non-stack allocation will be added to the pool for address reuse, and the probability that a new non-stack allocation will be taken from the pool. Stack allocations never get added to or taken from the pool. The default is 0.5. -Zmiri-address-reuse-cross-thread-rate=\u0026lt;rate\u0026gt; changes the probability that an allocation which attempts to reuse a previously freed block of memory will also consider blocks freed by other threads. The default is 0.1, which means by default, in 90% of the cases where an address reuse attempt is made, only addresses from the same thread will be considered. Reusing an address from another thread induces synchronization between those threads, which can mask data races and weak memory bugs. -Zmiri-compare-exchange-weak-failure-rate=\u0026lt;rate\u0026gt; changes the failure rate of compare_exchange_weak operations. The default is 0.8 (so 4 out of 5 weak ops will fail). You can change it to any value between 0.0 and 1.0, where 1.0 means it will always fail and 0.0 means it will never fail. Note that setting it to 1.0 will likely cause hangs, since it means programs using compare_exchange_weak cannot make progress. -Zmiri-disable-isolation disables host isolation. As a consequence, the program has access to host resources such as environment variables, file systems, and randomness. This overwrites a previous -Zmiri-isolation-error. -Zmiri-disable-leak-backtraces disables backtraces reports for memory leaks. By default, a backtrace is captured for every allocation when it is created, just in case it leaks. This incurs some memory overhead to store data that is almost never used. This flag is implied by -Zmiri-ignore-leaks. -Zmiri-env-forward=\u0026lt;var\u0026gt; forwards the var environment variable to the interpreted program. Can be used multiple times to forward several variables. Execution will still be deterministic if the value of forwarded variables stays the same. Has no effect if -Zmiri-disable-isolation is set. -Zmiri-env-set=\u0026lt;var\u0026gt;=\u0026lt;value\u0026gt; sets the var environment variable to value in the interpreted program. It can be used to pass environment variables without needing to alter the host environment. It can be used multiple times to set several variables. If -Zmiri-disable-isolation or -Zmiri-env-forward is set, values set with this option will have priority over values from the host environment. -Zmiri-ignore-leaks disables the memory leak checker, and also allows some remaining threads to exist when the main thread exits. -Zmiri-isolation-error=\u0026lt;action\u0026gt; configures Miri\u0026rsquo;s response to operations requiring host access while isolation is enabled. abort, hide, warn, and warn-nobacktrace are the supported actions. The default is to abort, which halts the machine. Some (but not all) operations also support continuing execution with a \u0026ldquo;permission denied\u0026rdquo; error being returned to the program. warn prints a full backtrace each time that happens; warn-nobacktrace is less verbose and shown at most once per operation. hide hides the warning entirely. This overwrites a previous -Zmiri-disable-isolation. -Zmiri-many-seeds=[\u0026lt;from\u0026gt;]..\u0026lt;to\u0026gt; runs the program multiple times with different seeds for Miri\u0026rsquo;s RNG. With different seeds, Miri will make different choices to resolve non-determinism such as the order in which concurrent threads are scheduled, or the exact addresses assigned to allocations. This is useful to find bugs that only occur under particular interleavings of concurrent threads, or that otherwise depend on non-determinism. If the \u0026lt;from\u0026gt; part is skipped, it defaults to 0. Can be used without a value; in that case the range defaults to 0..64. -Zmiri-many-seeds-keep-going tells Miri to really try all the seeds in the given range, even if a failing seed has already been found. This is useful to determine which fraction of seeds fails. -Zmiri-num-cpus states the number of available CPUs to be reported by miri. By default, the number of available CPUs is 1. Note that this flag does not affect how miri handles threads in any way. -Zmiri-permissive-provenance disables the warning for integer-to-pointer casts and ptr::with_exposed_provenance. This will necessarily miss some bugs as those operations are not efficiently and accurately implementable in a sanitizer, but it will only miss bugs that concern memory/pointers which is subject to these operations. -Zmiri-preemption-rate configures the probability that at the end of a basic block, the active thread will be preempted. The default is 0.01 (i.e., 1%). Setting this to 0 disables preemption. -Zmiri-report-progress makes Miri print the current stacktrace every now and then, so you can tell what it is doing when a program just keeps running. You can customize how frequently the report is printed via -Zmiri-report-progress=\u0026lt;blocks\u0026gt;, which prints the report every N basic blocks. -Zmiri-seed=\u0026lt;num\u0026gt; configures the seed of the RNG that Miri uses to resolve non-determinism. This RNG is used to pick base addresses for allocations, to determine preemption and failure of compare_exchange_weak, and to control store buffering for weak memory emulation. When isolation is enabled (the default), this is also used to emulate system entropy. The default seed is 0. You can increase test coverage by running Miri multiple times with different seeds. -Zmiri-strict-provenance enables strict provenance checking in Miri. This means that casting an integer to a pointer will stop execution because the provenance of the pointer cannot be determined. -Zmiri-symbolic-alignment-check makes the alignment check more strict. By default, alignment is checked by casting the pointer to an integer, and making sure that is a multiple of the alignment. This can lead to cases where a program passes the alignment check by pure chance, because things \u0026ldquo;happened to be\u0026rdquo; sufficiently aligned \u0026ndash; there is no UB in this execution but there would be UB in others. To avoid such cases, the symbolic alignment check only takes into account the requested alignment of the relevant allocation, and the offset into that allocation. This avoids missing such bugs, but it also incurs some false positives when the code does manual integer arithmetic to ensure alignment. (The standard library align_to method works fine in both modes; under symbolic alignment it only fills the middle slice when the allocation guarantees sufficient alignment.) The remaining flags are for advanced use only, and more likely to change or be removed. Some of these are unsound, which means they can lead to Miri failing to detect cases of undefined behavior in a program.\n-Zmiri-disable-alignment-check disables checking pointer alignment, so you can focus on other failures, but it means Miri can miss bugs in your program. Using this flag is unsound. -Zmiri-disable-data-race-detector disables checking for data races. Using this flag is unsound. This implies -Zmiri-disable-weak-memory-emulation. -Zmiri-disable-stacked-borrows disables checking the experimental aliasing rules to track borrows ( Stacked Borrows and Tree Borrows). This can make Miri run faster, but it also means no aliasing violations will be detected. Using this flag is unsound (but the affected soundness rules are experimental). Later flags take precedence: borrow tracking can be reactivated by -Zmiri-tree-borrows. -Zmiri-disable-validation disables enforcing validity invariants, which are enforced by default. This is mostly useful to focus on other failures (such as out-of-bounds accesses) first. Setting this flag means Miri can miss bugs in your program. However, this can also help to make Miri run faster. Using this flag is unsound. -Zmiri-disable-weak-memory-emulation disables the emulation of some C++11 weak memory effects. -Zmiri-native-lib=\u0026lt;path to a shared object file\u0026gt; is an experimental flag for providing support for calling native functions from inside the interpreter via FFI. The flag is supported only on Unix systems. Functions not provided by that file are still executed via the usual Miri shims. WARNING: If an invalid/incorrect .so file is specified, this can cause Undefined Behavior in Miri itself! And of course, Miri cannot do any checks on the actions taken by the native code. Note that Miri has its own handling of file descriptors, so if you want to replace some functions working on file descriptors, you will have to replace all of them, or the two kinds of file descriptors will be mixed up. This is work in progress; currently, only integer and pointers arguments and return values are supported and memory allocated by the native code cannot be accessed from Rust (only the other way around). Native code must not spawn threads that keep running in the background after the call has returned to Rust and that access Rust-allocated memory. Finally, the flag is unsound in the sense that Miri stops tracking details such as initialization and provenance on memory shared with native code, so it is easily possible to write code that has UB which is missed by Miri. -Zmiri-measureme=\u0026lt;name\u0026gt; enables measureme profiling for the interpreted program. This can be used to find which parts of your program are executing slowly under Miri. The profile is written out to a file inside a directory called \u0026lt;name\u0026gt;, and can be processed using the tools in the repository https://github.com/rust-lang/measureme. -Zmiri-mute-stdout-stderr silently ignores all writes to stdout and stderr, but reports to the program that it did actually write. This is useful when you are not interested in the actual program\u0026rsquo;s output, but only want to see Miri\u0026rsquo;s errors and warnings. -Zmiri-recursive-validation is a highly experimental flag that makes validity checking recurse below references. -Zmiri-retag-fields[=\u0026lt;all|none|scalar\u0026gt;] controls when Stacked Borrows retagging recurses into fields. all means it always recurses (the default, and equivalent to -Zmiri-retag-fields without an explicit value), none means it never recurses, scalar means it only recurses for types where we would also emit noalias annotations in the generated LLVM IR (types passed as individual scalars or pairs of scalars). Setting this to none is unsound. -Zmiri-provenance-gc=\u0026lt;blocks\u0026gt; configures how often the pointer provenance garbage collector runs. The default is to search for and remove unreachable provenance once every 10000 basic blocks. Setting this to 0 disables the garbage collector, which causes some programs to have explosive memory usage and/or super-linear runtime. -Zmiri-track-alloc-accesses show not only allocation and free events for tracked allocations, but also reads and writes. -Zmiri-track-alloc-id=\u0026lt;id1\u0026gt;,\u0026lt;id2\u0026gt;,... shows a backtrace when the given allocations are being allocated or freed. This helps in debugging memory leaks and use after free bugs. Specifying this argument multiple times does not overwrite the previous values, instead it appends its values to the list. Listing an id multiple times has no effect. -Zmiri-track-pointer-tag=\u0026lt;tag1\u0026gt;,\u0026lt;tag2\u0026gt;,... shows a backtrace when a given pointer tag is created and when (if ever) it is popped from a borrow stack (which is where the tag becomes invalid and any future use of it will error). This helps you in finding out why UB is happening and where in your code would be a good place to look for it. Specifying this argument multiple times does not overwrite the previous values, instead it appends its values to the list. Listing a tag multiple times has no effect. -Zmiri-track-weak-memory-loads shows a backtrace when weak memory emulation returns an outdated value from a load. This can help diagnose problems that disappear under -Zmiri-disable-weak-memory-emulation. -Zmiri-tree-borrows replaces Stacked Borrows with the Tree Borrows rules. Tree Borrows is even more experimental than Stacked Borrows. While Tree Borrows is still sound in the sense of catching all aliasing violations that current versions of the compiler might exploit, it is likely that the eventual final aliasing model of Rust will be stricter than Tree Borrows. In other words, if you use Tree Borrows, even if your code is accepted today, it might be declared UB in the future. This is much less likely with Stacked Borrows. Using Tree Borrows currently implies -Zmiri-strict-provenance because integer-to-pointer casts are not supported in this mode, but that may change in the future. -Zmiri-force-page-size=\u0026lt;num\u0026gt; overrides the default page size for an architecture, in multiples of 1k. 4 is default for most targets. This value should always be a power of 2 and nonzero. -Zmiri-unique-is-unique performs additional aliasing checks for core::ptr::Unique to ensure that it could theoretically be considered noalias. This flag is experimental and has an effect only when used with -Zmiri-tree-borrows. Some native rustc -Z flags are also very relevant for Miri:\n-Zmir-opt-level controls how many MIR optimizations are performed. Miri overrides the default to be 0; be advised that using any higher level can make Miri miss bugs in your program because they got optimized away. -Zalways-encode-mir makes rustc dump MIR even for completely monomorphic functions. This is needed so that Miri can execute such functions, so Miri sets this flag per default. -Zmir-emit-retag controls whether Retag statements are emitted. Miri enables this per default because it is needed for Stacked Borrows and Tree Borrows. Moreover, Miri recognizes some environment variables:\nMIRIFLAGS defines extra flags to be passed to Miri. MIRI_LIB_SRC defines the directory where Miri expects the sources of the standard library that it will build and use for interpretation. This directory must point to the library subdirectory of a rust-lang/rust repository checkout. MIRI_SYSROOT indicates the sysroot to use. When using cargo miri test/cargo miri run, this skips the automatic setup \u0026ndash; only set this if you do not want to use the automatically created sysroot. When invoking cargo miri setup, this indicates where the sysroot will be put. MIRI_NO_STD makes sure that the target\u0026rsquo;s sysroot is built without libstd. This allows testing and running no_std programs. This should not usually be used; Miri has a heuristic to detect no-std targets based on the target name. Setting this on a target that does support libstd can lead to confusing results. Miri extern functions # Miri provides some extern functions that programs can import to access Miri-specific functionality. They are declared in /tests/utils/miri_extern.rs.\nEntry point for no-std binaries # Binaries that do not use the standard library are expected to declare a function like this so that Miri knows where it is supposed to start execution:\n#[cfg(miri)] #[no_mangle] fn miri_start(argc: isize, argv: *const *const u8) -\u0026gt; isize { // Call the actual start function that your project implements, based on your target\u0026#39;s conventions. } Contributing and getting help # If you want to contribute to Miri, great! Please check out our contribution guide.\nFor help with running Miri, you can open an issue here on GitHub or use the Miri stream on the Rust Zulip.\nHistory # This project began as part of an undergraduate research course in 2015 by @solson at the University of Saskatchewan. There are slides and a report available from that project. In 2016, @oli-obk joined to prepare Miri for eventually being used as const evaluator in the Rust compiler itself (basically, for const and static stuff), replacing the old evaluator that worked directly on the AST. In 2017, @RalfJung did an internship with Mozilla and began developing Miri towards a tool for detecting undefined behavior, and also using Miri as a way to explore the consequences of various possible definitions for undefined behavior in Rust. @oli-obk\u0026rsquo;s move of the Miri engine into the compiler finally came to completion in early 2018. Meanwhile, later that year, @RalfJung did a second internship, developing Miri further with support for checking basic type invariants and verifying that references are used according to their aliasing restrictions.\nBugs found by Miri # Miri has already found a number of bugs in the Rust standard library and beyond, some of which we collect here. If Miri helped you find a subtle UB bug in your code, we\u0026rsquo;d appreciate a PR adding it to the list!\nDefinite bugs found:\nDebug for vec_deque::Iter accessing uninitialized memory Vec::into_iter doing an unaligned ZST read From\u0026lt;\u0026amp;[T]\u0026gt; for Rc creating a not sufficiently aligned reference BTreeMap creating a shared reference pointing to a too small allocation Vec::append creating a dangling reference Futures turning a shared reference into a mutable one str turning a shared reference into a mutable one rand performing unaligned reads The Unix allocator calling posix_memalign in an invalid way getrandom calling the getrandom syscall in an invalid way Vec and BTreeMap leaking memory under some (panicky) conditions beef leaking memory EbrCell using uninitialized memory incorrectly TiKV performing an unaligned pointer access servo_arc creating a dangling shared reference TiKV constructing out-of-bounds pointers (and overlapping mutable references) encoding_rs doing out-of-bounds pointer arithmetic TiKV using Vec::from_raw_parts incorrectly Incorrect doctests for AtomicPtr and Box::from_raw_in Insufficient alignment in ThinVec crossbeam-epoch calling assume_init on a partly-initialized MaybeUninit integer-encoding dereferencing a misaligned pointer rkyv constructing a Box\u0026lt;[u8]\u0026gt; from an overaligned allocation Data race in arc-swap Data race in thread::scope regex incorrectly handling unaligned Vec\u0026lt;u8\u0026gt; buffers Incorrect use of compare_exchange_weak in once_cell Dropping with unaligned pointers in vec::IntoIter Deallocating with the wrong layout in new specializations for in-place Iterator::collect Incorrect offset computation for highly-aligned types in portable-atomic-util Occasional memory leak in std::mpsc channels (original code in crossbeam) Weak-memory-induced memory leak in Windows thread-local storage A bug in the new RwLock::downgrade implementation (caught by Miri before it landed in the Rust repo) Violations of Stacked Borrows found that are likely bugs (but Stacked Borrows is currently just an experiment):\nVecDeque::drain creating overlapping mutable references Various BTreeMap problems BTreeMap iterators creating mutable references that overlap with shared references BTreeMap::iter_mut creating overlapping mutable references BTreeMap node insertion using raw pointers outside their valid memory area LinkedList cursor insertion creating overlapping mutable references Vec::push invalidating existing references into the vector align_to_mut violating uniqueness of mutable references sized-chunks creating aliasing mutable references String::push_str invalidating existing references into the string ryu using raw pointers outside their valid memory area ink! creating overlapping mutable references TiKV creating overlapping mutable reference and raw pointer Windows Env iterator using a raw pointer outside its valid memory area VecDeque::iter_mut creating overlapping mutable references Various standard library aliasing issues involving raw pointers \u0026lt;[T]\u0026gt;::copy_within using a loan after invalidating it Scientific papers employing Miri # Stacked Borrows: An Aliasing Model for Rust Using Lightweight Formal Methods to Validate a Key-Value Storage Node in Amazon S3 SyRust: Automatic Testing of Rust Libraries with Semantic-Aware Program Synthesis License # Licensed under either of\nApache License, Version 2.0 ( LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0) MIT license ( LICENSE-MIT or http://opensource.org/licenses/MIT) at your option.\nContribution # Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you shall be dual licensed as above, without any additional terms or conditions.\n","date":"7 October 2024","permalink":"/projects/miri/","section":"Projects","summary":"Miri # Miri is an Undefined Behavior detection tool for Rust.","title":"miri"},{"content":" Go Game in Go Lang # ⚪ ⚫ Implement the game of Go in the language Go\nPreview # ","date":"5 October 2024","permalink":"/projects/go-game-in-go-lang/","section":"Projects","summary":"Go Game in Go Lang # ⚪ ⚫ Implement the game of Go in the language Go","title":"go-game-in-go-lang"},{"content":" today # Create a new file with today\u0026rsquo;s date\nSetup \u0026amp; Install # ./install.sh Usage # today Output # --- Date: 10-11-2024 --- Why # I often take notes in markdown and I want a standard filename and file template. This allows me to just write today in the command line and start editing notes.\n","date":"30 September 2024","permalink":"/projects/today/","section":"Projects","summary":"today # Create a new file with today\u0026rsquo;s date","title":"today"},{"content":" assembly-ml # The goal of this project is to create a machine learning library for Python written in x86-64 Assembly.\nRoadmap # This project has been broken down into multiple parts.\n1. add_one - proof of concept # Part 1. is the folder called add_one which is a C extension for Python that adds a very simple function in C that is accessible in Python.\n2. linear-regression # Part 2. is the linear-regression folder which is linear regression written in C.\nYou can use regression.c and regression.h as a simple regression library.\n3. regression_lib # Part 3. A regression library written in C and accessible in Python through the C extension\n4. TODO # Part 4. will be to convert the linear regression code into x86-64 assembly\n5. TODO # Part 5. will be to write the C extension code in x86-64 assembly and add the linear regression assembly\n","date":"12 September 2024","permalink":"/projects/assembly-ml/","section":"Projects","summary":"assembly-ml # The goal of this project is to create a machine learning library for Python written in x86-64 Assembly.","title":"assembly-ml"},{"content":"","date":"12 September 2024","permalink":"/tags/c/","section":"Tags","summary":"","title":"C"},{"content":" A programming language that compiles to x86-64 assembly for math using postfix notation About # Create a program in the interpreter or run a code file.\nComponent converts your code to RISC-V or x86-64 assembly to be run.\nSee convert-to-assembly for more info.\nComponent is the follow-up language to Basis which is also a math language. Component was built with insights learned from writing Basis. The main difference between the two is that Basis has a more complicated syntax which included types and data structures whereas Component uses postfix notation with the addition of functions as operations.\nThe code for Component and Basis use the same lexer. Component is mainly different because it has a different syntax. The lexer for basis was written generally enough that this code can be shared. It also helps that Basis is a math language inspired by postfix notation but not exclusively postfix notation.\nInteractive interpreter # Note that the symbol \u0026gt; is used in code example to show that these expressions are being put into a REPL and evaluated. These symbols at the start of the expression are not a part of the syntax.\nComponent 0.1.0 A programming language for math using postfix notation USAGE: component [FLAGS] [OPTIONS] FLAGS: -h, --help Prints help information -V, --version Prints version information -v, --verbose Print the stack and lexer information on each command OPTIONS: -a, --asm \u0026lt;asm\u0026gt; Use `x86` or `x86-64` to get x86-64 assembly and use `RISCV` or `RISC-V` for RISC-V assembly -f, --filename \u0026lt;filename\u0026gt; Specify an input Component file to be run Guide # Component uses Postfix Notation. Postfix Notation has the arguments first, and then the function or operation. In the below example, we are adding 1 and 2 with the + operator to get 3. Try typing the following into the interactive prompt:\n1 2 + It should look like this once it\u0026rsquo;s been typed in and you press enter:\n\u0026gt; 1 2 + -\u0026gt; 3 Component has a lot of the operators you would expect from math and other languages:\n+ addition - subtraction * multiplication / division ^ exponentiation Some that may be familiar from other languages:\n! logical not And some that may not be as familiar:\n? conditional assignment Component also has functions that can be called in the same way as operators:\nsqrt square root And built-in constants:\ne Euler\u0026#39;s number C Speed of light (in m/s) pi Pi true and false Full List of Keywords # int converts types into NumericIntLiteral if possible dec converts types into NumericDecLiteral if possible bool converts types into BoolLiteral if possible vars prints all variables stack prints the current stack Constants # e Euler\u0026rsquo;s number pi Pi C Speed of light (in m/s) true Boolean true false Boolean false Built-in Functions # sqrt Square root Operations # + addition - subtraction * multiplication / division ^ exponentiation ! logical not ? conditional assignment Basic Math Operations # Add two numbers together.\n\u0026gt; 1 1 + -\u0026gt; 2 This also works with - for subtraction, * for multiplication, and / for division.\nVector Operations (Coming Soon) # \u0026gt; 1 2 3 \u0026gt; 5 6 7 \u0026gt; x -\u0026gt; 4 -8 4 Variables # Assign the value 2 to variable a.\n\u0026gt; 2 a = -\u0026gt; 2 Use the variable a.\na 4 * -\u0026gt; 8 Conditional Assignment # This will conditionally assign 5 to b if a is true.\n\u0026gt; true a = \u0026gt; 5 b a ? Variables are statically typed in Component. Here is an example usage of a variable.\nFunction # Create an addition function called foo. Note that to start a function you do in fact need to use the \u0026gt; again to tell the interpreter that the following is a function and should not be evaluated.\n\u0026gt; \u0026gt; + foo func \u0026gt; 1 2 foo -\u0026gt; 3 Here is the example of calculating the speed dilation of an object at velocity v (in m/s) for time t (in seconds). More info about this in the section #Component-in-action\n\u0026gt; \u0026gt; t 1 v 2 ^ C 2 ^ / - sqrt / s fn Type Conversion # \u0026gt; 4 5 / -\u0026gt; 0.8 \u0026gt; 4 5 / int -\u0026gt; 0 \u0026gt; 2 dec -\u0026gt; 1 Component in action # The following is a Component program to calculate the time dilation observed by an object moving at 300 m/s for 25 seconds.\nAfter adding C as a constant and compressing this whole program into a single expression, here is the same code minus the variable assignment.\nHere I created a function called s that computes the same equation.\nA test file with the same functionality can be found here: space.\nConvert to Assembly # Currently the interpreter creates x86-64 or RISC-V assembly as the code is being evaluated. In the future, an output file can be specified and the assembly will be put into that file and assembled.\nError Handling # Errors that occur in the interactive interpreter cause the line being interpreted to crash. When this happens, one of the following error messages will be displayed.\nAssignment Type Mismatch [E1] # An Assignment Type Mismatch happens when you try to assign a value to an existing variable of a different type. In the following example, the variable a is created as a NumericIntLiteral type. Then the value 0.8 is attempted to be assigned the variable a but because 0.8 is of the type NumericDecLiteral, it fails with an Assignment Type Mismatch error. This means that variables are statically typed and cannot be changed during runtime.\n\u0026gt; 1 a = -\u0026gt; 1 \u0026gt; 4 5 / a = Error: Assignment Type Mismatch [E1] 0.8 a = ^^^ cannot assign value 0.8 of type \u0026lt;NumericDecLiteral\u0026gt; to a variable of type \u0026lt;NumericIntLiteral\u0026gt; Wrong Type [E2] # A Wrong Type error happens when you try to call an operation or a function on one or more variables of the wrong type. Here is an example of an identifier being used before it has been assigned. Since it does not get swapped out for a value, since it hasn\u0026rsquo;t been assigned, it has the type Identifier and therefore is the wrong type. In the future, this will also happen for math operations on the type String.\n\u0026gt; a 1 + Error: Wrong Type [E2] a 1 + ^ value is not a \u0026lt;NumericIntLiteral\u0026gt; or \u0026lt;NumericDecLiteral\u0026gt; Invalid Type Cast [E3] # An Invalid Type Cast happens when you try to cast from type to type but there is not an operation where this is possible. Similar to the Wrong Type error, the example below shows how an Identifier that has not been assigned a value, is being attempted to cast to both a NumericDecLiteral with the keyword dec and a NumericIntLiteral with the keyword int. Since there has been no assignment to the variable a, it will not get swapped out for a value, and the type cannot be cast to either.\n\u0026gt; a dec Error: Invalid Type Cast [E3] a dec ^^^ Cannot convert \u0026lt;Identifier\u0026gt; to \u0026lt;NumericDecLiteral\u0026gt; \u0026gt; a int Error: Invalid Type Cast [E3] a int ^^^ Cannot convert \u0026lt;Identifier\u0026gt; to \u0026lt;NumericIntLiteral\u0026gt; Stack Empty [E4] # The Stack Empty error happens when the function or operation that has been called requires more arguments than are currently on the stack. This is an indication that not enough variables where provided. In the example below, a single NumericIntLiteral has been added to the stack and then the Addition operation has been called. This gives an error because the Addition operation requires two arguments.\n\u0026gt; 1 + Error: Stack Empty [E4] Operation Not Implemented [E5] # The Operation Not Implemented error occurs when a non-identifier symbol has been parsed that has not gotten functionality yet. Symbol here meaning one or many characters (e.g. foo or 123). Since symbols that are not keywords or existing identifiers get read as identifiers (so long they follow the identifier rules), there are few cases that trigger the Operation Not Implemented error. The one class of symbols that do cause this error are characters like # and $ which have not been assigned any operation.\n\u0026gt; # Error: Operation Not Implemented [E5] \u0026gt; $ Error: Operation Not Implemented [E5] Example of errors # Here is an example of what this might look like in the interactive interpreter.\nMore About Component # Why is it called Component # Component is named after vectors, where a single scalar in a vector is called a \u0026ldquo;component\u0026rdquo; of that vector (e.g. \u0026lt; 1 2 3 \u0026gt; where 1, 2, and 3 are all components. This is also in reference to the name Basis, where a basis vector is a subset of a vector space where the vectors are linearly independent.\n","date":"4 September 2024","permalink":"/projects/component/","section":"Projects","summary":"A programming language that compiles to x86-64 assembly for math using postfix notation About # Create a program in the interpreter or run a code file.","title":"component"},{"content":" PyTorch is a Python package that provides two high-level features:\nTensor computation (like NumPy) with strong GPU acceleration Deep neural networks built on a tape-based autograd system You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.\nOur trunk health (Continuous Integration signals) can be found at hud.pytorch.org.\nMore About PyTorch A GPU-Ready Tensor Library Dynamic Neural Networks: Tape-Based Autograd Python First Imperative Experiences Fast and Lean Extensions Without Pain Installation Binaries NVIDIA Jetson Platforms From Source Prerequisites NVIDIA CUDA Support AMD ROCm Support Intel GPU Support Get the PyTorch Source Install Dependencies Install PyTorch Adjust Build Options (Optional) Docker Image Using pre-built images Building the image yourself Building the Documentation Previous Versions Getting Started Resources Communication Releases and Contributing The Team License More About PyTorch # Learn the basics of PyTorch\nAt a granular level, PyTorch is a library that consists of the following components:\nComponent Description torch A Tensor library like NumPy, with strong GPU support torch.autograd A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch torch.jit A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code torch.nn A neural networks library deeply integrated with autograd designed for maximum flexibility torch.multiprocessing Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training torch.utils DataLoader and other utility functions for convenience Usually, PyTorch is used either as:\nA replacement for NumPy to use the power of GPUs. A deep learning research platform that provides maximum flexibility and speed. Elaborating Further:\nA GPU-Ready Tensor Library # If you use NumPy, then you have used Tensors (a.k.a. ndarray).\nPyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the computation by a huge amount.\nWe provide a wide variety of tensor routines to accelerate and fit your scientific computation needs such as slicing, indexing, mathematical operations, linear algebra, reductions. And they are fast!\nDynamic Neural Networks: Tape-Based Autograd # PyTorch has a unique way of building neural networks: using and replaying a tape recorder.\nMost frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world. One has to build a neural network and reuse the same structure again and again. Changing the way the network behaves means that one has to start from scratch.\nWith PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes from several research papers on this topic, as well as current and past work such as torch-autograd, autograd, Chainer, etc.\nWhile this technique is not unique to PyTorch, it\u0026rsquo;s one of the fastest implementations of it to date. You get the best of speed and flexibility for your crazy research.\nPython First # PyTorch is not a Python binding into a monolithic C++ framework. It is built to be deeply integrated into Python. You can use it naturally like you would use NumPy / SciPy / scikit-learn etc. You can write your new neural network layers in Python itself, using your favorite libraries and use packages such as Cython and Numba. Our goal is to not reinvent the wheel where appropriate.\nImperative Experiences # PyTorch is designed to be intuitive, linear in thought, and easy to use. When you execute a line of code, it gets executed. There isn\u0026rsquo;t an asynchronous view of the world. When you drop into a debugger or receive error messages and stack traces, understanding them is straightforward. The stack trace points to exactly where your code was defined. We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.\nFast and Lean # PyTorch has minimal framework overhead. We integrate acceleration libraries such as Intel MKL and NVIDIA ( cuDNN, NCCL) to maximize speed. At the core, its CPU and GPU Tensor and neural network backends are mature and have been tested for years.\nHence, PyTorch is quite fast — whether you run small or large neural networks.\nThe memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives. We\u0026rsquo;ve written custom memory allocators for the GPU to make sure that your deep learning models are maximally memory efficient. This enables you to train bigger deep learning models than before.\nExtensions Without Pain # Writing new neural network modules, or interfacing with PyTorch\u0026rsquo;s Tensor API was designed to be straightforward and with minimal abstractions.\nYou can write new neural network layers in Python using the torch API or your favorite NumPy-based libraries such as SciPy.\nIf you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate. No wrapper code needs to be written. You can see a tutorial here and an example here.\nInstallation # Binaries # Commands to install binaries via Conda or pip wheels are on our website: https://pytorch.org/get-started/locally/\nNVIDIA Jetson Platforms # Python wheels for NVIDIA\u0026rsquo;s Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, and Jetson AGX Orin are provided here and the L4T container is published here\nThey require JetPack 4.2 and above, and @dusty-nv and @ptrblck are maintaining them.\nFrom Source # Prerequisites # If you are installing from source, you will need:\nPython 3.9 or later A compiler that fully supports C++17, such as clang or gcc (gcc 9.4.0 or newer is required, on Linux) Visual Studio or Visual Studio Build Tool (Windows only) * PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise, Professional, or Community Editions. You can also install the build tools from https://visualstudio.microsoft.com/visual-cpp-build-tools/. The build tools do not come with Visual Studio Code by default.\n* We highly recommend installing an Anaconda environment. You will get a high-quality BLAS library (MKL) and you get controlled dependency versions regardless of your Linux distro.\nAn example of environment setup is shown below:\nLinux: $ source \u0026lt;CONDA_INSTALL_DIR\u0026gt;/bin/activate $ conda create -y -n \u0026lt;CONDA_NAME\u0026gt; $ conda activate \u0026lt;CONDA_NAME\u0026gt; Windows: $ source \u0026lt;CONDA_INSTALL_DIR\u0026gt;\\Scripts\\activate.bat $ conda create -y -n \u0026lt;CONDA_NAME\u0026gt; $ conda activate \u0026lt;CONDA_NAME\u0026gt; $ call \u0026#34;C:\\Program Files\\Microsoft Visual Studio\\\u0026lt;VERSION\u0026gt;\\Community\\VC\\Auxiliary\\Build\\vcvarsall.bat\u0026#34; x64 NVIDIA CUDA Support # If you want to compile with CUDA support, select a supported version of CUDA from our support matrix, then install the following:\nNVIDIA CUDA NVIDIA cuDNN v8.5 or above Compiler compatible with CUDA Note: You could refer to the cuDNN Support Matrix for cuDNN versions with the various supported CUDA, CUDA driver and NVIDIA hardware\nIf you want to disable CUDA support, export the environment variable USE_CUDA=0. Other potentially useful environment variables may be found in setup.py.\nIf you are building for NVIDIA\u0026rsquo;s Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are available here\nAMD ROCm Support # If you want to compile with ROCm support, install\nAMD ROCm 4.0 and above installation ROCm is currently supported only for Linux systems. By default the build system expects ROCm to be installed in /opt/rocm. If ROCm is installed in a different directory, the ROCM_PATH environment variable must be set to the ROCm installation directory. The build system automatically detects the AMD GPU architecture. Optionally, the AMD GPU architecture can be explicitly set with the PYTORCH_ROCM_ARCH environment variable AMD GPU architecture\nIf you want to disable ROCm support, export the environment variable USE_ROCM=0. Other potentially useful environment variables may be found in setup.py.\nIntel GPU Support # If you want to compile with Intel GPU support, follow these\nPyTorch Prerequisites for Intel GPUs instructions. Intel GPU is supported for Linux and Windows. If you want to disable Intel GPU support, export the environment variable USE_XPU=0. Other potentially useful environment variables may be found in setup.py.\nGet the PyTorch Source # git clone --recursive https://github.com/pytorch/pytorch cd pytorch # if you are updating an existing checkout git submodule sync git submodule update --init --recursive Install Dependencies # Common\nconda install cmake ninja # Run this command on native Windows conda install rust # Run this command from the PyTorch directory after cloning the source code using the “Get the PyTorch Source“ section below pip install -r requirements.txt On Linux\npip install mkl-static mkl-include # CUDA only: Add LAPACK support for the GPU if needed conda install -c pytorch magma-cuda121 # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo # (optional) If using torch.compile with inductor/triton, install the matching version of triton # Run from the pytorch directory after cloning # For Intel GPU support, please explicitly `export USE_XPU=1` before running command. make triton On MacOS\n# Add this package on intel x86 processor machines only pip install mkl-static mkl-include # Add these packages if torch.distributed is needed conda install pkg-config libuv On Windows\npip install mkl-static mkl-include # Add these packages if torch.distributed is needed. # Distributed package support on Windows is a prototype feature and is subject to changes. conda install -c conda-forge libuv=1.39 Install PyTorch # On Linux\nIf you would like to compile PyTorch with new C++ ABI enabled, then first run this command:\nexport _GLIBCXX_USE_CXX11_ABI=1 Please note that starting from PyTorch 2.5, the PyTorch build with XPU supports both new and old C++ ABIs. Previously, XPU only supported the new C++ ABI. If you want to compile with Intel GPU support, please follow Intel GPU Support.\nIf you\u0026rsquo;re compiling for AMD ROCm then first run this command:\n# Only run this if you\u0026#39;re compiling for ROCm python tools/amd_build/build_amd.py Install PyTorch\nexport CMAKE_PREFIX_PATH=\u0026#34;${CONDA_PREFIX:-\u0026#39;$(dirname $(which conda))/../\u0026#39;}:${CMAKE_PREFIX_PATH}\u0026#34; python setup.py develop On macOS\npython3 setup.py develop On Windows\nIf you want to build legacy python code, please refer to Building on legacy code and CUDA\nCPU-only builds\nIn this mode PyTorch computations will run on your CPU, not your GPU\npython setup.py develop Note on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you\u0026rsquo;ll need to manually download the library and set up the building environment by tweaking CMAKE_INCLUDE_PATH and LIB. The instruction here is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.\nCUDA based build\nIn this mode PyTorch computations will leverage your GPU via CUDA for faster number crunching\nNVTX is needed to build Pytorch with CUDA. NVTX is a part of CUDA distributive, where it is called \u0026ldquo;Nsight Compute\u0026rdquo;. To install it onto an already installed CUDA run CUDA installation once again and check the corresponding checkbox. Make sure that CUDA with Nsight Compute is installed after Visual Studio.\nCurrently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If ninja.exe is detected in PATH, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019. If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.\nAdditional libraries such as Magma, oneDNN, a.k.a. MKLDNN or DNNL, and Sccache are often needed. Please refer to the installation-helper to install them.\nYou can refer to the build_pytorch.bat script for some other environment variables configurations\ncmd :: Set the environment variables after you have downloaded and unzipped the mkl package, :: else CMake would throw an error as `Could NOT find OpenMP`. set CMAKE_INCLUDE_PATH={Your directory}\\mkl\\include set LIB={Your directory}\\mkl\\lib;%LIB% :: Read the content in the previous section carefully before you proceed. :: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block. :: \u0026#34;Visual Studio 2019 Developer Command Prompt\u0026#34; will be run automatically. :: Make sure you have CMake \u0026gt;= 3.12 before you do this when you use the Visual Studio generator. set CMAKE_GENERATOR_TOOLSET_VERSION=14.27 set DISTUTILS_USE_SDK=1 for /f \u0026#34;usebackq tokens=*\u0026#34; %i in (`\u0026#34;%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe\u0026#34; -version [15^,17^) -products * -latest -property installationPath`) do call \u0026#34;%i\\VC\\Auxiliary\\Build\\vcvarsall.bat\u0026#34; x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION% :: [Optional] If you want to override the CUDA host compiler set CUDAHOSTCXX=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\bin\\HostX64\\x64\\cl.exe python setup.py develop Adjust Build Options (Optional) # You can adjust the configuration of cmake variables optionally (without building first), by doing the following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done with such a step.\nOn Linux\nexport CMAKE_PREFIX_PATH=\u0026#34;${CONDA_PREFIX:-\u0026#39;$(dirname $(which conda))/../\u0026#39;}:${CMAKE_PREFIX_PATH}\u0026#34; python setup.py build --cmake-only ccmake build # or cmake-gui build On macOS\nexport CMAKE_PREFIX_PATH=\u0026#34;${CONDA_PREFIX:-\u0026#39;$(dirname $(which conda))/../\u0026#39;}:${CMAKE_PREFIX_PATH}\u0026#34; MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only ccmake build # or cmake-gui build Docker Image # Using pre-built images # You can also pull a pre-built docker image from Docker Hub and run with docker v19.03+\ndocker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g. for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you should increase shared memory size either with --ipc=host or --shm-size command line options to nvidia-docker run.\nBuilding the image yourself # NOTE: Must be built with a docker version \u0026gt; 18.06\nThe Dockerfile is supplied to build images with CUDA 11.1 support and cuDNN v8. You can pass PYTHON_VERSION=x.y make variable to specify which Python version is to be used by Miniconda, or leave it unset to use the default.\nmake -f docker.Makefile # images are tagged as docker.io/${your_docker_username}/pytorch You can also pass the CMAKE_VARS=\u0026quot;...\u0026quot; environment variable to specify additional CMake variables to be passed to CMake during the build. See setup.py for the list of available variables.\nmake -f docker.Makefile Building the Documentation # To build documentation in various formats, you will need Sphinx and the readthedocs theme.\ncd docs/ pip install -r requirements.txt make html make serve Run make to get a list of all available output formats.\nIf you get a katex error run npm install katex. If it persists, try npm install -g katex\nNote: if you installed nodejs with a different package manager (e.g., conda) then npm will probably install a version of katex that is not compatible with your version of nodejs and doc builds will fail. A combination of versions that is known to work is node@6.13.1 and katex@0.13.18. To install the latter with npm you can run npm install -g katex@0.13.18\nPrevious Versions # Installation instructions and binaries for previous PyTorch versions may be found on our website.\nGetting Started # Three-pointers to get you started:\nTutorials: get you started with understanding and using PyTorch Examples: easy to understand PyTorch code across all domains The API Reference Glossary Resources # PyTorch.org PyTorch Tutorials PyTorch Examples PyTorch Models Intro to Deep Learning with PyTorch from Udacity Intro to Machine Learning with PyTorch from Udacity Deep Neural Networks with PyTorch from Coursera PyTorch Twitter PyTorch Blog PyTorch YouTube Communication # Forums: Discuss implementations, research, etc. https://discuss.pytorch.org GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc. Slack: The PyTorch Slack hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is PyTorch Forums. If you need a slack invite, please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1 Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here: https://eepurl.com/cbG0rv Facebook Page: Important announcements about PyTorch. https://www.facebook.com/pytorch For brand guidelines, please visit our website at pytorch.org Releases and Contributing # Typically, PyTorch has three minor releases a year. Please let us know if you encounter a bug by filing an issue.\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.\nIf you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us. Sending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of.\nTo learn more about making a contribution to Pytorch, please see our Contribution page. For more information about PyTorch releases, see Release page.\nThe Team # PyTorch is a community-driven project with several skillful engineers and researchers contributing to it.\nPyTorch is currently maintained by Soumith Chintala, Gregory Chanan, Dmytro Dzhulgakov, Edward Yang, and Nikita Shulga with major contributions coming from hundreds of talented individuals in various forms and means. A non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Koepf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito.\nNote: This project is unrelated to hughperkins/pytorch with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.\nLicense # PyTorch has a BSD-style license, as found in the LICENSE file.\n","date":"29 August 2024","permalink":"/projects/pytorch/","section":"Projects","summary":"PyTorch is a Python package that provides two high-level features:","title":"pytorch"},{"content":" SafeDrive AI # Here is my teams submission for a hackathon called HackDavis. We build lane detection, obstacle avoidance, driver alertness monitoring, and a hardware alert system.\nhttps://devpost.com/software/safe-drive-ai\nhttps://github.com/JakeRoggenbuck/hackdavis-2024\nLinear Regression in Rust # Linear regression from scratch in Rust.\nhttps://github.com/JakeRoggenbuck/linear-regression-rs\nJML # A machine learning library in C++ from scratch.\nhttps://github.com/JakeRoggenbuck/jml\n","date":"28 August 2024","permalink":"/ml/","section":"","summary":"SafeDrive AI # Here is my teams submission for a hackathon called HackDavis.","title":"Machine Learning"},{"content":" special-relativity-time-dilation # Calculate the time dilation by velocity as described by special relativity using Rust! 🦀\nCode # fn time_dilation_by_velocity(t: f64, v: f64) -\u0026gt; f64 { t / (1.0 - ((v * v) / (C * C))).sqrt() } Units Used # t: time in seconds v: km/s Constants # const C: f64 = 299_792_458.0; Equation # Here is the equation for time dilation from velocity.\nSee Also # I also wrote this code in my language called Component seen below:\n","date":"27 August 2024","permalink":"/projects/special-relativity-time-dilation/","section":"Projects","summary":"special-relativity-time-dilation # Calculate the time dilation by velocity as described by special relativity using Rust!","title":"special-relativity-time-dilation"},{"content":" c-neural-network # A neural network written in the C programming language.\nSimilar Projects: # assembly-ml linear-regression-rs Todo # I still need to add these functions\nvoid feedforward(struct NeuralNetwork nn) {} void backprop(struct NeuralNetwork nn) {} ","date":"19 July 2024","permalink":"/projects/c-neural-network/","section":"Projects","summary":"c-neural-network # A neural network written in the C programming language.","title":"c-neural-network"},{"content":"","date":"21 June 2024","permalink":"/tags/commands/","section":"Tags","summary":"","title":"commands"},{"content":"","date":"21 June 2024","permalink":"/tags/git/","section":"Tags","summary":"","title":"git"},{"content":"Here are git commands I used very frequently when I was TPM for Clubly. I still use these commands very frequently for other collaborative git projects and I provide these commands to TPMs that I train at AggieWorks. This was originally written January 10th of 2024.\nEditors Note: This guide uses HTTP git remotes but SSH remotes are what I use and are what I prefer.\nTesting someone else\u0026rsquo;s PR locally # # Fetch the PR where \u0026lt;NUMBER\u0026gt; can be changed to the number that the PR is git fetch upstream pull/\u0026lt;NUMBER\u0026gt;/head git checkout FETCH_HEAD # Test locally Pushing to someone\u0026rsquo;s PR - Here Be Dragons # # \u0026lt;DEV\u0026#39;s Branch Name\u0026gt; can be changed to their branch name (ex. fix-bugs) git switch -c \u0026lt;DEV\u0026#39;s Branch Name\u0026gt; # \u0026lt;DEV\u0026#39;s Name\u0026gt; can be changed to the name of the developer (ex. jake) # This name is set when you add the remote of the developer. # See the next section for how to add a name (remote). git pull \u0026lt;DEV\u0026#39;s Name\u0026gt; \u0026lt;DEV\u0026#39;s Branch Name\u0026gt; # Make changes - Be careful here git push \u0026lt;DEV\u0026#39;s Name\u0026gt; \u0026lt;DEV\u0026#39;s Branch Name\u0026gt; Adding a remote for a dev # # Add the URL - \u0026lt;NAME\u0026gt; is their name (ex. jake), \u0026lt;USERNAME\u0026gt; is their github username (ex. jakeroggenbuck) git remote add \u0026lt;NAME\u0026gt; https://github.com/\u0026lt;USERNAME\u0026gt;/Clubly.git Pushing to preview - Here Be Dragons # # Update your local main git swicth origin main git pull origin main # Test locally # Pushing to preview git push origin preview # Go to github and create a pull request # Review the pull request and merge after checks pass # Check clubly.dev to see changes worked Pushing to production - Here Be Dragons # # Update your local main git swicth origin main git pull origin main # Test locally # Pushing to production git push origin production # Go to github and create a pull request # Review the pull request and merge after checks pass # Check clubly.org to see changes worked ","date":"21 June 2024","permalink":"/posts/git-for-technical-leadership/","section":"Posts","summary":"Here are git commands I used very frequently when I was TPM for Clubly.","title":"Git for Technical Leadership"},{"content":" About the article # This is an updated version of the original article from April of 2023 found here originally made for the Clubly team. This resource may be helpful for other teams. This article is also shows the use of Circular Development.\nCircular Development Setting up a project # Fork the project Clone the fork Use git clone git@github.com:JakeRoggenbuck/clubly.git Add the upstream remote Use git remote add upstream git@github.com:aggieworks/clubly.git What your setup should look like (git remote -v) # origin\tgit@github.com:JakeRoggenbuck/clubly.git (fetch) origin\tgit@github.com:JakeRoggenbuck/clubly.git (push) upstream\tgit@github.com:aggieworks/clubly.git (fetch) upstream\tgit@github.com:aggieworks/clubly.git (push) Getting information # # list the remotes git remote -v # check your staging area git status # check commits git log Update from latest # # make sure you are on your own main and don\u0026#39;t have any current changes git switch main git pull upstream main Before a feature/assignment/change # This should be done before the start of any distinct feature or large change.\ngit switch main git pull upstream main git switch -c feature-name Submitting a feature/assignment/change # # add all changed but prevously staged files git add -u git commit git push origin feature-name ","date":"20 June 2024","permalink":"/posts/updated-helpful-git-commands/","section":"Posts","summary":"About the article # This is an updated version of the original article from April of 2023 found here originally made for the Clubly team.","title":"Helpful Git Commands (Updated)"},{"content":" JCCC (JabaCat C Compiler) # A C compiler and preprocessor written from scratch in C that targets x86-64 assembly\nBuild Instructions # Make sure you have cmake installed on your machine, then run ./scripts/build.sh. Then, you can run JCCC with ./build/jccc.\nUsage # Only lexing is supported at this stage. To lex, run the following: jccc --token-dump \u0026lt;filename\u0026gt;\nDocs # 1. Testing # 1.1 Building tests # Use the normal ./scripts/build.sh that will also build an executable called test_jccc.\n1.2 Running the tests # Run the executable ./test_jccc and you should see a list of the tests being run.\nRunning tests from \u0026#34;test_lexer\u0026#34; ... Running \u0026#34;test_ttype_name\u0026#34; Running \u0026#34;test_ttype_from_string\u0026#34; Running \u0026#34;test_ttype_many_chars\u0026#34; Running \u0026#34;test_ttype_one_char\u0026#34; Concluded tests from \u0026#34;test_lexer\u0026#34; Running tests from \u0026#34;test_x86\u0026#34; ... Running \u0026#34;test_init_int_literal\u0026#34; Concluded tests from \u0026#34;test_x86\u0026#34; For errors, no news is good news because tasserts that fail will show the failure but tasserts that succeed will not display anything.\n1.3 Writing tests # Here is an example usage of the testing. This is from \u0026ldquo;lexer/test_lexer.c\u0026rdquo;. For each module of code, create a test_{module_name} file. This file should include a test_{module_name} function that includes the testing_setup and the testing_cleanup functions.\n#include \u0026#34;lex.h\u0026#34; #include \u0026lt;testing/test_utils.h\u0026gt; int test_lexer() { testing_module_setup(); test_ttype_from_string(); testing_module_cleanup(); return 0; } After this, include a call to this function in the \u0026ldquo;testing/main.c\u0026rdquo; file like how it\u0026rsquo;s done for test_lexer.\n#include \u0026#34;lexer/test_lexer.h\u0026#34; int main() { test_lexer(); return 0; } Finally, here is what a test might look like. Make sure to include a call to testing_func_setup at the start.\nint test_ttype_from_string() { testing_func_setup(); tassert(ttype_from_string(\u0026#34;1\u0026#34;) == TT_LITERAL); tassert(ttype_from_string(\u0026#34;1.2\u0026#34;) == TT_LITERAL); // ... tassert(ttype_from_string(\u0026#34;;\u0026#34;) == TT_SEMI); return 0; } ","date":"18 June 2024","permalink":"/projects/jccc/","section":"Projects","summary":"JCCC (JabaCat C Compiler) # A C compiler and preprocessor written from scratch in C that targets x86-64 assembly","title":"jccc"},{"content":"I enjoy studying and learning math.\nMath research # Here is the first math paper I wrote with Adam Hutchings about the Thue-Morse Sequence.\nEarlier Math Research # On Logs Research # Github: https://github.com/JakeRoggenbuck/on-logs-research\nPascals Triangle # Github: https://github.com/JakeRoggenbuck/pascals-triangle\nDerivative Solver in C # https://github.com/JakeRoggenbuck/derive\nCalculate the integral of a function in C # https://github.com/JakeRoggenbuck/integrate\nNewtons method in C # https://github.com/JakeRoggenbuck/newtons-method\nApproximate Pi # https://github.com/JakeRoggenbuck/approximate-pi\n","date":"4 June 2024","permalink":"/math/","section":"","summary":"I enjoy studying and learning math.","title":"Math"},{"content":"Space has always interested me. Here are a collection of projects that relate to space.\nSatellite Code # I am currently in the Space and Satellite Systems club at UC Davis. We are writing code for a Cube Sat in C.\nIntelliSat # https://github.com/JakeRoggenbuck/IntelliSat\nIntelliSat_Prototype # https://github.com/JakeRoggenbuck/IntelliSat_Prototype\nSatellite-c # https://github.com/JakeRoggenbuck/satellite-c\nOther Code # Cosmic Ray Detector # https://github.com/JakeRoggenbuck/cosmic-ray-detector\nFuture project ideas # Build a reflecting telescope ","date":"4 June 2024","permalink":"/space/","section":"","summary":"Space has always interested me.","title":"Space"},{"content":"A collection of monthly summaries comprised of particular projects, progress, and discoveries I\u0026rsquo;ve found to be noteworthy.\n","date":"20 May 2024","permalink":"/devlogs/","section":"Dev Logs","summary":"A collection of monthly summaries comprised of particular projects, progress, and discoveries I\u0026rsquo;ve found to be noteworthy.","title":"Dev Logs"},{"content":" Machine Dependent Programming # All of this code is private because it was homework.\nI got to use x86-64 Assembly and RISC-V. I learned so much from this class and I will continue to write lots of Assembly.\nMore to add here later.\n","date":"20 May 2024","permalink":"/devlogs/may-2024/","section":"Dev Logs","summary":"Machine Dependent Programming # All of this code is private because it was homework.","title":"May 2024"},{"content":"These are the top 5 most useful git commands that I use almost every day. Out of the 30+ git command aliases I have, these are the for sure the best 5! Please let me know in the comments which ones you like the best!\nHere is the post on my dev.to page.\nIf you have a favorite command of your own, please share in the comments! I\u0026rsquo;ll highlight my favorite ones!\n1. Git List (GLS) # View a super short summary of the most recent commits. Instead of filling the page with just a few commits, view each commit in a single line.\ngit log --pretty=oneline --abbrev-commit I have this aliased to gls with\nalias gls=\u0026#39;git log --pretty=oneline --abbrev-commit\u0026#39; 2. Push Origin (PUSHO) # Push to the current default branch. This one is simple, but I use this one the most.\ngit push origin $(git symbolic-ref --short HEAD) I have this aliases to pusho in my .bashrc with\nalias pusho=\u0026#39;git push origin $(git symbolic-ref --short HEAD)\u0026#39; 3. Git logg (LOGG) # View an ascii art history of all of the branches.\ngit log --graph --decorate --all I have this aliased to logg with\nalias logg=\u0026#39;git log --graph --decorate --all\u0026#39; 4. Git Diff Precise (GDP) # Have you ever been looking at a git diff and it just tells you the whole line is different? Well, this command tells you exactly what characters have changed. Extremely helpful for catching issues in review or remembering exactly what you changed in a large line.\nBefore # After # This is a simple example and it\u0026rsquo;s easy to see what has changed, but if you had a really complicated line with a few different changes in different places, it can be hard to catch. Say an SQL command or similar.\nHere is how to do it!\ngit diff --word-diff=color --word-diff-regex=. I have this aliased to gdp with\nalias gdp=\u0026#39;git diff --word-diff=color --word-diff-regex=.\u0026#39; 5. The MOST useful of all (lookz) # Have you ever tried looking through a whole git history to try to find where you added or removed something? Has a line gone missing or a type changed that has caused an issue in production that you need to fix ASAP! Looking through each commit for a single line may take hours with a big repository. This allows you to search through every commit and use FZF to do it.\nThis does require you to have fzf installed, but I recommend having that anyway. It\u0026rsquo;s super useful for this type of thing.\ngit remote \u0026amp;\u0026amp; git log --pretty=oneline --abbrev-commit \\ | awk \u0026#39;{print $1}\u0026#39; \\ | xargs -I {} git show {} | fzf I have this aliased to lookz with\nalias lookz=\u0026#39;git remote \u0026amp;\u0026amp; git log --pretty=oneline --abbrev-commit | awk \u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;{print $1}\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39; | xargs -I {} git show {} | fzf\u0026#39; Note the single quotes having to be escaped with '\u0026quot;'\u0026quot;' in the bash alias version but not the one directly for the shell.\nI need help making this command more useful. It lets you find a specific name of something that can then be used to search, but once it\u0026rsquo;s selected, you can\u0026rsquo;t find the commit. If anyone knows how to do this, please leave a comment!\n","date":"5 May 2024","permalink":"/posts/5-best-bash-commands/","section":"Posts","summary":"These are the top 5 most useful git commands that I use almost every day.","title":"5 BEST Git Commands"},{"content":" all-the-NaN-floats # Write all of the NaN floats from IEEE 754 32 bit floats to disk.\nIn IEEE 754, if all 8 bits of the exponent in a 32 bit float are set to 1, and at least one of the bits in the mantissa is set to 1, then the float is NaN.\nThis means that there are (2 ^ (32 - 8)) - 2 or 16777214 possible values for NaN for the version of NaN where all exponent bits are set to 1. There are 23 bits in the mantissa and a bit for positive and negative that all can be set to either 1 or 0. You also have to remember to subtract 2 because when just the sign and exponent are set, then it can be infinity for when the sign bit is 0 and -infinity when it\u0026rsquo;s 1. You can see this is done when I start at (0b011111111 \u0026lt;\u0026lt; 23) + 1, where the + 1 makes sure that not only the sign and exponent are set by adding 1 to the mantissa.\nThese are all of the NaNs between the range of start and end as seen here:\nunsigned int start = (0b011111111 \u0026lt;\u0026lt; 23) + 1; unsigned int end = 1 \u0026lt;\u0026lt; 31; Helpful visualization: https://www.h-schmidt.net/FloatConverter/IEEE754.html\nThe resulting file size is ((2 ^ (32 - 8)) - 2) * 4, where 4 is the amount of bytes a float 32 takes up. This results in ~67.11 Megabytes of disk space. If we compress this with something like xz with the highest level of compression, we get a file size of ~4.16 Megabytes.\nThat file was small enough to include in the git project.\nThis also means that a 64 bit IEEE float would have (2 ^ (64 - 11)) - 2 or 9007199254740990 different NaNs. This means a file with all of the NaNs would be ((2 ^ (64 - 11)) - 2) * 4 bytes or about 36,028TB and that\u0026rsquo;s not realistically feasible to create.\nAlso, a 128 bit float would have 36028797018963970 Exbibytes of NaN floats.\n","date":"30 April 2024","permalink":"/projects/all-the-nan-floats/","section":"Projects","summary":"all-the-NaN-floats # Write all of the NaN floats from IEEE 754 32 bit floats to disk.","title":"all-the-NaN-floats"},{"content":" HackDavis - Hackathon # HackDavis is hosts a yearly hackathon at UC Davis.\nWhat my team built # Safe Drive AI - Make three different OpenCV models for lane detection, human obstacle detection, and driver alertness detection.\nGithub\nHere is our DevPost too.\nClubly Speed Improvements # I reduced the amount of data sent on each search and this resulted in a statistically significant speed improvement of 0.02 seconds, so 20 milliseconds on production which is 120% improvement for free.\nhttps://api.clubly.org/api/v1/clubs 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:16\u0026lt;00:00, 3.06it/s] 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:14\u0026lt;00:00, 3.53it/s] current.std()=0.0757401695006395 reduced.std()=0.009859827712916693 current.mean()=0.24328888000000004 reduced.mean()=0.20660545999999996 t_stat=3.361959534140494 p_value=0.001104776517782507 Here is the code to test this.\n# ... def current_prod_test(): data = { \u0026#34;keywords\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;limit\u0026#34;: 100, } times = [] for x in tqdm(range(50)): res = requests.post(URL, data=data) time = res.elapsed.total_seconds() times.append(time) return times def reduced_prod_test(): data = { \u0026#34;keywords\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;limit\u0026#34;: 100, \u0026#34;reduced\u0026#34;: True } times = [] for x in tqdm(range(50)): res = requests.post(URL, data=data) time = res.elapsed.total_seconds() times.append(time) return times # ... def prod_experiment(): print(URL) current_data = current_prod_test() reduced_data = reduced_prod_test() current = np.array(current_data) reduced = np.array(reduced_data) print(f\u0026#34;{current.std()=}\u0026#34;) print(f\u0026#34;{reduced.std()=}\u0026#34;) print(f\u0026#34;{current.mean()=}\u0026#34;) print(f\u0026#34;{reduced.mean()=}\u0026#34;) t_stat, p_value = stats.ttest_ind(current, reduced) print(f\u0026#34;{t_stat=} {p_value=}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: prod_experiment() all-the-NaN-floats # Write all of the NaN floats from IEEE 754 32 bit floats to disk.\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int main() { FILE *fp; fp = fopen(\u0026#34;all-the-nan-floats\u0026#34;, \u0026#34;wb\u0026#34;); // This is first number where all the exponent bits are set, but nothing // else is set except the first bit, otherwise it would be infinity or zero unsigned int start = (0b011111111 \u0026lt;\u0026lt; 23) + 1; // This is number where all the bits have been set except the sign bit unsigned int end = (1 \u0026lt;\u0026lt; 31) - 1u; int counter = 0; do { // Write the positive value fwrite(\u0026amp;start, sizeof(unsigned int), 1, fp); start = -start; // Write the negative value fwrite(\u0026amp;start, sizeof(unsigned int), 1, fp); start = -start; start++; // Count twice for the two writes counter++; counter++; } while (start \u0026lt; end); printf(\u0026#34;Wrote %d NaN floats\\n\u0026#34;, counter); } This was for fun and to learn more about IEEE floating point numbers.\nLots of leetcode! # I got to 400 problems solved!\nVector C # Vector math in C\nGithub\n","date":"30 April 2024","permalink":"/devlogs/april-2024/","section":"Dev Logs","summary":"HackDavis - Hackathon # HackDavis is hosts a yearly hackathon at UC Davis.","title":"April 2024"},{"content":" HackDavis 2024 # Our Github\nMission: Reduce car related injuries and deaths\nOur project for https://hackdavis.io/event 2024.\nUse Intel Developer Cloud to train AI 🚀 Performance gains (reduce PyTorch train time by 4 minutes) with bfloat16 and ipex ⚡ Tensorflow performance gains (train time going from 2:23s to 1:14s) Almost twice as fast with ipex! Use ipex with PyTorch in IDC Make three different OpenCV models for lane detection, human obstacle detection, and driver alertness detection Parts of the whole system # Lane Detection # Use a forward facing camera and OpenCV to recognize lanes and alert the driver if they do not stay within their lane.\nDriver Alertness Detection # Use a driver facing camera and OpenCV to detect if the driver is awake and paying attention to the road\nBlind Spot Detection # Use PyTorch and Intel Developer Cloud Notebook to detect pedestrians walking infront and next to the car and alert the driver if they get too close. We were able to use Intel\u0026rsquo;s ipex, PyTorch plugin, and bfloat16 to reduce the training time by 4 entire minutes. We used ipex from Intel AI and the Intel PyTorch plugin to leverage Intel AMX.\nipex example usage # import torch import intel_extension_for_pytorch as ipex from engine import train_one_epoch, evaluate import datetime # ... params = [p for p in model.parameters() if p.requires_grad] optimizer = torch.optim.SGD( params, lr=0.005, momentum=0.9, weight_decay=0.0005 ) lr_scheduler = torch.optim.lr_scheduler.StepLR( optimizer, step_size=3, gamma=0.1 ) model = get_model_instance_segmentation(num_classes) model, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.float32) model = model.to(device) Optimization Experiment:\nStart time: 2024-04-27 16:27:36.983459 End time: 2024-04-27 16:47:38.238154 datetime.timedelta(seconds=1201, microseconds=254695) Start time: 2024-04-27 17:46:16.191538 End time: 2024-04-27 18:02:06.233684 datetime.timedelta(seconds=950, microseconds=42146) Start time: 2024-04-27 17:02:22.302090 End time: 2024-04-27 17:18:17.402227 datetime.timedelta(seconds=955, microseconds=100137) As you can see, intel ipex and bfloat16 saved us 4 entire minutes for this model. Imagine a much larger model and the type of time and compute cost savings that could be achived.\nIntel LeaderBoard # We submitted a custom bfloat16 model to the intel leaderboard. Here is the model fine tuned from Gemma\nUse of LLMs and implications of hallucinations # We used LLMs to generate and extend data that we used to fine tune our text directions Gemma model. Hallucinations for directions in this case could cause people to go to places that do not exist. This could be fixed by using existing databases of locations (Like a mapping API) to insure that the LLM is always directing someone to a real place.\nProximity Alert # Use an Arduino and an ultrasonic distance sensor to alert the driver if they are too close to anything, including another car\nUsing Intel Developer Cloud for our PyTorch Model for Blind Stop Detection # OpenCV Person Detection # OpenCV Lane Detection # Original Image # Black and White # Canny Processing # Applying a crop to area of interest # Final by averaging out differences # Driver Alertness Detection # The driver is not looking at the road # The driver is looking at the road # Our Team # ","date":"27 April 2024","permalink":"/projects/hackdavis-2024/","section":"Projects","summary":"HackDavis 2024 # Our Github","title":"hackdavis-2024"},{"content":"","date":"27 April 2024","permalink":"/tags/jupyter-notebook/","section":"Tags","summary":"","title":"Jupyter Notebook"},{"content":" IntelliSat_Prototype # The prototype for the IntelliSat OS developed by Flight Software team, combining components such as the Logging system, Scheduler, and Error Correction methods.\n","date":"26 April 2024","permalink":"/projects/intellisat_prototype/","section":"Projects","summary":"IntelliSat_Prototype # The prototype for the IntelliSat OS developed by Flight Software team, combining components such as the Logging system, Scheduler, and Error Correction methods.","title":"IntelliSat_Prototype"},{"content":" rsbmalloc # A binned allocator for Rust. It’s quite simple, but reasonably fast single and multi-threaded. Single-threaded, it generally similar to the built-in allocator, sometimes faster, but sometimes with higher memory usage. Multi-threaded, it ranges from similar speed to quite a bit slower. It’s pure Rust, so it should work smoothly on any platform that provides standard mmap and munmap functions, and also Windows (though Windows support isn’t tested).\nRelies exclusively on thread-local caches for multi-threaded support. 4 times the number of cpus are created on the first allocation and no more are created after that, so each ‘thread-local’ cache is fully thread-safe in case it is reused between threads.\nrsbmalloc is entirely a binned allocator, with bins ranging from 4 bytes to 16 KiB (some ARM pages sizes are 16 KiB). If an allocation is larger than 16 KiB, it gets counted as a large allocation and goes straight to mmap and munmap. So, when freed in Rust, it gets munmap-ed. Bins, however, are allocated a page at a time as necessary and are never released back to the OS. Freed slots just act as a linked list that can be reused by the same thread (or another thread that scores the same thread cache).\nIt implements the GlobalAllocator trait, and comes with a single-threaded no_std version. The no_std version still requires a libc with mmap and munmap or Windows, but it doesn’t depend on the Rust standard library. Note that the no_std version is still thead-safe, it just doesn’t use the thread-local caches, so it’s a lot slower because it relies of spinlocks when operating multi-threaded. On the other hand, it uses less memory and would be a similar speed if there’s no lock contention. Once the allocator-api is stable, it should be a fairly easy port to that.\nrsbmallocc provides a slightly slower C interface to rsbmalloc, including standard names (malloc, free) and prefixed names (rsbmalloc, rsbfree).\nrsbmalloc also exposes the page-only allocator it uses under the hood.\nA Broch Web Solutions project.\nCheck out the blog post for more info.\n","date":"19 April 2024","permalink":"/projects/rsbmalloc/","section":"Projects","summary":"rsbmalloc # A binned allocator for Rust.","title":"rsbmalloc"},{"content":" FSW Training # This training is meant to be a guide to FSW\u0026rsquo;s bigger project, IntelliSat. It\u0026rsquo;s meant to be completed alonside the General FSW Onboarding\n","date":"17 April 2024","permalink":"/projects/fsw_training/","section":"Projects","summary":"FSW Training # This training is meant to be a guide to FSW\u0026rsquo;s bigger project, IntelliSat.","title":"FSW_Training"},{"content":" Pretzel 🥨 Modern, open-source Jupyter alternative. Try it here » Discord · Website · Issues · Contact https://github.com/pretzelai/pretzelai/assets/121360087/ff4643b1-c931-410e-aa0b-9233e0766223\nPretzel is a fork of Jupyter with the goal to improve Jupyter\u0026rsquo;s capabilities. We\u0026rsquo;ve added AI code generation and editing, inline tab completion, sidebar chat and error fixing to Jupyter for now with a lot more to come.\nSwitching to Pretzel from Jupyter is extremely easy since it\u0026rsquo;s simply an improved version of Jupyter. All of your Jupyter config, settings, keybindings, and extensions will work out of the box.\nQuick Start # Installation: pip install pretzelai then run pretzel lab to open the web interface. OR, use our free hosted version: pretzelai.app See this if you get errors during installation Simply start typing in a cell to get inline tab completions In any Jupyter cell, click “Ask AI” or press Cmd+K (Mac) / Ctrl+K (Linux/Windows) to prompt AI Use the AI Sidebar with Ctrl+Cmd+B (Mac) or Ctrl+Alt+B (Linux/Windows) to chat with AI, generate code, and ask questions. Type @ to trigger auto-complete with function and variable names. To use your own model (OpenAI, Anthropic/Claude, Ollama or Groq), see the Configuration section Our roadmap includes building features such as:\nNative AI code generation and understanding features similar to Cursor Frictionless realtime collaboration: pair-programming, comments, version history, etc. SQL support (both in code cells and as a standalone SQL IDE) Visual analysis builder (see more here) VSCode like code-writing experience using Monaco 1-click dashboard creation and sharing from Jupyter notebooks Installation # Using pip # Install Pretzel with pip\npip install pretzelai For conda, install pip first with conda install pip and then pip install pretzelai.\nThen, start Pretzel with:\npretzel lab You\u0026rsquo;ll be able to access the Pretzel interface via the provided URL.\nTo use your own AI model, see the Configuration section.\nRunning within a docker container # If you\u0026rsquo;re having trouble installing Pretzel (for eg on Windows), you can run it in a Docker container.\nCreate a Dockerfile: FROM python:3.9-slim RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ build-essential \\ gcc \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* WORKDIR /root/pretzel RUN pip install pretzelai EXPOSE 8888 CMD [\u0026#34;pretzel\u0026#34;, \u0026#34;lab\u0026#34;, \u0026#34;--ip=0.0.0.0\u0026#34;, \u0026#34;--allow-root\u0026#34;, \u0026#34;--notebook-dir=/root/pretzel\u0026#34;, \u0026#34;--ServerApp.allow_remote_access=True\u0026#34;, \u0026#34;--ServerApp.token=\u0026#39;\u0026#39;\u0026#34;, \u0026#34;--no-browser\u0026#34;] In the same folder where you have your Dockerfile, run docker build -t pretzel .\nTo run pretzel, you can run: docker run --name pretzel -p 8888:8888 pretzel and once the container is running, you can access it at http://localhost:8888/lab. To stop the container, press Ctrl + C followed by docker stop pretzel.\nIf you want to access your local folder in Pretzel, you can run: docker run --rm -p 8888:8888 -v $(pwd):/root/pretzel pretzel - this will map your current directory to the docker container\u0026rsquo;s /root/pretzel folder. Make sure Docker has access to your current directory.\nTo update Pretzel to the latest version, just rebuild the Docker image with the \u0026ndash;no-cache flag: docker build --no-cache -t pretzel . and now you can run docker like step 3.\nBleeding Edge Version # You can use this Dockerfile to build and run the bleeding edge version. Follow the steps (starting at step 2) in the section Running within a docker container\nInstallation problems # If you get an error during installation that looks like this:\nFailed to build installable wheels for some pyproject.toml based projects (pystemmer) That means the installation failed to install the dependency PyStemmer. This usually happens because you don\u0026rsquo;t have the right build tools installed. To fix this:\nOn Windows: Install Miscrosoft build tools from here. Click on the Download Build Tools button and then install it. On Ubuntu (and Debian flavoured systems): Install the required build tools by running the following command in your terminal: sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install build-essential python3-dev On macOS: brew install gcc. If this doesn\u0026rsquo;t work, you may also need to run xcode-select --install Once this is done, you should be able to pip install pretzelai to install Pretzel.\nUsage # Inline Tab Completion # Start typing in a cell to get inline tab completions Wait for 1 second to trigger completions. You\u0026rsquo;ll see a little spinner just before the blue Ask AI button in the cell The default Pretzel AI Server uses Mistral\u0026rsquo;s Codestral but you can switch the inline completion model in Pretzel AI Settings. See the configuration section.\nGenerating and editing code in notebook cells # In a cell, press Cmd+K (Mac) / Ctrl+K (Windows/Linux) or click \u0026ldquo;Ask AI\u0026rdquo; to open AI prompt textbox and write your code generation/editing instruction Type @ to get a dropdown of available variables in your session. Adding this @vairable to the prompt will send its value to the AI We automatically send relevant code in the current notebook as context to the AI If there\u0026rsquo;s existing code in a cell, the prompt will edit the existing code If you select/highlight some code in the cell, only the selected code will be edited You can accept/reject the response or edit your prompt if you want to re-submit with modifications Use ↑ / ↓ to cycle through prompt history Using the AI Sidebar # Use Ctrl+Cmd+B (Mac) / Ctrl+Alt+B (Linux/Windows) or the Pretzel Icon on the right sidebar to activate the AI Sidebar You can ask questions, generate code, or search for existing code The AI always uses the code in the active cell as context. If you highlight some code in the active cell, only the highlighted code will be used as context We automatically send relevant code in the current notebook as context to the AI You can also use the @ syntax to refer to variables and dataframes in memory, similar to the notebook cells Example uses of AI Sidebar:\n\u0026ldquo;Modify the function my_function to be more efficient\u0026rdquo; ← this will find the function my_function in the whole notebook and modify it \u0026ldquo;Where is the code that removes outliers\u0026rdquo;? ← this will search the notebook for code that removes outliers and show it to you (While cursor is in a cell with some code) \u0026ldquo;Can you explain what this code does?\u0026rdquo; ← this will explain the code in the current cell \u0026ldquo;Plot a histogram of @df for age\u0026rdquo; ← this will generate code to plot a histogram of the \u0026lsquo;age\u0026rsquo; column from the dataframe named \u0026lsquo;df\u0026rsquo; \u0026ldquo;Calculate the mean revenue in @salesdata for each product type\u0026rdquo; ← _this will generate code to calculate mean revenue by product type using the sales_data dataframe Adding code in the middle of existing code # Put your cursor either on an empty line or an existing line of code. Bring up the AI prompting text box with Cmd+K Start your prompt with the word inject or ij (case-insensitive) - this tells the AI to only add new code and not edit the existing code in the cell Code will be added one line below where your cursor was placed Fix errors with AI # When there\u0026rsquo;s an error, you\u0026rsquo;ll see a button on top-right \u0026ldquo;Fix Error with AI\u0026rdquo;. Click it try fixing the error Configuration # Pretzel works out-of-the-box, no configuration needed. We DO NOT store any code or data you send to the Pretzel AI Server.\nStill, if you want to use a different AI model, you can configure Pretzel to use AI models from multiple different vendors, including local models. In this case, the AI request goes directly from your computer to the AI service (OpenAI, Anthropic etc).\nOpen the Settings menu in the top menubar, then click Pretzel AI Settings Under AI Settings you can pick which AI models to use for chat and for inline completion (see screenshot below) We recommend using GPT-4 class models (e.g., GPT-4 Turbo, GPT-4o, Claude-3.5 Sonnet, Claude-3 Opus and Llama-3.1 405B) for the best performance Under the section Configure AI Services, you can\nEnable or disable certain AI services (for eg, Azure and Ollama are disabled by default but can be enabled) Enter your API key or URL as needed for each service Remember to save your settings after making changes. Pretzel will validate your configuration to ensure everything is set up correctly.\nPlease note: We haven\u0026rsquo;t tested Azure Enterprise OpenAI models yet. If you find any bugs, please report them in GitHub issues and we\u0026rsquo;ll fix them ASAP.\nFeedback, bugs and docs # Please report bugs here: https://github.com/pretzelai/pretzelai/issues Have any feedback? Any complains? We\u0026rsquo;d love feedback: founders@withpretzel.com Jupyter specific information # The original Jupyter documentation is available here and the Jupyterlab README is available here.\nPrivacy Policy, Data Collection and Retention # We collect no personal information. We use basic telemetry for only the AI features we\u0026rsquo;ve built - for example, when you click on \u0026ldquo;Ask AI\u0026rdquo;, we receive an event that someone clicked on \u0026ldquo;Ask AI\u0026rdquo;. We only associate an anonymous ID to your user. If you allow cookies, that helps us tell that it\u0026rsquo;s the same user across multiple browser sessions (which is very helpful!). If you don\u0026rsquo;t allow cookies, every time you open a browser, you\u0026rsquo;re a new anonymous user to us.\nWe also collect prompts (but not the responses) for the AI features we\u0026rsquo;ve built. This can be turned off in the settings (Settings \u0026gt; Pretzel AI \u0026gt; Uncheck Prompt Telemetry) but we\u0026rsquo;d really appreciate if you didn\u0026rsquo;t - this is very helpful in improving our prompts.\nWe do not collect any code whatsoever. Even when you use Pretzel\u0026rsquo;s cloud AI server for completions, we don\u0026rsquo;t store any of this code.\nIf you use the hosted version of Pretzel ( https://pretzelai.app), we create a user for you based on your email address. You can always simply log-in and delete any data you may have stored on our hosted server. We make no backups or copies of your data.\nOur hosted server is free to use. However, we will delete your data and your account 30 days after your last login. If you\u0026rsquo;d like to delete your account sooner, please email us at founders@withpretzel.com with the subject line \u0026ldquo;Account Deletion\u0026rdquo; and we\u0026rsquo;ll delete your account immediately.\nFAQ # Q. What happened to the old version of Pretzel AI - the visual, in-browser data manipulation tool?\nA. It\u0026rsquo;s available in the pretzelai_visual folder here. Please see this PR for more info.\nQ. What AI model does Pretzel use?\nA. Pretzel uses different AI models for various tasks:\nDefault model: GPT-4o\nOffers a good balance between speed and quality Can be changed in Pretzel Settings if you\u0026rsquo;re using your own API key Inline completions: Mistral\u0026rsquo;s Codestral model\nExcellent for code completion Very fast performance (22B parameter model) Fallback option:\nIf you\u0026rsquo;re using your own API key without providing a Mistral API Key, Pretzel will use GPT-4o for inline completions as well We\u0026rsquo;re continuing to experiment with models and supporting local models and Anthropic\u0026rsquo;s Claude is at the top of our list.\nQ. What about feature X?\nA. There\u0026rsquo;s a ton we want to build. Please open an issue and tell us what you want us to build!\nQ. Where\u0026rsquo;s the roadmap?\nA. We have a rough roadmap at the top of this README. There are many features we\u0026rsquo;d like to build, but there\u0026rsquo;s just two of us. So, we\u0026rsquo;re collecting feedback about what would be most helpful. Please open an issue or just email us with your feedback! Based on what we find, we\u0026rsquo;ll prioritize our roadmap.\nQ. Why are you using the AGPL license? Or, why not use MIT/BSD3 licenses?\nA. Our goal with building Pretzel is to make an amazing data tool that is free for both individuals and companies to use. That said, we are a two-person startup - and we don\u0026rsquo;t want some third party to just take our code and sell a hosted version of it without giving back to the community. Jupyter code is licensed as BSD-3 and if we keep our new code BSD-3 licensed, there would be no way to stop a third party from doing this. As a result, we went with the AGPLv3 license for all the new code. This ensures that if someone else does want to take our code and sell it (SaaS or otherwise), they have to open-source all of their modifications under AGPLv3 as well.\nQ. Why a fork of Jupyter? Why not contribute into Jupyter directly?\nA. This deserves a longer answer but here\u0026rsquo;s the short answer: We\u0026rsquo;ve set out to make the new de-facto, modern, open-source data tool. Initially, we wanted to start from scratch. However, after talking to several data professionals, we realized it will be very hard to get people to switch to a new tool, no matter how good. The best way to get people to switch is to not have them switch at all. That\u0026rsquo;s why we decided to fork Jupyter - for the near zero switching costs. Also, Jupyter is a mature product, and we\u0026rsquo;re shipping feature really fast - frankly, at the pace we\u0026rsquo;re shipping features, the code we write won\u0026rsquo;t be accepted into the Jupyter codebase 😅. There are also many downsides to this decision - we\u0026rsquo;ve had to spend considerable time understanding the whole Jupyter ecosystem and multiple codebases, the complex release processes, the various APIs etc. However, we think this is the right decision for us.\nQ. My company is worried about using an AGPLv3 licensed tool. What can I do?\nA. The AGPL is a barrier ONLY IF you\u0026rsquo;re modifying Pretzel AND redistributing it to the public. If you\u0026rsquo;re simply using it as a tool in your company (even with modifications), the AGPL DOES NOT ask you to share your code. Still, if AGPL is an issue for you, please contact us, and we can figure out something that works.\nQ. How are you planning on making money? OR, how are you free? I\u0026rsquo;m worried that you\u0026rsquo;ll make this tool paid in the future.\nA. We\u0026rsquo;re planning on selling a hosted version of the tool to companies to make money. This hosted version will probably have some company specific features that individuals don\u0026rsquo;t want or need such as data access controls, connectors for data sources, integration with GitHub, hosted and shareable dashboard, scalable and on-demand compute for large data jobs etc. We will not retroactively make Pretzel\u0026rsquo;s individual version paid.\n","date":"23 March 2024","permalink":"/projects/pretzelai/","section":"Projects","summary":"Pretzel 🥨 Modern, open-source Jupyter alternative.","title":"pretzelai"},{"content":" linear-regression-rs # Super simple linear regression library in Rust ( 🦀 ).\nInstallation # Add this to your Cargo.toml:\n[dependencies] linear_regression_rs = { path = \u0026#34;../linear-regression-rs\u0026#34; } Usage # Here\u0026rsquo;s a quick example to get you started:\nuse linear_regression_rs::{LinearFrame, Regression}; fn main() { let mut frame = LinearFrame { x: vec![1.0, 2.0, 3.0, 4.0, 5.0], y: vec![1.0, 2.0, 4.0, 4.0, 5.0], verbose: true, }; let (slope, b) = frame.regression(1000, 0.01); println!(\u0026#34;Model: y = {}x + {}\u0026#34;, slope, b); } API # fn squared_error(\u0026amp;mut self, f: \u0026amp;dyn Fn(f64) -\u0026gt; f64) -\u0026gt; f64; fn mean_squared_error(\u0026amp;mut self, f: \u0026amp;dyn Fn(f64) -\u0026gt; f64) -\u0026gt; f64; fn gradient_descent(\u0026amp;mut self, slope: f64, b: f64, learning_rate: f64) -\u0026gt; (f64, f64); fn regression(\u0026amp;mut self, epoch: i32, learning_rate: f64) -\u0026gt; (f64, f64); Contributing # Contributions are welcome! Please feel free to submit a pull request or open an issue if you have suggestions or find a bug.\nLicense # This library is released under the MIT License. See the LICENSE file for more details.\n","date":"9 March 2024","permalink":"/projects/linear-regression-rs/","section":"Projects","summary":"linear-regression-rs # Super simple linear regression library in Rust ( 🦀 ).","title":"linear-regression-rs"},{"content":" Vectors-c # Vector math in C\nint main() { struct Vec3 *v = build_vec3(1, 2, 3); struct Vec3 *u = build_vec3(4, 5, 6); double res = dot_vec3(v, u); // (1*4) + (2*5) + (3*6) assert(res == 4 + 10 + 18); struct Vec2 *a = build_vec2(1, 2); struct Vec2 *b = build_vec2(3, 4); double res2 = dot_vec2(a, b); // (1*3) + (2*4) assert(res2 == 3 + 8); interactive(); return 0; } ","date":"11 February 2024","permalink":"/projects/vector-c/","section":"Projects","summary":"Vectors-c # Vector math in C","title":"vector-c"},{"content":" Format filename # Format filenames to a command-line friendly format\nWhy? # I often find myself in the command-line running programs like Neovim (aliased to v) on text files or Zathura on PDF files. I usually name files with underscores, as it\u0026rsquo;s easier to type out than a space when using the command-line. Similarly, I often don\u0026rsquo;t use slashes, parentheses, or any other character that needs to be excaped for the reason of not having to type out more than needed. However, not all files are named this way by default, so this program makes it easier to change it quickly. It\u0026rsquo;s also a good excuse to write something in Rust ( 🦀 ).\nUsage # formatfilename 0.1.0 Format filenames to a command-line friendly format USAGE: formatfilename \u0026lt;filename\u0026gt; FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: \u0026lt;filename\u0026gt; Input file ","date":"17 January 2024","permalink":"/projects/format-filename/","section":"Projects","summary":"Format filename # Format filenames to a command-line friendly format","title":"format-filename"},{"content":" Trig Function Viewer # Create a visualization of the Sine and Cosine functions on the Unit Circle using p5.js\n","date":"24 December 2023","permalink":"/projects/trig-function-viewer/","section":"Projects","summary":"Trig Function Viewer # Create a visualization of the Sine and Cosine functions on the Unit Circle using p5.","title":"trig-function-viewer"},{"content":" Basis (math language) # A math console language with a bunch of useful functions and constants\nInterpreter Progress # The lexer has been completed. Next up is the parser and code gen.\nSyntax # Note: The \u0026gt; at the start of the line is the prompt. This prompt will evaluate the line and then display the value of the console attribute.\nExpressions # Basis supports simple expressions like addition through the summation function (See more about functions below), which takes exactly two arguments on the left.\n\u0026gt; 1 1 + int: 2 Note: 1 1 + is called an anonymous expression because it is not given a name.\nBasis supports fraction representation as the default for rational numbers. Rationals stay as fractions until you convert to a float (See more below).\n\u0026gt; 2 3 / ratio: 2 / 3 Basis allows you to convert fractions to decimal with the decimal . function which operates on one rational to the left.\n\u0026gt; (2 5 /) . dec: 0.4 The precision of rations can be specified, they default to 32 bit max, but can be set with the tilde function. The shorthand prec will show the precision of the ratio in the console.\n\u0026gt; (1 3 /) ~ ratio (prec 1): 1 / 3 \u0026gt; (1 3 /) ~ . dec: 0.3 Variables # Number types # There are many types along with the number type which cannot be instantiated but which gets derived from by every type listed below called the \u0026ldquo;number types\u0026rdquo;\nint ratio real dec complex imaginary size \u0026gt; a int 5 = a = int: 5 \u0026gt; b ratio (3 4 /) = b = ratio: 3 / 4 Zero in any number type, when cast to a bool is false\nMethods on number types # Every number type, has a method to turn each number type into any other number type. If this method is called, is saves the value of the newly casted value into it\u0026rsquo;s own data so that it can be called again with no expense. This is a speed vs memory tradeoff.\nLiteral # Literal can wrap the number type and other types. It essentially is like a normal variable, but instead of being included in the expression execution, it stays a literal.\n\u0026gt; mynum literal (3 4 /) = literal: mynum \u0026gt; mynum unwrap ratio: 3 / 4 Why have a literal like this? Sometimes you may want to apply functions to an expression but not get a specific decimal or other numeric output, you may want another expression with the literals still persisting. Say if you wanted to take an expression 0 cos and multiply it by pi, instead of getting 3.14... you would get pi (because 0 cos is 1), the literal is not evaluated until you unwrap it by doing 0 cos pi * unwrap to get some decimal representation of pi.\nStrings # \u0026gt; \u0026#34;hello\u0026#34; string: \u0026#34;hello\u0026#34; -\u0026gt; \u0026#34;size: 6\u0026#34; Size # The size type is a special type around the real type that denotes that it is a size of another type\n\u0026gt; 4 size size: 4 Why is the size in double quotes? The tuple syntax and it\u0026rsquo;s console attribute has its size in the tuple parentheses, so it seems only natural that the console attribute for string would also be have its size in the characters that surround its instantiation.\nLate initialization # \u0026gt; a int = a = late: int The late type # The late type is a wrapper around the option type, making any late initialization not of the type of the vairable, but of type late, which is just a wrapper for the option type, so late initialized types are all basically options (but technically of type late).\nAfter adding a value to a variable defined as a late type, it just becomes the type that late was given as a genetic. E.g. a int = is a late type passed int as a generic.\nProvided Constants (of type literal) # A few constants are provided such as g (Gravity of Earth), e (Euler\u0026rsquo;s number), pi (Pi), h (Planck constant)\n\u0026gt; e literal: e \u0026gt; pi literal: pi \u0026gt; pi 0 ~ literal (prec 0): 3 \u0026gt; pi 0 ~ . dec: 3 \u0026gt; pi 1 ~ . dec: 3.1 \u0026gt; pi desc \u0026#34;Ratio of a circle\u0026#39;s circumference to it\u0026#39;s radius\u0026#34; \u0026gt; e . dec: 2.718281828459045 Bool # The bool type is either true or false\n\u0026gt; a bool true = a = bool: true Nil type # The nil type is a type of type nil\nWhen cast to a bool, it becomes false later referred to as a fasle type\nMore math functions # Multiplication # Used with *.\nMultiplication can be applied to a vector as a scalar)\nI/O functions # print # The print function operated on the type type\n\u0026gt; a int 1 = a = int: 1 \u0026gt; a print 1 input # \u0026gt; a int = a = nil \u0026gt; a input = a = \u0026lt;value provided by input\u0026gt; ~ or more concisely ~ \u0026gt; a int input = Comments # Comments are done like the following with a space before and after any text inside the comment as an homage to Cruz Lang and later Planck Lang and Z-Flat along with many more.\n\u0026gt; ~~ hello world ~~ comment: ~~ hello world ~~ Comments are the comment type and can be converted into the string type\nFunctions # Functions in basis can be created with the following syntax\n\u0026gt; f (x int) =: x 1 + function: f (int) -\u0026gt; int \u0026gt; 1 f 2 Option # The option type is a wrapper for any type which has an attribute exists which is a bool.\nThere are two methods specific to option, some and none inspired by Rust. The some method is equivalent to comparing the exists attribute to true and the none method is evaluated to comparing the exists to false.\nThe option type has an attribute called value which is of type type and is defined generically as seen in the section about generic types.\nThe option type will return the exists bool when cast into a bool\nClosures # The function syntax may seem weird at first, but when given the closure syntax, it makes slightly more sense.\nSingle line closure # The following is a new closure (scope) that is evaluated as an expression\n\u0026gt; : (1 1 +) closure: 1 1 + \u0026gt; : (1 1 +) unwrap int: 2 Multi-line closure # The following is a new multi-line closure (scope) that is evaluated as an expression as well\n\u0026gt; { 1 1 + } closure: 1 1 + \u0026gt; { 1 1 + } unwrap int: 2 The unwrap function # Many types are wrappers, like literal, closure, option. The unwrap function takes a type of type (i.e. any type) and unwraps the type to its inner type.\nGeneric Types # Since number cannot be initialized, if we want to write a function that works for any number, we must write a function using the generic syntax that allows any of the types specified by the syntax to operate on. Generic types are only allowed in function and can only be initialized as parameters.\n\u0026gt; f (x \u0026lt;number\u0026gt;) x \u0026lt;1\u0026gt; + function: f (\u0026lt;number\u0026gt;) -\u0026gt; \u0026lt;number\u0026gt; Do types have attributes or method or both? # All attributes are actually functions (lazily loaded when specified to be so)\n\u0026gt; (3 4 /) . ~ create a ratio of 3 / 4 and then call the . function to cast it into a decimal type ~ dec: 0.75 \u0026gt; a (3 4 /) = a = ratio: 3 / 4 \u0026gt; a . ~ cast a into a decimal type, also stores the decimal representation of a into a ~ dec: 0.75 \u0026gt; a . ~ load the existing decimal cast of value a ~ dec: 0.75 Note: a . would be called an anonymous expression because it is not given a name.\nAny use of the generic type in the type declaration must be wrapped in \u0026lt; \u0026gt; and any actual creation of data must also be wrapped in \u0026lt; \u0026gt;, which when evaluated will be cast into the type provided to the function. (E.g. 2 f where 2 is an int, will return 3 because internally the \u0026lt;1\u0026gt; was cast to an int with 1 int)\nMaking custom types # TODO Deriving from types # TODO Vectors # Vectors in basis are like mathematical vectors by default but can also be used as containers\n\u0026gt; [1 2 3] vector[int]: [1 2 3] -\u0026gt; [size: 1x3] Multidimensional vectors\n\u0026gt; [1 2 3; 2 3 4; 3 4 5] vector[int]: [[1 2 3] [2 3 4] [3 4 5]] -\u0026gt; [size: 3x3] Vector operations + and -\n\u0026gt; [1 2] [2 3] + vector[int]: [3 5] -\u0026gt; [size: 1x2] \u0026gt; [1 2] [2 3] - vector[int]: [-1 -1] -\u0026gt; [size: 1x2] Dot product in basis\n\u0026gt; [1 2 3] [2 3 4] . vector[int]: [2 6 12] -\u0026gt; [size: 1x3] Containers # The vector type is a type of container, but basis also has support for tuple and set, both of which are derived from container.\n\u0026gt; [2 3 4 5 5] vector[int]: [2 3 4 5 5] -\u0026gt; [size: 1x5] \u0026gt; (2 3 4 5 5) tuple[int]: (2 3 4 5 5) -\u0026gt; (size: 5) \u0026gt; {2 3 4 5 5} set[int]: (2 3 4 5) -\u0026gt; {size: 4} There are more containers like sample and population for statistics described below.\nStatistics # Mean of a vector, tuple, or set. For simplicity, this function is shown described with only the ratio type, but the real definition would be written for the number type with the generic type syntax.\n\u0026gt; mean (c container[ratio]) =: sum: ratio = 0 count: int = 0 for x: ratio in c { sum x += count 1 + count += } \u0026gt; [1 2 3] mean ratio: 2 \u0026gt; (1 2 3) mean ration: 2 Sample and Population # The sample and population types are very similar in that they wrap the tuple type. They are useful because the statistical functions (as described in this section) apply to both of them differently.\nStatistical tests\n\u0026gt; a (1 2 3) sample = ~ creates a tuple, casts it to a sample type, assign it to a ~ a = sample: (1 2 3) -\u0026gt; (size: 3) \u0026gt; b (1 2 4) sample = ~ creates a tuple, casts it to a sample type, assign it to b ~ b = sample: (1 2 4) -\u0026gt; (size: 3) \u0026gt; test a b ttest = test = ttestresult: (pvalue: 0.768) (t: -0.316) \u0026gt; test pvalue dec: 0.768 Scripting in basis # Entire scripts can be written outside of the REPL to be saved and run.\nminecraft_coord_convert.basis\noverworld_to_neither(coords vector[int]) =: 8 coords * neither_to_overworld(coords vector[int]) =: coords 8 / out_coords [100, 40, 10] overworld_to_neither = out_coords print Magic # You may have noticed that the previous examples all have a console. How does the console know what to display below each line? Well, it is[1] lazily computed when any type is created. Any type derived from type have a few common attributes that are either lazily loaded if they are specific to the instance (e.g. like the representation for the console (console representation), or the printable representation). Other attributes like desc are pointers to a constant shared description that is the same for in instance of any type. This might be something like the desc for int, \u0026ldquo;Integer numeric type that stores whole positive and negative numbers\u0026rdquo;. The console attribute is actually a function and may use the printable attribute (which is also a function). For example, printable for an int might be \u0026ldquo;1\u0026rdquo;, and the console will be \u0026quot;int: {}\u0026quot; printable, which in full form will look like console (): \u0026quot;int: {}\u0026quot; printable\n[1] Note: This is just the document outlining the implementation, the implementation is not yet written, so technically it isn\u0026rsquo;t lazily loaded yet, but will be when the implementation is written in it\u0026rsquo;s entirety. Other instances of \u0026ldquo;is\u0026rdquo; statements may be the same case.\n","date":"16 December 2023","permalink":"/projects/basis/","section":"Projects","summary":"Basis (math language) # A math console language with a bunch of useful functions and constants","title":"basis"},{"content":" color-valgrind # Highlight important information from the already great valgrind\nWhy? # I often switch quickly between valgrind output and code. This leads to me lossing my place often. I also re-run valgrind many times in s short period of time, leading to me having to find what I was looking at previously to compare. All of this is a bit easier with colorful out.\nNew # Previously # Install # git clone https://github.com/JakeRoggenbuck/color-valgrind.git cd color-valgrind cargo install --path . ","date":"12 November 2023","permalink":"/projects/color-valgrind/","section":"Projects","summary":"color-valgrind # Highlight important information from the already great valgrind","title":"color-valgrind"},{"content":" Preface # React is extremely popular and has become ubiquitous in frontend development. There are many reasons you would want to learn React. It\u0026rsquo;s used in industry, it\u0026rsquo;s a required skill in many applications for job, and it\u0026rsquo;s a great tool to use to build amazing projects. Here is a simple list of tips and resources to help you understand React. If you\u0026rsquo;ve used the terminal before, you can safely jump into Starting my project otherwise the next section will guide you threw it.\nFirst steps # The first step is to jump right in an make a project! For this, you\u0026rsquo;re going to have to use the terminal. When you first open your terminal, you should be in your \u0026ldquo;home\u0026rdquo; folder. It\u0026rsquo;s usually named after your username.\nOpen your terminal and navigate (using cd and ls) to the place where you keep all of your code. If you don\u0026rsquo;t have a specific place, make a folder in your Documents folder, and call it Code. You can do this by running cd Documents then running mkdir Code\nNavigate into the Code folder using cd Code\nStarting my project # You may need to install npx. Now it comes with npm, so make sure you have that installed along with node. That\u0026rsquo;s a lot to keep track of, so if you\u0026rsquo;re just starting out, you can just install node and everything should be installed with it. If you are on linux, you can use your package manager to install node if it\u0026rsquo;s not included, if you do not use linux, you can use this link https://nodejs.org/en/download.\nI highly recommend learning by doing. Let\u0026rsquo;s start a small learning project to test out React. To start, type out the following command in the directory you save coding projects in. This will create a basic template for us to add to. It will name the folder \u0026ldquo;my-app\u0026rdquo;. It may take some time (a few minutes).\nThe following will run the program called create-react-app, which creates the app. Here is their website about it create-react-app.dev. Do note, that it\u0026rsquo;s no longer recommended by the React team [1].\nnpx create-react-app my-app Now that it\u0026rsquo;s done, cd my-app into the project to start coding!\nRun the command cd src to see the main source code for your new project.\nIf you run ls, you should see the following files.\nApp.css App.js App.test.js index.css index.js logo.svg reportWebVitals.js setupTests.js You can safely ignore everything but App.js.\nOpen App.js in your favorite text editor.\nYou should see the following.\nimport logo from \u0026#39;./logo.svg\u0026#39;; import \u0026#39;./App.css\u0026#39;; function App() { return ( \u0026lt;div className=\u0026#34;App\u0026#34;\u0026gt; \u0026lt;header className=\u0026#34;App-header\u0026#34;\u0026gt; \u0026lt;img src={logo} className=\u0026#34;App-logo\u0026#34; alt=\u0026#34;logo\u0026#34; /\u0026gt; \u0026lt;p\u0026gt; Edit \u0026lt;code\u0026gt;src/App.js\u0026lt;/code\u0026gt; and save to reload. \u0026lt;/p\u0026gt; \u0026lt;a className=\u0026#34;App-link\u0026#34; href=\u0026#34;https://reactjs.org\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34; \u0026gt; Learn React \u0026lt;/a\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;/div\u0026gt; ); } export default App; This is the basic source code. There are three main parts to it.\nFirst, the imports. These tell the interpreter where resources are located, like our CSS files and images, but also other javascript files.\nimport logo from \u0026#39;./logo.svg\u0026#39;; import \u0026#39;./App.css\u0026#39;; Secondly, the function in jsx. Here I have removed all of the html-like code and just kept the function. These two parts are the basis for all of it.\nfunction App() { return ( // ... ); } export default App; Before we look at the third part, let\u0026rsquo;s add a simple text field.\nfunction App() { return ( \u0026lt;p\u0026gt;Hello!\u0026lt;/p\u0026gt; ); } export default App; The third part is the html-like code we removed in the second example (where the ellipsis is). Here is what that looks like.\n\u0026lt;div className=\u0026#34;App\u0026#34;\u0026gt; \u0026lt;header className=\u0026#34;App-header\u0026#34;\u0026gt; \u0026lt;img src={logo} className=\u0026#34;App-logo\u0026#34; alt=\u0026#34;logo\u0026#34; /\u0026gt; \u0026lt;p\u0026gt; Edit \u0026lt;code\u0026gt;src/App.js\u0026lt;/code\u0026gt; and save to reload. \u0026lt;/p\u0026gt; \u0026lt;a className=\u0026#34;App-link\u0026#34; href=\u0026#34;https://reactjs.org\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34; \u0026gt; Learn React \u0026lt;/a\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;/div\u0026gt; One common point of confusion is that this is HTML. It close, but not exactly. It\u0026rsquo;s called jsx. It\u0026rsquo;s kinda like a combination of JS and HTML. It makes it really easy to write frontend.\nNow that we have looked at the code, go back to that App.js and try to predict what you will see when we open the page.\nLet\u0026rsquo;s open the page. Go back to your terminal, navigate back to the my-app folder, run the following command to install the needed requirements.\nnpm install Note: you can do npm i for short.\nNext is to run it! Do so by running the following\nnpm run start The page should now automatically open in your browser!\nYou should see this the React logo with some text that says Edit src/App.js and save to reload..\nYou\u0026rsquo;ve created a project!\nNext level # It\u0026rsquo;s now your chance to take this to the next level. Make something interesting! Try things out!\nHere are some resoruces to help on your journey.\nReact # Getting started with React React Quick Start React.js Cheatsheet JavaScript # MDN Docs Basics Typescript # Handbook (New Programmers) Handbook (For JS Programmers) Learn Typescript ","date":"1 October 2023","permalink":"/posts/learning-react/","section":"Posts","summary":"Preface # React is extremely popular and has become ubiquitous in frontend development.","title":"Learning React"},{"content":"","date":"1 October 2023","permalink":"/tags/react/","section":"Tags","summary":"","title":"react"},{"content":"","date":"18 September 2023","permalink":"/tags/c++/","section":"Tags","summary":"","title":"C++"},{"content":" JABACAT-created machine learning library from scratch. Build # [!IMPORTANT] This project uses The Meson Build system. Follow instructions posted on the website to install meson. On Windows, there is a winget package available.\nTo build, setup the build directory. Any configuration steps should be done now (specifying compiler or linker, etc.).\nmeson setup build # replace `build` with any arbitrary output build directory Then, compile it.\nmeson compile -C build # replace `build` with the directory specified in the previous step The outputs are in build/ (or whatever directory specified). Install the outputs:\nmeson install -C build # again, replace `build` Test # This project uses Catch2 as its testing framework.\nRun meson test to test the project.\nmeson test -C build # replace `build` Unfortunately, meson doesn\u0026rsquo;t provide good output when a test fails. Running the test executable manually is often more helpful. Look under build/tests/ to find the right executable. The executable for testing the core library is core_tests.\nFormat # We use clang-format to check our formatting. Before you push your code, you can run the following that finds all source code files, and then formats them all in place.\nfind . -iname \u0026#34;*.hpp\u0026#34; -o -iname \u0026#34;*.cpp\u0026#34; -o -iname \u0026#34;*.c\u0026#34; -o -iname \u0026#34;*.h\u0026#34; -o -iname \u0026#34;*.tpp\u0026#34; | xargs -I {} clang-format {} You can also add this to your editors formatting system or some precommit step.\n","date":"18 September 2023","permalink":"/projects/jml/","section":"Projects","summary":"JABACAT-created machine learning library from scratch.","title":"jml"},{"content":" Taxonomy Search # Taxonomy search using SolidJS, FastAPI, Tailwind, and the full ITIS database\nImage # backend # Install # python3 -m venv venv source venv/bin/activate pip install uvicorn fastapi Running # uvicorn main:app ","date":"22 July 2023","permalink":"/projects/taxonomy-search/","section":"Projects","summary":"Taxonomy Search # Taxonomy search using SolidJS, FastAPI, Tailwind, and the full ITIS database","title":"taxonomy-search"},{"content":" IntelliSat # Welcome to code respository for the Space And Systems Satellite Club\u0026rsquo;s REALOP 1 Flight Software! The code here will be run on our flight computer, the Orbital Platform, which will be responsible for controlling and monitoring the CubeSat\u0026rsquo;s various subsystems. Details regarding the Orbital Platform are available on this github repository: uwu64/orbital-platform.\nGetting Started # To get started with our codebase, you\u0026rsquo;ll need to have a basic understanding of C programming. In order to work on the codebase itself, you\u0026rsquo;ll need the tools to be able to clone, build, and eventually flash and debug the code. For this, refer to the following section:\nGetting Started For those developing code, many elements might be hardware specific. Reference manuals, datasheets, and programming guides of the various chips on the Orbital Platform are located under:\nReference_Manuals Note about Board Revisions # The Orbital Platform goes through revisions. IntelliSat attempts to stay functional on each revision. However, certain GPIO and hardware configurations change across revisions, and thus change within IntelliSat. When using IntelliSat, keep the following in mind:\nBased on the Revision you are working with, make sure to change the OP_REV macro in Src/globals.h before building or flashing IntelliSat Differences between the Revisions can be found here Certain LED function names differ between revisions, so it\u0026rsquo;s important to know those ","date":"12 July 2023","permalink":"/projects/intellisat/","section":"Projects","summary":"IntelliSat # Welcome to code respository for the Space And Systems Satellite Club\u0026rsquo;s REALOP 1 Flight Software!","title":"IntelliSat"},{"content":" more-than-half # Given an array of slightly less than half random numbers and slightly more than half of one specific number, find the the number that makes up slightly more than half. Below are the speeds of multiple different algorithms to do this.\nResults # find_more_than_half_naive_hashmap_approach:\tμ: 367376311.2 nanos, σ: 68236547.97368439, n: 30, reps: 100 find_more_than_half_array_index_approach:\tμ: 9152388.3 nanos, σ: 1705146.465021606, n: 30, reps: 100 find_more_than_half_array_index_approach_iter_max:\tμ: 9270903.3 nanos, σ: 1726418.191527093, n: 30, reps: 100 find_more_than_half_probability_approach:\tμ: 313865.73333333334 nanos, σ: 59295.28083972807, n: 30, reps: 100 ","date":"10 July 2023","permalink":"/projects/more-than-half/","section":"Projects","summary":"more-than-half # Given an array of slightly less than half random numbers and slightly more than half of one specific number, find the the number that makes up slightly more than half.","title":"more-than-half"},{"content":" HackDavis - Hackathon # HackDavis is a hackathon group on UC Davis campus and organizes coding events.\nAggieWorks Presentation - Workshop # I presented a workshop with a group of other AggieWorks Product Managers about Product Management. I specifically talked about software development practices, agile, using git collaboratively, and other high level technical product manager topics and advice.\nMy hackdavis project # I didn\u0026rsquo;t have much time to write my project so I allocated an hour to work on it. My project slogan is Send a positive message to an Aggie! Simple can be powerful. It takes no effort to share kindness.. I am very proud of the idea here and I really liked experimenting with the OpenAI API to moderate messages.\nProjects # sxbsbamdws # The backlight brightness changing has not been working on my Arch Linux laptop and no tools could change the brightness, so I built my own tool. I have it the over-the-top acronym sxbsbamdws.\nOpenAI projects # So far, I have just experimented with the API to see how it can be used in the future. These are a lot of great tools to use here. I am very impressed. I remember using GPT2 when it came out on github in 2019 and running the docker for some text completion. It has come a long long way. I currently haven\u0026rsquo;t released any open source projects that include the use of the API but those are soon to be released.\n","date":"31 May 2023","permalink":"/devlogs/may-2023/","section":"Dev Logs","summary":"HackDavis - Hackathon # HackDavis is a hackathon group on UC Davis campus and organizes coding events.","title":"May 2023"},{"content":" UpLiftAggie # Send a positive message to an Aggie! Simple can be powerful. It takes no effort to share kindness.\nTechnical # Use OpenAI\u0026rsquo;s gpt-3.5-turbo model to determine if messages are positive for content moderation Use FastAPI to receive and store messages FastAPI Backend # Docs # ","date":"21 May 2023","permalink":"/projects/upliftaggie-hackdavis-2023/","section":"Projects","summary":"UpLiftAggie # Send a positive message to an Aggie!","title":"UpLiftAggie-HackDavis-2023"},{"content":" sxbsbamdws Simple X Backlight Substitute Because AMD Wasn\u0026rsquo;t Supported # At least not for me when I tried it a few months ago and have had none functioning backlight since.\nTODO # check if path exists and give error if not ","date":"6 May 2023","permalink":"/projects/sxbsbamdws/","section":"Projects","summary":"sxbsbamdws Simple X Backlight Substitute Because AMD Wasn\u0026rsquo;t Supported # At least not for me when I tried it a few months ago and have had none functioning backlight since.","title":"sxbsbamdws"},{"content":"404: Not Found\n","date":"2 May 2023","permalink":"/projects/gcovr/","section":"Projects","summary":"404: Not Found","title":"gcovr"},{"content":" Computer Systems Oribtal # Welcome to the GitHub repository for the Space And Systems Satellite Club\u0026rsquo;s Computer Systems Team! Our goal is to design and build a CubeSat that will be launched into space. This repository contains the code for our team\u0026rsquo;s computer systems, which will be responsible for controlling and monitoring the CubeSat\u0026rsquo;s various subsystems.\nGetting Started # To get started with our code, you\u0026rsquo;ll need to have a basic understanding of programming and the tools we\u0026rsquo;re using. We\u0026rsquo;re primarily using C++ for our code. You\u0026rsquo;ll also need to follow the instructions listed in the following sections:\nPrerequisites Running Locally Developing on the board Prerequisites # A list of software that must be installed before hand:\nSTM32CubeIDE:\nhttps://www.st.com/en/development-tools/stm32cubeide.html#get-software STM32 ST-LINK Utility:\nhttps://www.st.com/en/development-tools/stsw-link004.html Contributing # Contributions are always welcome!\nSee contributing.md for ways to get started.\nPlease adhere to this project\u0026rsquo;s code of conduct.\nRunning Locally # Create a personal access token # Go to\nhttps://github.com/settings/tokens Select\nGenerate new token-\u0026gt; Generate new token(classic) Under the \u0026ldquo;Select Scopes\u0026rdquo; check the boxes\nrepo gist After you will be given a randomly generated token MAKE SURE TO SAVE THE KEY.\nInstall EGit # Setup EGit in STM32CubeIDE\nNavigate to\nHelp-\u0026gt;Eclipse Marketplace In the \u0026ldquo;find:\u0026rdquo; search bar type\nEGit - Git Integration for Eclipse Hit install and restart STM32CubeIDE when complete.\nCloning the repository # Right click on \u0026ldquo;Project Explorer\u0026rdquo; and navigate to Import-\u0026gt;Git-\u0026gt;Projects from Git-\u0026gt;Clone URL. Enter in the \u0026ldquo;URL:\u0026rdquo; field\nhttps://github.com/rbretmounet/CS-Ortibal.git Enter in your github username under user and your github token under password. Click next until the project is imported.\nSetting up the project properties # The project properties must be configured so a .bin file is generated when the project is built. In order to so you must right click orbital_r1 and select properties at the bottom of the list. A new window should open and you must navigate to C/C++ Builds-\u0026gt;Settings-\u0026gt;MCU Post build outputs. Tick the boxe that says `Convert to binary file (-O binary).\nDeveloping on the board # Connecting the board # Connect the board to your computer\u0026rsquo;s USB port and open up STM32 ST-LINK Utility.exe. Navigate to Target -\u0026gt; Connect the device memory should popluate with addresses if successful.\nFlashing the board # Navigate to Target-\u0026gt;Program. The application with ask your for a file that ends in .bin,.srec, or .hex. If STM32CubeIDE is correctly configured See how to configure. These files are generated in the projects Debug folder. Navigate to the project directory ~/orbital_r1/Debug/ and select the file that ends in the correct file format.\n","date":"1 May 2023","permalink":"/projects/cs-ortibal/","section":"Projects","summary":"Computer Systems Oribtal # Welcome to the GitHub repository for the Space And Systems Satellite Club\u0026rsquo;s Computer Systems Team!","title":"CS-Ortibal"},{"content":" Classes # Classes have been very great. I am currently taking an Assembly class, Discrete math, Calculus, and another student lead CS course about linux. These classes have been super helpful and fun and I\u0026rsquo;m learning a lot!\nMy favorite topic in my Assembly class is implementing algorithms with bitwise operations.\nMy favorite part of Discrete math is set theory and number theory. I also really like proofs. I foresee proof writing being really helpful for math research in the future.\nI really like Calculus. I am currently taking integral calculus and it\u0026rsquo;s been wonderful. My favorite topic is using integrals to calculate areas of shapes.\nMy student lead CS course has been great as well. I have been getting to use regex and unix tools more and that\u0026rsquo;s been helpful. I also learn about CUnit and it\u0026rsquo;s super great!\nClubly - AggieWorks # For the past few months, I have been leading a team of 10 software engineers as TPM to build a club discovery app called Clubly. Our code is currently closed source and will likely remain that way, but I can share the stack we are using.\nThis experience has been so amazing and I cannot say enough good things about AggieWorks as a whole. I have learned so much from my peers, they have been absolutely amazing.\nFor the backend, we are using Go and a framework called Fiber. For the frontend, we are using Svelte and Typescript. For the database, we are using postgresql.\nThese are all design decisions I am very happy about.\nI have used a Go + Svelte stack twice before and it\u0026rsquo;s really nice. The first time was a trial run to test the stack. This project is called Best Next Step and can be found at BestNextStep.org. The second was a project I made for the company I worked at over the summer of 2022 but I cannot share more detail than that.\nThere are a few learning takeaways from the early development of this project I would like to reflect on.\nFirstly, planning out your architecture and design in the start saves massive amounts of time in the mid/end development stages. We started with our database design first. Answering the question of \u0026ldquo;What data do we have\u0026rdquo; and \u0026ldquo;How do we store it\u0026rdquo;. This lead us to designing the backend routes. This was the first month of development.\nAfter this, we were able to move at light speed because we had confidence in our design and architecture. We have had no breaking changes in our design since, only small addendums. However, we remain able to change our data if it\u0026rsquo;s requested by shareholders.\nIn the last two months especially, we have been moving really quickly, getting tons of work done and I am so proud of the whole team. We have been deploying and testing since we started but our first publicly facing deployment will be on Clubly.dev and Clubly.org in the next few weeks. (Now released!)\nUsing Git Post # I wrote a post about Git for my AggieWorks/Clubly team to reference. I may turn some of these ideas into a full post.\nFuture projects # I got the domain afternow.dev for a project I have been thinking about for a few weeks. So far, it\u0026rsquo;s a landing page with a simple design I made, but expect some updates sometime in the next few months. I think this will be a tool I use all of the time. I will start to pretotype this to see if it\u0026rsquo;s something I will use and if it\u0026rsquo;s something other people will use. I don\u0026rsquo;t foresee this being super popular with the general public, but it may be a tool for the people who use tools like notion to keep themselves organized.\n","date":"30 April 2023","permalink":"/devlogs/april-2023/","section":"Dev Logs","summary":"Classes # Classes have been very great.","title":"April 2023"},{"content":" What your setup should look like (git remote -v) # origin\tgit@github.com:JakeRoggenbuck/ClubApp.git (fetch) origin\tgit@github.com:JakeRoggenbuck/ClubApp.git (push) upstream\tgit@github.com:aggieworks/ClubApp.git (fetch) upstream\tgit@github.com:aggieworks/ClubApp.git (push) Getting information # # list the remotes git remote -v # check your staging area git status # check commits git log Update from latest # # make sure you are on your own main and don\u0026#39;t have any current changes git switch main git pull upstream main Before a feature/assignment/change # This should be done before the start of any distinct feature or large change.\ngit switch main git pull upstream main git switch -c feature-name Submitting a feature/assignment/change # # add all changed but prevously staged files git add -u git commit git push origin feature-name ","date":"27 April 2023","permalink":"/posts/helpful-git-commands/","section":"Posts","summary":"What your setup should look like (git remote -v) # origin\tgit@github.","title":"Helpful Git Commands"},{"content":"404: Not Found\n","date":"25 April 2023","permalink":"/projects/random-idea-spinner/","section":"Projects","summary":"404: Not Found","title":"random-idea-spinner"},{"content":" example-go-project-waves # Calculate wavelength from frequency or frequency from wavelength. This is an example for project for go learning after the hello world project.\nImage # This image actually shows that a frequency of 140Mhz is on the 2 Meter radio band.\n","date":"18 April 2023","permalink":"/projects/example-go-project-waves/","section":"Projects","summary":"example-go-project-waves # Calculate wavelength from frequency or frequency from wavelength.","title":"example-go-project-waves"},{"content":" Coming Soon - Devious Code Contest # I started the website for the soon to come Devious Code Contest found at deviouscc.org. This contest will happen sometime in the next year. The problems have been chosen and written but other details are still being decided.\nsatellite-c - Practice for writing satellite subsystems in C [link] # This was a super interesting project that is still in development. I join a club called Space and Satellite Systems and we will be writing code for a satellite in C.\n/* Build the task list and alloc the first chunk for the internal array */ struct TaskList *build_tasklist() { struct TaskList *l = malloc(sizeof(struct TaskList)); l-\u0026gt;size = 8; alloc_list(l); l-\u0026gt;index = 0; return l; } /* Allocate the internal array with the size */ void alloc_list(struct TaskList *l) { l-\u0026gt;task_list = malloc(l-\u0026gt;size * sizeof(struct Task)); } /* Add 8 to the size so that the alloc_list do so in chunks */ void add_list_chunk(struct TaskList *l) { l-\u0026gt;size += 8; } /* Add a task to the end of the internal array */ void add(struct TaskList *l, struct Task *t) { if (l-\u0026gt;index == l-\u0026gt;size) { add_list_chunk(l); alloc_list(l); } l-\u0026gt;task_list[l-\u0026gt;index] = t; l-\u0026gt;index++; } /* Get a task from the end of the internal array */ struct Task *pop(struct TaskList *l) { if (l-\u0026gt;index == 0) { return NULL; } l-\u0026gt;index--; return l-\u0026gt;task_list[l-\u0026gt;index]; } int empty(struct TaskList *l) { return l-\u0026gt;index == 0; } Updating the LocalList [link] # I added some small UI changes to give it a more modern look. New more centered logo and better borders around the elements and below the search bar.\nNew on left - Old on right\nreadme - Create readme files with one command [link] # void create_readme(char *fname) { char cwd[PATH_MAX]; if (getcwd(cwd, sizeof(cwd)) != NULL) { char *dirname = last_dir(cwd); FILE *f; f = fopen(\u0026#34;README.md\u0026#34;, \u0026#34;w\u0026#34;); if (f == NULL) { printf(\u0026#34;Unable to create file.\\n\u0026#34;); exit(1); } fputs(\u0026#34;# \u0026#34;, f); fputs(dirname, f); fputs(\u0026#34;\\n\u0026#34;, f); } else { perror(\u0026#34;getcwd() error.\\n\u0026#34;); } } # ","date":"31 March 2023","permalink":"/devlogs/march-2023/","section":"Dev Logs","summary":"Coming Soon - Devious Code Contest # I started the website for the soon to come Devious Code Contest found at deviouscc.","title":"March 2023"},{"content":" nobuild # Header only library for writing build recipes in C.\nMain idea # The idea is that you should not need anything but a C compiler to build a C project. No make, no cmake, no shell, no cmd, no PowerShell etc. Only C compiler. So with the C compiler you bootstrap your build system and then you use the build system to build everything else.\nTry it out right here:\n$ cc ./nobuild.c -o nobuild $ ./nobuild Explore nobuild.c file and the examples folder to learn more.\nThis is an Experimental Project # I\u0026rsquo;m not sure if this is even a good idea myself. This is why I\u0026rsquo;m implementing it. This is a research project. I\u0026rsquo;m not making any claims about suitability of this approach to any project.\nRight now I\u0026rsquo;m actively using nobuild only in bm. It works quite well for me there.\nIt\u0026rsquo;s likely Not Suitable for Your Project # If you are using cmake with tons of modules to manage and find tons of dependencies you probably don\u0026rsquo;t want to use this tool. nobuild is more like writting shell scripts but in C.\nAdvantages of nobuild # Extremely portable builds across variety of systems including (but not limited to) Linux, MacOS, Windows, FreeBSD, etc. This is achieved by reducing the amount of dependencies to just a C compiler, which exists pretty much for any platform these days. You end up using the same language for developing and building your project. Which may enable some interesting code reusage strategies. The build system can use the code of the project itself directly and the project can use the code of the build system also directly. You get to use C more. \u0026hellip; Disadvantages of nobuild # You need to be comfortable with C and implementing things yourself. As mentioned above this is like writing shell scripts but in C. It probably does not make any sense outside of C/C++ projects. You get to use C more. \u0026hellip; Why is it called \u0026ldquo;nobuild\u0026rdquo; when it\u0026rsquo;s clearly a build tool? # You know all these BS movements that supposedly remove the root cause of your problems? Things like NoSQL, No-code, Serverless, etc. This is the same logic. I had too many problems with the process of building C projects. So there is nobuild anymore.\nHow to use the library in your own project # Keep in mind that nobuild.h is an stb-style header-only library. That means that just including it does not include the implementations of the functions. You have to #define NOBUILD_IMPLEMENTATION before the include. See our nobuild.c for an example.\nCopy nobuild.h to your project Create nobuild.c in your project with the build recipe. See our nobuild.c for an example. Bootstrap the nobuild executable: $ cc nobuild.c -o nobuild on POSIX systems $ cl.exe nobuild.c on Windows with MSVC Run the build: $ ./nobuild If you enable the Go Rebuild Urself™ Technology the nobuild executable will try to rebootstrap itself every time you modify its source code.\n","date":"15 March 2023","permalink":"/projects/nobuild/","section":"Projects","summary":"nobuild # Header only library for writing build recipes in C.","title":"nobuild"},{"content":" readme # Create readme files with one command\nTODO # add cli flags for verbose, force write, should add a build badge Why? # I make a readme for something with a frequency withing [weekly,monthly] and it usually takes me between a half a minute to a minute when adding the build badge and other important stuff. I also just like writing C.\n","date":"12 March 2023","permalink":"/projects/readme/","section":"Projects","summary":"readme # Create readme files with one command","title":"readme"},{"content":" satellite-c # Practice for writing satellite subsystems in C\nImportant Things # Manage a task list TaskList that is a queue of Tasks TODO # Make struct Message ","date":"12 March 2023","permalink":"/projects/satellite-c/","section":"Projects","summary":"satellite-c # Practice for writing satellite subsystems in C","title":"satellite-c"},{"content":"This month I was working a lot on school and clubs I am associated with so I didn’t have a bunch of time to work on personal projects, but I do have some highlights to share.\npowerd [link] # power(d)aemon - check battery capacity efficiently using the inotify library written in C\nfd = inotify_init(); if (fd \u0026lt; 0) { print_and_exit(\u0026#34;Couldn\u0026#39;t initialize inotify.\\n\u0026#34;); } char filepath[200]; sprintf(filepath, \u0026#34;/sys/class/power_supply/%s\u0026#34;, filename); wd = inotify_add_watch(fd, filepath, IN_CREATE | IN_MODIFY | IN_DELETE); if (wd == -1) { printf(\u0026#34;Couldn\u0026#39;t add watch to \\\u0026#34;%s\\\u0026#34;\\n\u0026#34;, filepath); exit(1); } else { printf(\u0026#34;Watching \\\u0026#34;%s\\\u0026#34;\\n\u0026#34;, filepath); } manyrepr - view input in multiple formats at once [link] # I got to use std::map in C++ and other interesting concepts. This project is very early in development but I plan to make it useful. I often have base64 or binary that I need to turn into ascii or I have a text I need to urlencode. This tool doesn\u0026rsquo;t even ask what option you want, it just does it all and the use can copy what they want. The value of being able to have it convert input into \u0026ldquo;all of the above\u0026rdquo; make the use of this tool really fast,\nvoid get_opt(std::string intype, enum Options \u0026amp;opt) { std::map\u0026lt;std::string, enum Options\u0026gt; argmap; std::map\u0026lt;std::string, enum Options\u0026gt;::iterator it; argmap[\u0026#34;--text\u0026#34;] = TEXT; argmap[\u0026#34;--hex\u0026#34;] = HEX; argmap[\u0026#34;--bin\u0026#34;] = BIN; argmap[\u0026#34;--base64\u0026#34;] = BASE64; argmap[\u0026#34;--dec\u0026#34;] = DEC; argmap[\u0026#34;--rot13\u0026#34;] = ROT13; argmap[\u0026#34;--urlenc\u0026#34;] = URLENC; it = argmap.find(intype); if (it != argmap.end()) { opt = it-\u0026gt;second; } } New Draft.vim features [link] # I also added features to my project from 3 years ago draft.vim\nhttps://github.com/JakeRoggenbuck/draft.vim/releases/tag/0.6\n","date":"28 February 2023","permalink":"/devlogs/february-2023/","section":"Dev Logs","summary":"This month I was working a lot on school and clubs I am associated with so I didn’t have a bunch of time to work on personal projects, but I do have some highlights to share.","title":"Febuary 2023"},{"content":" calplusplus # The cal tool written in C++. Still a work in progress.\nFuture plans # /* void display_whole_month() { */ /* Month month = Month::now(); */ /* month += 1; */ /* std::cout \u0026lt;\u0026lt; month.full() \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; */ /* /1* Su Mo Tu We Th Fr Sa */ /* 1 2 3 4 */ /* 5 6 7 8 9 10 11 */ /* 12 13 14 15 16 17 18 */ /* 19 20 21 22 23 24 25 */ /* 26 27 28 29 30 31 */ /* *1/ */ /* } */ ","date":"7 February 2023","permalink":"/projects/calplusplus/","section":"Projects","summary":"calplusplus # The cal tool written in C++.","title":"calplusplus"},{"content":"I joined AggieWorks as a Technical Product Manager for a new project. I spent a lot of time working on AggieWorks and School. I didn’t do as much for personal projects but I still have some interesting things to share for this month.\nI started implementing different data structures in C. Then I started incorporating these into a handful of projects.\nths - local thesaurus in C [link] # Use of djb2 hash and a binary tree\nvoid insert(struct Node *base, struct Word *word) { if (word-\u0026gt;id \u0026lt; base-\u0026gt;id) { if (base-\u0026gt;left == NULL) { base-\u0026gt;left = build_node(word); } else { insert(base-\u0026gt;left, word); } } if (word-\u0026gt;id \u0026gt; base-\u0026gt;id) { if (base-\u0026gt;right == NULL) { base-\u0026gt;right = build_node(word); } else { insert(base-\u0026gt;right, word); } } } mountain - lightweight tool to auto mount drives with inotify [link] # This uses the sys/inotify library for linux to create a callback for when files are created on the system. It checks specifically for files in /dev with a certain name pattern.\nfor (int i = 1; i \u0026lt; argc; ++i) { if (argv[i][0] == \u0026#39;-\u0026#39;) { if (!has_option) { if (does_match(argv[i], \u0026#34;-v\u0026#34;, \u0026#34;--version\u0026#34;)) { args.option = VERSION; has_option = 1; } else if (does_match(argv[i], \u0026#34;-h\u0026#34;, \u0026#34;--help\u0026#34;)) { args.option = HELP; has_option = 1; } } if (does_match(argv[i], \u0026#34;-V\u0026#34;, \u0026#34;--verbose\u0026#34;)) { args.verbose = 1; } if (does_match(argv[i], \u0026#34;-m\u0026#34;, \u0026#34;--mount\u0026#34;)) { args.mount = 1; } if (does_match(argv[i], \u0026#34;-n\u0026#34;, \u0026#34;--notify\u0026#34;)) { args.notify = 1; } } else { // Only accept the first non-flag as a file if (!has_file) { size_t len = strlen(argv[i]); args.filename = malloc(len * sizeof(char)); strcpy(args.filename, argv[1]); has_file = 1; } } } one-time-pad-gen [link] # Generate one time pads for use in simple manual ciphers. This project was interesting because learning to generate a bunch of new random numbers is an interesting problem.\n$ otpg 10 L M K J S J Y O D C S E M V E T W Y W J B A Z D C C B W T W I E K U O D D O T G S L N H H T A F U Z O V B Q A D U E B N A K S N G G Q M W J S P W H W D B Y I V X Z S A R S D L Y F Y B R Q O Z Y E L V P E K L N I R O G Z L G A D I R Y O C W V B X M T N L S R X N I D Z U S J L H R M S Z P Y I G W W J U R M U F F H S X B R M J W N F P Y S Y Q H S R W S Z C O X N L Q Z H V F O Q E R J T B G I I X H B V X I P O E I Q I Y P W J G X Q D E H T L Y D E B L M K I T N F S V U J B C B L B Q J K W H D C N M void generate(int rows) { for (int i = 0; i \u0026lt; rows; ++i) { for (int j = 0; j \u0026lt; COLS; ++j) { for (int k = 0; k \u0026lt; WORDS; ++k) { int v = rand() % 26; printf(\u0026#34;%c \u0026#34;, tochar(v)); } printf(\u0026#34; \u0026#34;); } printf(\u0026#34;\\n\u0026#34;); } } void ref_pad() { for (int i = 0; i \u0026lt; 26; ++i) { printf(\u0026#34;%c: \u0026#34;, tochar(i)); for (int j = 0; j \u0026lt; 26; ++j) { printf(\u0026#34;%c \u0026#34;, tochar(j)); } printf(\u0026#34;\\n\u0026#34;); printf(\u0026#34; \u0026#34;); for (int j = 0; j \u0026lt; 26; ++j) { printf(\u0026#34;%c \u0026#34;, tochar((j + i) % 26)); } printf(\u0026#34;\\n\u0026#34;); printf(\u0026#34;\\n\u0026#34;); } } int get_seed() { int val; FILE *fp; fp = fopen(\u0026#34;/dev/urandom\u0026#34;, \u0026#34;rb\u0026#34;); fread(\u0026amp;val, sizeof(int), 1, fp); fclose(fp); return val; } ","date":"31 January 2023","permalink":"/devlogs/january-2023/","section":"Dev Logs","summary":"I joined AggieWorks as a Technical Product Manager for a new project.","title":"January 2023"},{"content":" darktable # darktable is an open source photography workflow application and non-destructive raw developer - a virtual lighttable and darkroom for photographers. It manages your digital negatives in a database, lets you view them through a zoomable lighttable and enables you to develop raw images, enhance them and export them to local or remote storage.\ndarktable is not a free Adobe® Lightroom® replacement.\nhttps://www.darktable.org/\nTable of Contents # Documentation Website Requirements Supported platforms Hardware Installing Latest release Development snapshot Updating from older versions Obtaining extensions Building Dependencies Get the source Get submodules Compile Further reading Using Test/unstable version Regular/stable version Contributing FAQ Why is my camera not detected when plugged-in ? Why is my lens not detected/corrected in darkroom ? Why do the thumbnails in the lighttable view look different to the preview in the darkroom view ? Wiki Mailing lists Documentation # The darktable user manual is maintained in the dtdocs repository.\nLua API documentation is maintained in the luadocs repository.\nWebsite # The website ( https://www.darktable.org/) is maintained in the dtorg repository.\nRequirements # Supported platforms # Linux (64-bit) FreeBSD (64-bit) Windows (64-bit), 8.1 w/ UCRT and later macOS Big-endian platforms are not supported.\n32-bit platforms are not officially supported - they might or might not work.\nWindows support is still young and suffers from bugs that do not affect Linux. If possible, prefer using darktable on Linux.\nHardware # (workable minimum / recommended minimum):\nRAM: 4 GB / 8 GB CPU: Intel Pentium 4 (Core 2 for Windows) / Intel Core i5 4×2.4 GHz GPU: none / Nvidia with 1024 CUDA cores, 4 GB, OpenCL 1.2 compatible free disk space: 250 MB / 1 GB darktable can run on lightweight configurations (even on a Raspberry Pi), but expect modules like denoise, local contrast, contrast equalizer, retouch or liquify to be slow beyond usable.\nA GPU is not mandatory but is strongly recommended for a smoother experience. Nvidia GPUs are recommended for safety because some AMD drivers behave unreliably with some modules (e.g. local contrast).\nInstalling # If the latest release is still not available as a pre-built package for your distribution, you can build the software yourself following the instructions below.\nLatest release # 4.2.0 (stable)\nDownload executable for Windows Download executable for macOS on Intel Download executable for macOS on Apple Silicon Install native packages and repositories for Linux Install Flatpak package for Linux More information about installing darktable on any system When using a pre-built package, ensure that it has been built with Lua, OpenCL, OpenMP and Colord support. These are optional and will not prevent darktable from running if missing, but their absence will degrade the user experience. Notably, some Flatpak, Snap and Appimage packages lack OpenCL and Lua support.\nDevelopment snapshot # The development snapshot reflects the current state of the master branch. It is intended for testing and is generally not safe. See the notes below for warnings and precautions about using the master branch.\nInstall native packages and repositories for Linux (one snapshot per day). No pre-compiled packages are provided for the master branch on macOS and Windows. See how to build it manually below. Updating from older versions # When updating darktable from an older release, you only need install the newest version. Existing files will be preserved.\nHowever, newer releases occasionally need to change the structure of the library database (containing the whole list of images known to darktable, with their editing history). If this happens you will be prompted with a request to either upgrade the database or close the software.\nMigration to a newer database structure/newer release means that your edits (both new and old) will no longer be compatible with older versions of darktable. Upgrades are definitive. Newer versions are always compatible with older edits, but newer edits are generally not compatible with older versions.\ndarktable automatically backs up the library database when a new version causes it to be upgraded (in ~/.config/darktable/library.db-pre-3.0.0 for example), so you can revert to the previous release by restoring this backup if needed (simply rename it to library.db).\nIf you try to open a newer database with an older version of the software, any portions of your edits that were undertaken with new features will be discarded and you will lose them. This also applies to the sidecar XMP files.\nIf you plan to move regularly between two versions (new/unstable and old/stable) see below for details of how to do it safely.\nObtaining extensions # Extensions and plugins use the Lua scripting language and can be downloaded here. Lua support is optional in darktable, so make sure you have the lua interpreter and its development files (package lua-dev or lua-devel, depending on distributions) installed on your system while building or ensure the package you are using has been built with this library.\nExtensions allow exporting for various media and websites, merge/stack/blend HDR, panoramas or focus bracketing, apply AI-based facial recognition, manage tags and GPS data, etc.\nBuilding # Dependencies # Compatible compilers:\nClang: 8 and later GCC: 8 and later MinGW-w64: 6 and later Required dependencies (minimum version):\nCMake 3.18 GTK 3.24.15 GLib 2.40 SQLite 3.15 (but 3.24 or newer strongly recommended) Exiv2 0.24 (but at least 0.27.4 built with ISO BMFF support needed for Canon CR3 raw import) Required dependencies (no version requirement):\nLensfun (for automatic lens correction) (Note: alpha 0.3.95 and git master branch are not supported) Little CMS 2 Optional dependencies (minimum version):\nOpenMP 4.5 (for CPU multi-threading and SIMD vectorization) LLVM 3.9 (for OpenCL checks at compilation time) OpenCL 1.2 (for GPU-accelerated computing) Lua 5.4 (for plugins and extension scripting) libgphoto2 2.5 (for camera tethering) Imath 3.1.0 (for 16-bit \u0026ldquo;half\u0026rdquo; float TIFF export and faster import) libavif 0.8.2 (for AVIF import \u0026amp; export) libheif 1.13.0 (for HEIF/HEIC/HIF import; also for AVIF import if no libavif) libjxl 0.7.0 (for JPEG XL import \u0026amp; export) WebP 0.3.0 (for WebP import \u0026amp; export) Optional dependencies (no version requirement):\ncolord, Xatom (for fetching the system display color profile) G\u0026rsquo;MIC (for .gmz compressed LUT support) PortMidi (for MIDI input support) SDL2 (for gamepad input support) CUPS (for print mode support) OpenEXR (for EXR import \u0026amp; export) OpenJPEG (for JPEG 2000 import \u0026amp; export) GraphicsMagick or ImageMagick (for misc image format import) To install all the dependencies on Linux systems, you may use the source repositories of your distribution (provided they are up-to-date):\nFedora and RHEL # sudo dnf builddep darktable OpenSuse # sudo zypper si -d darktable Ubuntu # sed -e \u0026#39;/^#\\sdeb-src /s/^# *//;t;d\u0026#39; \u0026#34;/etc/apt/sources.list\u0026#34; \\ | sudo tee /etc/apt/sources.list.d/darktable-sources-tmp.list \u0026gt; /dev/null \\ \u0026amp;\u0026amp; ( sudo apt-get update sudo apt-get build-dep darktable ) sudo rm /etc/apt/sources.list.d/darktable-sources-tmp.list Debian # sudo apt-get build-dep darktable Install missing dependencies # If mandatory dependencies are missing on your system, the software build will fail with errors like Package XXX has not been found or Command YYY has no provider on this system. If you see one of these errors you should find out which package provides the missing package/command in your distribution, then install it. This can usually be done in your package manager (not the application manager customarily provided by default in your distribution) or from the internet with a search engine. You may need to install a package manager first (like APT on Debian/Ubuntu, or DNF on Fedora/RHEL).\nThis process might be tedious but you only need to do it once. See this page on building darktable for one-line commands that will install most dependencies on the most common Linux distributions.\nGet the source # Master branch (unstable) # The master branch contains the latest version of the source code and is intended:\nas a working base for developers, for beta-testers to chase bugs, for users willing to sacrifice stability for new features without waiting for the next release. The master branch comes with no guarantee of stability and might corrupt your database and XMP files, result in loss of data and edit history or temporarily break compatibility with previous versions and commits.\nHow dangerous is it? Most of the time, it is fairly stable. As with any rolling-release kind of deployment, bugs appear more often but are fixed faster too. Sometimes, though, these bugs can result in losses or inconsistencies in the editing history of your pictures. This is fine if you don\u0026rsquo;t need to open your edits again in the future, but maybe not if you manage an estate.\nAfter backing up your ~/.config/darktable directory and the sidecar .XMP files of any pictures you intend to open with the master branch, you may obtain the source as follows:\ngit clone --recurse-submodules --depth 1 https://github.com/darktable-org/darktable.git cd darktable See below (in \u0026ldquo;Using\u0026rdquo;) how to start a test install of the unstable version without damaging your regular stable install and files.\nLatest stable release # 4.2.0\nThe darktable project releases two major versions every year, on Summer and Winter Solstices, tagged with even numbers (e.g. 4.0, 4.2, 4.4, 4.6). Minor revisions are tagged with a third digit (e.g. 4.0.1, 4.0.2) and mostly provide bug fixes and camera support. You may want to compile these stable releases yourself to get better performance for your particular computer:\ngit clone --recurse-submodules --depth 1 https://github.com/darktable-org/darktable.git cd darktable git fetch --tags git checkout tags/release-4.2.0 Get submodules # Note that libxcf, OpenCL, RawSpeed, whereami and LibRaw are tracked via git submodules, so after checking-out darktable, you need to update/checkout the submodules too:\ngit submodule update --init Compile # Easy way # WARNING: If you have previously built darktable, don\u0026rsquo;t forget to first completely remove (rm -R) the build and /opt/darktable directories to avoid conflicting files from different versions. Many weird behaviours and transient bugs have been reported that can be traced to the build cache not properly invalidating the changed dependencies, so the safest way is to completely remove previously built binaries and start again from scratch.\ndarktable provides a shell script that automatically takes care of building on Linux and macOS for classic cases in a single command.\n./build.sh --prefix /opt/darktable --build-type Release --install --sudo If you want to install a test version alongside your regular/stable version, change the install prefix:\n./build.sh --prefix /opt/darktable-test --build-type Release --install --sudo This builds the software for your architecture only, with:\n-O3 optimization level, SSE/AVX support if detected, OpenMP support (multi-threading and vectorization) if detected, OpenCL support (GPU offloading) if detected, Lua scripting support if detected. If you want to have dartkable displayed along your other applications, you only need to add a symbolic link:\nln -s /opt/darktable/share/applications/org.darktable.darktable.desktop /usr/share/applications/org.darktable.darktable.desktop Now, your custom-built darktable is ready to be used just like any pre-packaged software.\nManual way # Alternatively, you can use a manual build to pass custom arguments.\nLinux/macOS # mkdir build/ cd build/ cmake -DCMAKE_INSTALL_PREFIX=/opt/darktable/ .. make sudo make install Windows # See https://github.com/darktable-org/darktable/tree/master/packaging/windows\nUsing # Test/unstable version # To use a test version of darktable without damaging your regular/stable version\u0026rsquo;s files and database, start darktable in a terminal with:\n/opt/darktable-test/bin/darktable --configdir \u0026#34;~/.config/darktable-test\u0026#34; and ensure that you set the option \u0026ldquo;write sidecar file for each image\u0026rdquo; to \u0026ldquo;never\u0026rdquo; in preferences -\u0026gt; storage -\u0026gt; XMP. This way, your regular/stable version will save its configuration files in ~/.config/darktable, as usual, the test/unstable one will save in ~/.config/darktable-test, and the two versions will not produce database conflicts.\nRegular/stable version # Simply launch it from your desktop application menu or, from a terminal, run darktable or /opt/darktable/bin/darktable. If the installation did not create a launcher in your applications menu, run:\nsudo ln -s /opt/darktable/share/applications/org.darktable.darktable.desktop /usr/share/applications/org.darktable.darktable.desktop You may find darktable configuration files in ~/.config/darktable. If you experience crashes at startup, try launching darktable from a terminal with OpenCL disabled using darktable --disable-opencl.\nFurther reading # There is a comprehensive list of build instructions for Ubuntu/Debian related distributions or for Fedora and related distributions. These build instructions can be easily adapted to many other Linux distributions.\nContributing # There are many ways you can contribute to the darktable project:\nWrite a blog about darktable Create a tutorial for darktable Help expand the user wiki or user manual Answer questions on the user mailing list or the pixls.us forums Share your ideas on the developer mailing list Test releases Review pull requests Start hacking on darktable and see developer\u0026rsquo;s guide FAQ # Why is my camera not detected when plugged-in ? # Check that you have the latest gphoto2 library installed in order to support the newest cameras.\nWhy is my lens not detected/corrected in darkroom ? # Lens correction profiles are provided by Lensfun, which has 2 parts: a program and a database. Most Linux distributions provide a recent enough version of the program, but provide an outdated version of the database. If Lensfun is correctly installed, then update its database in a terminal by running:\nlensfun-update-data or alternatively\n/usr/bin/g-lensfun-update-data Why do the thumbnails in the lighttable view look different to the preview in the darkroom view ? # For RAW files that have never been edited in darktable (when you have just imported them), the lighttable view, by default, shows the JPEG preview placed into the RAW file by your camera. Loading this JPEG file is faster and makes the lighttable view more responsive when importing large collections of images.\nHowever, this JPEG thumbnail is processed by the firmware of the camera, with proprietary algorithms, and colors, sharpness and contrast that might not look the same as darktable processing (which is what you see when opening the image in the darkroom view). Camera manufacturers don\u0026rsquo;t publish details of the pixel processing they perform in their firmware so their look is not exactly or easily reproducible by other software.\nHowever, once RAW images have been edited in darktable, the lighttable thumbnail should exactly match the darkroom preview, as they are processed in the same way.\nIf you never want to see the embedded JPEG thumbnail in the lighttable view, for RAW files, you should set the option \u0026ldquo;use raw file instead of embedded JPEG from size\u0026rdquo; to \u0026ldquo;never\u0026rdquo; in preferences -\u0026gt; lighttable.\nWiki # GitHub wiki Developer wiki Mailing lists # User\u0026rsquo;s [ subscribe | archive] Developer\u0026rsquo;s [ subscribe | archive] ","date":"30 January 2023","permalink":"/projects/darktable/","section":"Projects","summary":"darktable # darktable is an open source photography workflow application and non-destructive raw developer - a virtual lighttable and darkroom for photographers.","title":"darktable"},{"content":" mountain - lightweight tool to auto mount drives with inotify # Use sys/inotify to listen for new files in /dev/ Notify the user that such file has been created Mount the drive listed ","date":"24 January 2023","permalink":"/projects/mountain/","section":"Projects","summary":" mountain - lightweight tool to auto mount drives with inotify # Use sys/inotify to listen for new files in /dev/ Notify the user that such file has been created Mount the drive listed ","title":"mountain"},{"content":" timer-cli # ⏲️ Really basic timer cli built in C\n","date":"24 January 2023","permalink":"/projects/timer-cli/","section":"Projects","summary":"timer-cli # ⏲️ Really basic timer cli built in C","title":"timer-cli"},{"content":" one-time-pad-gen # Generate one time pads for use in simple manual ciphers written in C.\n$ otpg 10 L M K J S J Y O D C S E M V E T W Y W J B A Z D C C B W T W I E K U O D D O T G S L N H H T A F U Z O V B Q A D U E B N A K S N G G Q M W J S P W H W D B Y I V X Z S A R S D L Y F Y B R Q O Z Y E L V P E K L N I R O G Z L G A D I R Y O C W V B X M T N L S R X N I D Z U S J L H R M S Z P Y I G W W J U R M U F F H S X B R M J W N F P Y S Y Q H S R W S Z C O X N L Q Z H V F O Q E R J T B G I I X H B V X I P O E I Q I Y P W J G X Q D E H T L Y D E B L M K I T N F S V U J B C B L B Q J K W H D C N M ","date":"2 January 2023","permalink":"/projects/one-time-pad-gen/","section":"Projects","summary":"one-time-pad-gen # Generate one time pads for use in simple manual ciphers written in C.","title":"one-time-pad-gen"},{"content":" loader-test # Testing the hyper-minimal-loaders project.\nWhat is hyper-minimal-loaders? # It\u0026rsquo;s just a simple percentage loader\n","date":"1 January 2023","permalink":"/projects/loader-test/","section":"Projects","summary":"loader-test # Testing the hyper-minimal-loaders project.","title":"loader-test"},{"content":" New projects # I created a program to learn linearization.\nSalt is another project (Symbol And Lexer Toolkit). Salt aims to lex and tokenize c source code for three reasons.\nMake auto include suggestions for file and libraries Check where every imported symbol is from and warn if imports aren\u0026rsquo;t needed Generate documentation of a library in a c file School # I spent a lot of time this month working on school projects and preparing for finals.\nI try to overlap coding and my school as much as possible, so before finals I spent a few hours working on projects that reinforce my understanding in Math.\nI made this diagram for find-slope-with-secant recently.\nI also made this diagram for newtons-method.\n","date":"31 December 2022","permalink":"/devlogs/december-2022/","section":"Dev Logs","summary":"New projects # I created a program to learn linearization.","title":"December 2022"},{"content":" Sniper # Overview # Sniper is a very simple game where you run from an adversary which tries to shoot you, but you can collect the bullets and fire them back to gain score.\nBuild # Dependencies # Make sure you have GLFW installed:\nGLFW Install Instructions Arch Linux # sudo pacman -S glfw-x11 Debian/Ubuntu # sudo apt install libglfw3 libglfw3-dev Windows # Download a pre-compiled binary from the GLFW website.\nmacOS # Install from Homebrew:\nbrew install glfw Or download a pre-compiled binary from the GLFW website.\nFrom source # See the instructions on the GLFW website.\nThen check that OpenGL 4.1 or above is installed. You can download OpenGL from here.\nRun build script # To build, run ./scripts/build.sh (or .\\scripts\\build.bat on Windows).\n","date":"31 December 2022","permalink":"/projects/sniper/","section":"Projects","summary":"Sniper # Overview # Sniper is a very simple game where you run from an adversary which tries to shoot you, but you can collect the bullets and fire them back to gain score.","title":"sniper"},{"content":" Cryptography? # The methodology of concealing the content of messages. Originates from the Greek root word kryptos, which means hidden. The modern scientific study of crytography is sometimes referred to as cryptology1.\nCiphers? # A function that encrypts some plaintext into an unreadable ciphertext.\ndef cipher(plaintext) -\u0026gt; str: ... return ciphertext Codebreaking? # The analysis and attack of classical cryptosystems. In the context of modern cryptography, codebreaking refers to the exploitation of modern encryption systems.\nProject progress # (python only + no codebreaker required yet)\nTable substitution cipher Digraph substitution cipher Playfair cipher Shift cipher Vigenere cipher Affine cipher Steganography Xor One-time pad + proof of perfect secrecy Information theory: entropy analysis Information theory: wordle solver Block cipher Stream cipher Data encryption standard Padding oracle attack Hashing MACs Rainbow table attack RSA Diffie-Hellman key exchange ElGamal Complexity theory Abstract algebra review Factorization Discrete log Decisional Diffie-Hellman assumption Primality testing Elliptic-curve cryptography Lenstra\u0026rsquo;s elliptic-curve factorization Elliptic-curve digital signature algorithm RSA signature ElGamal signature Existential forgery Shortest Vector Problem (bounds) Fundamental domain of lattices Reduction of Closest Vector Problem to SVP Subset sum of cryptosystem NTRU encryption algorithm Ring Learning With Errors The first third is mainly symmetric cryptography. The rest is assymetric cryptography.\nTable substitution cipher # This substitution cipher looks up each plaintext letter in an encryption table and writes the corresponding ciphertext letter in its place. Evidently, the decryption table is the inverse of the encryption table\ndecryption_table = {v: k for k, v in encryption_table.items()} In a table substitution cipher, the ciphertext alphabet is a randomly chosen permutation of the 26 alphabet letters. The random permutation for this cipher is both lowercase and capital alphabet letters\nRandom permutation example: OaTyqwGerPSApdfghjXUIlzxcZLMVWKuZvbCRnmYNoQBkisFDtJH\nplaintext ciphertext a O b a c T plaintext to ciphertext values like O, a, and T are chosen randomly (or by using a generator).\nDigraph substitution cipher # Similar to the monoalphabetic table substitution cipher, this is another substitution cipher. Unlike the monoalphabetic table cipher, this cipher replaces every plaintext digraph with its corresponding ciphertext digraph instead of replacing every plaintext letter with its corresponding ciphertext letter. Each ciphertext digraph is located by position given a row index and column index determined by the plaintext digraph\nshift_row = 4 shift_column = 17 alpha = \u0026#34;abcdefghijklmnopqrstuvwxyz\u0026#34; # Shifts the alphabet over by `shift` amount. Loops overflow values to start. def shift_alpha(alpha, shift) -\u0026gt; str: return alpha[shift:len(alpha)] + alpha[:shift] sbox = [[shift_alpha(alpha, shift_column)[i] + (shift_alpha(alpha, shift_row)[j]) for j in range(len(alpha))] for i in range(len(alpha))] plaintext ciphertext aa re ab se he vl For an explanation of how plaintext turns into ciphertext, visit the title link.\nPlayfair cipher # The playfair cipher! This one is sort of like the digraph substitution cipher in utilizing a table (this one\u0026rsquo;s is 5x5) and multiple digraph substitutions, but this cipher has a few more rules. Unlike the digraph substitution cipher mentioned above, this one does not have a ciphertext digraph intersection value based on two inputted label points from a plaintext digraph. The playfair cipher instead shifts isolated digraph characters up, down, or diagonally in the matrix according to the digraph classification.\nAdditionally, this cipher has a key! See more about keys and about the playfair encryption method at the title link.\nkey = \u0026#34;shadow\u0026#34; 625 s h a d o w ⠀ ⠀ ⠀ s h a d o w b c e f g i j k l m n p q r t u v y z Shift Cipher # Otherwise known as a caesar cipher, the shift cipher takes each letter in a plaintext message and shifts it by n indexes in the looping alphabet.\nHOFFSTEIN, JEFFREY. Mathematical Cryptography. SPRINGER-VERLAG NEW YORK, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"24 December 2022","permalink":"/projects/cryptography/","section":"Projects","summary":"Cryptography?","title":"cryptography"},{"content":" autoignore # Overview # autoignore automatically adds all untracked files to your .gitignore file so you won\u0026rsquo;t ever have to worry about them again.\nInstall # If path/to/autoignore/ is the directory containing this file, add the following line:\nalias autoignore=\u0026#39;python3 path/to/autoignore/src/main.py\u0026#39; to your bash profile (which is located at ~/.bash_profiles on macOS and at ~/.bash_profile on Linux), and then restart your terminal.\nUninstall # Remove the line you added during the install step.\nUsage # Navigate to the directory with the git repository in question and run the command autoignore.\n","date":"14 December 2022","permalink":"/projects/autoignore/","section":"Projects","summary":"autoignore # Overview # autoignore automatically adds all untracked files to your .","title":"autoignore"},{"content":" Salt (Symbol And Lexer Toolkit) # ","date":"12 December 2022","permalink":"/projects/salt/","section":"Projects","summary":" Salt (Symbol And Lexer Toolkit) # ","title":"salt"},{"content":" linearization # double lin(double (*func)(double), double a) { int x = round(a); return func(a) + derivative(func, a) * (x - a); } l(x) = f(a) + f\u0026#39;(a) * (x - a) for a value of a near x lin(16.10): 4.003599 f(16.10): 4.012481 |Δ|: 0.008882 ","date":"8 December 2022","permalink":"/projects/linearization/","section":"Projects","summary":"linearization # double lin(double (*func)(double), double a) { int x = round(a); return func(a) + derivative(func, a) * (x - a); } l(x) = f(a) + f\u0026#39;(a) * (x - a) for a value of a near x lin(16.","title":"linearization"},{"content":" Description # bspwm is a tiling window manager that represents windows as the leaves of a full binary tree.\nIt only responds to X events, and the messages it receives on a dedicated socket.\nbspc is a program that writes messages on bspwm\u0026rsquo;s socket.\nbspwm doesn\u0026rsquo;t handle any keyboard or pointer inputs: a third party program (e.g. sxhkd) is needed in order to translate keyboard and pointer events to bspc invocations.\nThe outlined architecture is the following:\nPROCESS SOCKET sxhkd --------\u0026gt; bspc \u0026lt;------\u0026gt; bspwm Configuration # The default configuration file is $XDG_CONFIG_HOME/bspwm/bspwmrc: this is simply a shell script that calls bspc.\nAn argument is passed to that script to indicate whether is was executed after a restart ($1 -gt 0) or not ($1 -eq 0).\nKeyboard and pointer bindings are defined with sxhkd.\nExample configuration files can be found in the examples directory.\nMonitors, desktops and windows # bspwm holds a list of monitors.\nA monitor is just a rectangle that contains desktops.\nA desktop is just a pointer to a tree.\nMonitors only show the tree of one desktop at a time (their focused desktop).\nThe tree is a partition of a monitor\u0026rsquo;s rectangle into smaller rectangular regions.\nEach node in a tree either has zero or two children.\nEach internal node is responsible for splitting a rectangle in half.\nA split is defined by two parameters: the type (horizontal or vertical) and the ratio (a real number r such that 0 \u0026lt; r \u0026lt; 1).\nEach leaf node holds exactly one window.\nInsertion modes # When bspwm receives a new window, it inserts it into a window tree at the specified insertion point (a leaf) using the insertion mode specified for that insertion point.\nThe insertion mode tells bspwm how it should alter the tree in order to insert new windows on a given insertion point.\nBy default the insertion point is the focused window and its insertion mode is automatic.\nManual mode # The user can specify a region in the insertion point where the next new window should appear by sending a node -p|\u0026ndash;presel-dir DIR message to bspwm.\nThe DIR argument allows to specify how the insertion point should be split (horizontally or vertically) and if the new window should be the first or the second child of the new internal node (the insertion point will become its brother).\nAfter doing so the insertion point goes into manual mode.\nLet\u0026rsquo;s consider the following scenario:\na a a / \\ / \\ / \\ 1 b ---\u0026gt; c b ---\u0026gt; c b ^ / \\ / \\ / \\ / \\ / \\ 2 3 4 1 2 3 d 1 2 3 ^ / \\ 5 4 ^ +-----------------------+ +-----------------------+ +-----------------------+ | | | | | | | | | | | | 2 | | 4 | 2 | | 5 | 4 | 2 | | | | | ^ | | | ^ | | | | 1 |-----------| |-----------|-----------| |-----------|-----------| | ^ | | | | | | | | | | 3 | | 1 | 3 | | 1 | 3 | | | | | | | | | | +-----------------------+ +-----------------------+ +-----------------------+ X Y Z In state X, the insertion point is 1.\nWe send the following message to bspwm: node -p north.\nThen add a new window: 4, this leads to state Y: the new internal node, c becomes a\u0026rsquo;s first child.\nFinally we send another message: node -p west and add window 5.\nThe ratio of the preselection (that ends up being the ratio of the split of the new internal node) can be changed with the node -o|\u0026ndash;presel-ratio message.\nAutomatic mode # The automatic mode, as opposed to the manual mode, doesn\u0026rsquo;t require any user choice. The way the new window is inserted is determined by the value of the automatic scheme and the initial polarity settings.\nLongest side scheme # When the value of the automatic scheme is longest_side, the window will be attached as if the insertion point was in manual mode and the split direction was chosen based on the dimensions of the tiling rectangle and the initial polarity.\nLet\u0026rsquo;s consider the following scenario, where the initial polarity is set to second_child:\n1 a a ^ / \\ / \\ ---\u0026gt; 1 2 ---\u0026gt; 1 b ^ / \\ 2 3 ^ +-----------------------+ +-----------------------+ +-----------------------+ | | | | | | | | | | | | | | | 2 | | | | | | | | | | 1 | | 1 | 2 | | 1 |-----------| | ^ | | | ^ | | | | | | | | | | | 3 | | | | | | | | ^ | +-----------------------+ +-----------------------+ +-----------------------+ X Y Z In state X, a new window is added.\nSince 1 is wide, it gets split vertically and 2 is added as a\u0026rsquo;s second child given the initial polarity.\nThis leads to Y where we insert window 3. 2 is tall and is therefore split horizontally. 3 is once again added as b\u0026rsquo;s second child.\nAlternate scheme # When the value of the automatic scheme is alternate, the window will be attached as if the insertion point was in manual mode and the split direction was chosen based on the split type of the insertion point\u0026rsquo;s parent and the initial polarity. If the parent is split horizontally, the insertion point will be split vertically and vice versa.\nSpiral scheme # When the value of the automatic scheme is spiral, the window will take the space of the insertion point.\nLet\u0026rsquo;s dive into the details with the following scenario:\na a a / \\ / \\ / \\ 1 b ---\u0026gt; 1 c ---\u0026gt; 1 d / \\ / \\ / \\ 2 3 4 b 5 c ^ ^ / \\ ^ / \\ 3 2 b 4 / \\ 3 2 +-----------------------+ +-----------------------+ +-----------------------+ | | | | | | | | | | | 2 | | | 4 | | | 5 | | | ^ | | | ^ | | | ^ | | 1 |-----------| | 1 |-----------| | 1 |-----------| | | | | | | | | | 3 | | | | 3 | | | 3 | 2 | | |-----| 4 | | | | | | | | | | 2 | | +-----------------------+ +-----------------------+ +-----------------------+ X Y Z In state X, the insertion point, 2 is in automatic mode.\nWhen we add a new window, 4, the whole tree rooted at b is reattached, as the second child of a new internal node, c.\nThe splitting parameters of b (type: horizontal, ratio: ½) are copied to c and b is rotated by 90° clockwise.\nThe tiling rectangle of 4 in state Y is equal to the tiling rectangle of 2 in state X.\nThen the insertion of 5, with 4 as insertion point, leads to Z.\nThe spiral automatic scheme generates window spirals that rotate clockwise (resp. anti-clockwise) if the insertion point is the first (resp. second) child of its parent.\nSupported protocols and standards # The RandR and Xinerama protocols. A subset of the EWMH and ICCCM standards. Community # Want to get in touch with other bspwm users or you need help? Join us on our:\nSubreddit at r/bspwm. IRC channel at #bspwm on irc.libera.chat (maintained by @dannycolin / sdk on IRC). Matrix room at https://matrix.to/#/#bspwm:matrix.org ","date":"4 December 2022","permalink":"/projects/bspwm/","section":"Projects","summary":"Description # bspwm is a tiling window manager that represents windows as the leaves of a full binary tree.","title":"bspwm"},{"content":"","date":"3 December 2022","permalink":"/tags/algorithms/","section":"Tags","summary":"","title":"algorithms"},{"content":" Intro # A bloom filter is a data structure that allows you to quickly identify if some data has been previously added to the structure. What makes a bloom filter unique is that is that it gives up full accuracy for huge speed boost. A bloom filter has small false positive rate, and this rate can be decreased by using more memory and more hash algorithms, however you can find an optimal amount of memory and hash algorithm count to achieve great speed while still maintaining lower memory than a normal list. This specific implementation uses three different hashing algorithms.\nUse cases # Bloom filters are very convenient for many different use cases.\nMy favorite application is for checking if a username or unique id exists somewhere. Bloom filters have very low memory usage as well as being fast, so for a solution that doesn\u0026rsquo;t need 100% accuracy and can get away with something close to 99%, then a bloom filter might be the correct structure.\nImplementation # We will define a structure in Rust to represent the bloom filter.\nstruct BloomFilter { size: usize, hash_count: i8, bitvector: BitVec, } Bloom filters usually have two traits (methods) associated with the structure.\nadd an item to the structure fn add(\u0026amp;mut self, value: String); check if an item likely exists in the structure fn check(\u0026amp;self, value: String) -\u0026gt; bool; We define these traits for the structure by \u0026ldquo;Implementing them like this\u0026rdquo;.\ntrait Filter { fn add(\u0026amp;mut self, value: String); fn check(\u0026amp;self, value: String) -\u0026gt; bool; fn hash(\u0026amp;self, s: String, i: usize) -\u0026gt; i32; } impl Filter for BloomFilter { fn add(\u0026amp;mut self, value: String) { // ... } fn check(\u0026amp;self, value: String) -\u0026gt; bool { // ... } fn hash(\u0026amp;self, s: String, i: usize) -\u0026gt; i32 { // ... } } For the add trait, we need to call each hash function for the value given to get a likely unique set of keys for the value.\nfn add(\u0026amp;mut self, value: String) { for x in 0..self.hash_count { let v = self.hash(value.clone(), x.try_into().unwrap()); let k = v as usize % self.size; self.bitvector.set(k, true); } } We need to do something similar to check if a value has been added.\nfn check(\u0026amp;self, value: String) -\u0026gt; bool { let mut acc = 0; for x in 0..self.hash_count { let v = self.hash(value.clone(), x.try_into().unwrap()); let k = v as usize % self.size; if self.bitvector.get(k).unwrap_or(false) { acc += 1; } } return acc \u0026gt;= self.hash_count; } The hash function is just a collection of the other hash functions.\nfn hash(\u0026amp;self, s: String, i: usize) -\u0026gt; i32 { let functions: [\u0026amp;dyn Fn(String) -\u0026gt; i32; 3] = [\u0026amp;hash_1, \u0026amp;hash_2, \u0026amp;hash_3]; return functions[i](s); } Here are the other hash functions.\nfn hash_1(s: String) -\u0026gt; i32 { let mut hash = 0; let size = s.len(); for i in 0..size { hash = hash + (s.chars().nth(i)).unwrap() as i32 - 0x30; } hash } fn hash_2(s: String) -\u0026gt; i32 { let mut hash = 7; let size = s.len(); for i in 0..size { hash = (hash * 31 + (s.chars().nth(i)).unwrap() as i32 - 0x30) % size as i32; } hash % size as i32 } fn hash_3(s: String) -\u0026gt; i32 { (hash_2(s) + 7) * 3 } All together, it should look like this.\nfn hash_1(s: String) -\u0026gt; i32 { let mut hash = 0; let size = s.len(); for i in 0..size { hash = hash + (s.chars().nth(i)).unwrap() as i32 - 0x30; } hash } fn hash_2(s: String) -\u0026gt; i32 { let mut hash = 7; let size = s.len(); for i in 0..size { hash = (hash * 31 + (s.chars().nth(i)).unwrap() as i32 - 0x30) % size as i32; } hash % size as i32 } fn hash_3(s: String) -\u0026gt; i32 { (hash_2(s) + 7) * 3 } struct BloomFilter { size: usize, hash_count: i8, bitvector: BitVec, } trait Filter { fn add(\u0026amp;mut self, value: String); fn check(\u0026amp;self, value: String) -\u0026gt; bool; fn hash(\u0026amp;self, s: String, i: usize) -\u0026gt; i32; } impl Filter for BloomFilter { fn add(\u0026amp;mut self, value: String) { for x in 0..self.hash_count { let v = self.hash(value.clone(), x.try_into().unwrap()); let k = v as usize % self.size; self.bitvector.set(k, true); } } fn check(\u0026amp;self, value: String) -\u0026gt; bool { let mut acc = 0; for x in 0..self.hash_count { let v = self.hash(value.clone(), x.try_into().unwrap()); let k = v as usize % self.size; if self.bitvector.get(k).unwrap_or(false) { acc += 1; } } return acc \u0026gt;= self.hash_count; } fn hash(\u0026amp;self, s: String, i: usize) -\u0026gt; i32 { let functions: [\u0026amp;dyn Fn(String) -\u0026gt; i32; 3] = [\u0026amp;hash_1, \u0026amp;hash_2, \u0026amp;hash_3]; return functions[i](s); } } Other types of searches # For testing purposes, we can use two different types of searches to compare against the bloom filter.\nLinear Search # Linear search iterates through the array, checking if it exists. This has a linear time complexity O(n) and is not ideal for this and many other use cases.\n/// Search for the term using linear search fn linear_search(array: \u0026amp;[String], term: String) -\u0026gt; bool { for c in array { if c == term.as_str() { return true; } } return false; } Bogo Search # Bogo search is an algorithm that was designed to be purposefully bad. This algorithm has a time complexity of factorial time O(n!). This algorithm should never ever be used.\nIt essentially picks a random number to use at an index to check if the item is at that index. If it\u0026rsquo;s not, it repeats.\n/// Search for the term using the worst search algorithm, bogo search fn bogo_search(array: \u0026amp;[String], term: String) -\u0026gt; bool { let mut num: usize; loop { num = thread_rng().gen_range(0..array.len()); if array[num] == term.as_str() { return true; } } } Testing and Setup # The rest of the code in the project is for setting up the data to be searched and testing of the search algorithms.\nfn fill_array_and_bloom_filter(num_vec: \u0026amp;mut [String], bf: \u0026amp;mut BloomFilter) -\u0026gt; Result\u0026lt;()\u0026gt; { let file = File::open(\u0026#34;english-words/words.txt\u0026#34;)?; let reader = BufReader::new(file); let mut index = 0; for line in reader.lines() { if index \u0026lt; num_vec.len() { let l = line?; // Add word to array num_vec[index] = l.clone(); // Add word to bloom filter bf.add(l); } index += 1; } Ok(()) } fn main() { // Set up bloom filter let mut bf = BloomFilter { bitvector: BitVec::from_elem(10000, false), hash_count: 3, size: 10000, }; // Set up num vec let mut num_vec: Vec\u0026lt;String\u0026gt; = vec![String::new(); 17000]; fill_array_and_bloom_filter(\u0026amp;mut num_vec, \u0026amp;mut bf).unwrap(); num_vec.shuffle(\u0026amp;mut thread_rng()); let length = num_vec.len(); // Pick random term let mut rng = rand::thread_rng(); let x: usize = rng.gen_range(0..num_vec.len()); let term: String = num_vec[x].clone(); println!(\u0026#34;The randomly selected term is \u0026#39;{term}\u0026#39;\u0026#34;); // Calculate all of the hashes for the term let one = hash_1(term.clone()); let two = hash_2(term.clone()); let three = hash_3(term.clone()); println!(\u0026#34;The hashes for \u0026#39;{term}\u0026#39; are {one}, {two}, {three}\\n\u0026#34;); println!( \u0026#34;This will test how fast the word \u0026#39;{term}\u0026#39; can be found in the array of {length} words.\u0026#34; ); // Test the time it takes for linear search let before = Instant::now(); for _ in 0..100 { linear_search(\u0026amp;mut num_vec, term.clone()); } println!(\u0026#34;Elapsed time for linear_search: {:.2?}\u0026#34;, before.elapsed()); // Test the time it takes for bloom filter check let before = Instant::now(); for _ in 0..100 { bf.check(term.clone()); } println!( \u0026#34;Elapsed time for bloom filter check: {:.2?}\u0026#34;, before.elapsed() ); // Test the time is takes for bogo search let before = Instant::now(); for _ in 0..100 { bogo_search(\u0026amp;mut num_vec, term.clone()); } println!(\u0026#34;Elapsed time for bogo_search: {:.2?}\u0026#34;, before.elapsed()); } Results # The randomly selected term is \u0026#39;amicabilities\u0026#39; The hashes for \u0026#39;amicabilities\u0026#39; are 736, 11, 54 This will test how fast the word \u0026#39;amicabilities\u0026#39; can be found in the array of 17000 words. Elapsed time for linear_search: 220.81ms Elapsed time for bloom filter check: 3.61ms Elapsed time for bogo_search: 4.64s The randomly selected term is \u0026#39;Aldermaston\u0026#39; The hashes for \u0026#39;Aldermaston\u0026#39; are 618, 0, 21 This will test how fast the word \u0026#39;Aldermaston\u0026#39; can be found in the array of 17000 words. Elapsed time for linear_search: 245.29ms Elapsed time for bloom filter check: 2.69ms Elapsed time for bogo_search: 5.53s The randomly selected term is \u0026#39;Achille\u0026#39; The hashes for \u0026#39;Achille\u0026#39; are 354, 1, 24 This will test how fast the word \u0026#39;Achille\u0026#39; can be found in the array of 17000 words. Elapsed time for linear_search: 273.41ms Elapsed time for bloom filter check: 1.30ms Elapsed time for bogo_search: 6.68s Setting up the project # Cloning # git clone https://github.com/JakeRoggenbuck/bloom-filter-rs.git cd bloom-filter-rs Words Submodule # git submodule init git submodule update ","date":"3 December 2022","permalink":"/posts/bloom-filters/","section":"Posts","summary":"Intro # A bloom filter is a data structure that allows you to quickly identify if some data has been previously added to the structure.","title":"Bloom filters in Rust"},{"content":"","date":"3 December 2022","permalink":"/tags/concept/","section":"Tags","summary":"","title":"concept"},{"content":"","date":"3 December 2022","permalink":"/tags/data-structures/","section":"Tags","summary":"","title":"data-structures"},{"content":"","date":"3 December 2022","permalink":"/tags/language/","section":"Tags","summary":"","title":"language"},{"content":" newtons-method # Implementation of newtons method of zero approximation and more generally equation solution approximation.\ndouble newtons_method(double (*func)(double), double n) { // Question... where does func(x) = n? // func(x) - n = 0 double f(double x) { return func(x) - n; } // initial arbitrary guess double guess = 0.5; printf(\u0026#34;Guess for func(x) - %.2lf = 0\\n\u0026#34;, n); printf(\u0026#34;%lf\\n\u0026#34;, guess); for (int i = 0; i \u0026lt; 7; ++i) { // update guess for next iteration // x_1 = x_0 - f(x_0) / f\u0026#39;(x_0) // (x_n+1) = x_n - f(x_n) / f\u0026#39;(x_n) guess = guess - (f(guess) / derivative(f, guess)); printf(\u0026#34;%lf\\n\u0026#34;, guess); } } ","date":"30 November 2022","permalink":"/projects/newtons-method/","section":"Projects","summary":"newtons-method # Implementation of newtons method of zero approximation and more generally equation solution approximation.","title":"newtons-method"},{"content":"This month, I did 191 commits in 37 repos. I also opened 31 pull requests.\nResearch # I started a new paper that will be added to research when it\u0026rsquo;s done.\nTap # I worked on my program called Tap. This tool helps you quickly tap basic files into existence. Similar to snippets, tap generates full boiler plate files like simple make files, python with argparse setup, etc.\nFlamegraph # I started using flamegraph for auto-clock-speed\nHere is an example from Optimize read_lid_state function call #465\nMore with C # I have been continuing with using C for a handful of projects as well as my homework for a class.\nnewtons-method derive cutil jimbot c-data-structures Local List # I worked on the Inventory Management system for Local List.\n","date":"30 November 2022","permalink":"/devlogs/november-2022/","section":"Dev Logs","summary":"This month, I did 191 commits in 37 repos.","title":"November 2022"},{"content":" picom # picom is a compositor for X, and a fork of Compton.\nThis is a development branch, bugs to be expected\nYou can leave your feedback or thoughts in the discussion tab.\nChange Log # See Releases\nBuild # Dependencies # Assuming you already have all the usual building tools installed (e.g. gcc, python, meson, ninja, etc.), you still need:\nlibx11 libx11-xcb libXext xproto xcb xcb-damage xcb-xfixes xcb-shape xcb-renderutil xcb-render xcb-randr xcb-composite xcb-image xcb-present xcb-xinerama xcb-glx pixman libdbus (optional, disable with the -Ddbus=false meson configure flag) libconfig (optional, disable with the -Dconfig_file=false meson configure flag) libGL, libEGL (optional, disable with the -Dopengl=false meson configure flag) libpcre (optional, disable with the -Dregex=false meson configure flag) libev uthash On Debian based distributions (e.g. Ubuntu), the needed packages are\nlibxext-dev libxcb1-dev libxcb-damage0-dev libxcb-xfixes0-dev libxcb-shape0-dev libxcb-render-util0-dev libxcb-render0-dev libxcb-randr0-dev libxcb-composite0-dev libxcb-image0-dev libxcb-present-dev libxcb-xinerama0-dev libxcb-glx0-dev libpixman-1-dev libdbus-1-dev libconfig-dev libgl-dev libegl-dev libpcre2-dev libpcre3-dev libevdev-dev uthash-dev libev-dev libx11-xcb-dev meson On Fedora, the needed packages are\ndbus-devel gcc git libconfig-devel libdrm-devel libev-devel libX11-devel libX11-xcb libXext-devel libxcb-devel libGL-devel libEGL-devel meson pcre-devel pixman-devel uthash-devel xcb-util-image-devel xcb-util-renderutil-devel xorg-x11-proto-devel To build the documents, you need asciidoc\nTo build # $ git submodule update --init --recursive $ meson --buildtype=release . build $ ninja -C build Built binary can be found in build/src\nIf you have libraries and/or headers installed at non-default location (e.g. under /usr/local/), you might need to tell meson about them, since meson doesn\u0026rsquo;t look for dependencies there by default.\nYou can do that by setting the CPPFLAGS and LDFLAGS environment variables when running meson. Like this:\n$ LDFLAGS=\u0026#34;-L/path/to/libraries\u0026#34; CPPFLAGS=\u0026#34;-I/path/to/headers\u0026#34; meson --buildtype=release . build As an example, on FreeBSD, you might have to run meson with:\n$ LDFLAGS=\u0026#34;-L/usr/local/lib\u0026#34; CPPFLAGS=\u0026#34;-I/usr/local/include\u0026#34; meson --buildtype=release . build $ ninja -C build To install # $ ninja -C build install Default install prefix is /usr/local, you can change it with meson configure -Dprefix=\u0026lt;path\u0026gt; build\nHow to Contribute # Code # You can look at the Projects page, and see if there is anything that interests you. Or you can take a look at the Issues.\nNon-code # Even if you don\u0026rsquo;t want to contribute code, you can still contribute by compiling and running this branch, and report any issue you can find.\nContributions to the documents and wiki will also be appreciated.\nContributors # See CONTRIBUTORS\nThe README for the original Compton project can be found here.\nLicensing # picom is free software, made available under the MIT and MPL-2.0 software licenses. See the individual source files for details.\n","date":"30 November 2022","permalink":"/projects/picom/","section":"Projects","summary":"picom # picom is a compositor for X, and a fork of Compton.","title":"picom"},{"content":" RTC with DS1307 # Get current time with Real Time Clock using the Elegoo DS1307 Module\nResources # https://www.youtube.com/watch?v=6fSgAO36IfI https://pypi.org/project/smbus2/ https://microdigisoft.com/interfacing-rtc-ds1307-module-with-raspberry-pi-using-python/ https://datasheets.maximintegrated.com/en/ds/DS1307.pdf TODO # how to read data using i2c (using smbus2) where to read data from (from datasheet) ","date":"20 November 2022","permalink":"/projects/ds1307_rtc/","section":"Projects","summary":"RTC with DS1307 # Get current time with Real Time Clock using the Elegoo DS1307 Module","title":"DS1307_RTC"},{"content":" c-data-structures # A collection of data structures implemented in C.\nBuilding # gcc nobuild.c -o nobuild Running the following will put all of the executables in ./build/\n./nobuild ","date":"17 November 2022","permalink":"/projects/c-data-structures/","section":"Projects","summary":"c-data-structures # A collection of data structures implemented in C.","title":"c-data-structures"},{"content":"404: Not Found\n","date":"17 November 2022","permalink":"/projects/minimal-perfect-hash-gen/","section":"Projects","summary":"404: Not Found","title":"minimal-perfect-hash-gen"},{"content":" Efficiency First Color Library (EFCL) # The most simple, small, and fast terminal color text library.\nPriorities # Runtime speed Dependency size Quickstart # efcl = \u0026#34;0.1.3\u0026#34; use efcl::{color, Color, bold}; fn main() { println!(\u0026#34;Hello, {}!\u0026#34;, color!(Color::BLUE, \u0026#34;world\u0026#34;)); println!(\u0026#34;{}!\u0026#34;, bold!(\u0026amp;color!(Color::RED, \u0026#34;EFCL\u0026#34;).to_string())); } Why? # Here is a flamegraph of the auto-clock-speed project. In green is the time it takes for colored text to render. That is just over 9% of runtime cpu is taken by color formatting.\nThis is way too long for what we need it for. In auto-clock-speed, color is used sparingly and only a few default colors on top of that.\nThis library is for the most basic text coloring for the terminal and only includes the default terminal colors, no background color or text styles other than bold. This library is for speed.\n","date":"11 November 2022","permalink":"/projects/efcl/","section":"Projects","summary":"Efficiency First Color Library (EFCL) # The most simple, small, and fast terminal color text library.","title":"efcl"},{"content":"Here is an interesting thing that shows why compiled languages are good, specifically why Rust is great.\nTake this code example that checks an enum.\nfrom enum import Enum class Mode(Enum): Easy = 0 Hard = 1 if mode == Mode.Easy: print(\u0026#34;It\u0026#39;s easy.\u0026#34;) elif mode == Mode.Hard: print(\u0026#34;Hard mode.\u0026#34;) What happens if mode is set to None, or 2 or \u0026quot;Anything\u0026quot;. If that\u0026rsquo;s the case, neither method will be executed.\nif mode == Mode.Easy: print(\u0026#34;It\u0026#39;s easy.\u0026#34;) elif mode == Mode.Hard: print(\u0026#34;Hard mode.\u0026#34;) else: print(\u0026#34;Error?\u0026#34;) In languages like this, you will usually find a comment like this # This should never happen and like this Why? What went wrong? Time to debug to find the line that broke this.\nIn compiled languages, specifically a feature of statically typed language evaluated at compile time, mode would have a type, most likely int. This means that the options of a None type or a string cannot be represented in a variable of that type. This is cool and this happens in languages like C/C++, Java, etc. but this isn\u0026rsquo;t as good as it can get.\nIn Rust, We make invalid types unrepresentable. Here is a Rust example no more complex than the Python example, yet it insures safety that Python does not.\nThis code compiles because mode can only be Easy or Hard. The evaluation is between the same types. mode == Mode::EASY =\u0026gt; \u0026lt;Mode\u0026gt; == \u0026lt;Mode\u0026gt; =\u0026gt; type valid.\n#[derive(PartialEq)] enum Mode { EASY = 0, HARD = 1, } fn main() { let mode = Mode::EASY; if mode == Mode::EASY { println!(\u0026#34;It\u0026#39;s easy.\u0026#34;); } else if mode == Mode::HARD { println!(\u0026#34;Hard mode.\u0026#34;) } } However, if you accidentally what mode out for a string\u0026hellip;\nlet mode = \u0026#34;This is a string\u0026#34;; if mode == Mode::EASY { println!(\u0026#34;It\u0026#39;s easy.\u0026#34;); } else if mode == Mode::HARD { println!(\u0026#34;Hard mode.\u0026#34;) } You get many helpful compiler errors, like can't compare '\u0026amp;str' with 'Mode' so you know exactly what is wrong.\nAny even when C/C++ would fail, Rust gives an error letting you know what is wrong early.\nlet mode = 3; if mode == Mode::EASY { println!(\u0026#34;It\u0026#39;s easy.\u0026#34;); } else if mode == Mode::HARD { println!(\u0026#34;Hard mode.\u0026#34;) } =\u0026gt; can't compare '{integer}' with 'Mode'\n","date":"5 November 2022","permalink":"/posts/type-safety-in-rust/","section":"Posts","summary":"Here is an interesting thing that shows why compiled languages are good, specifically why Rust is great.","title":"Type safety in Rust"},{"content":" Update # In October I did 146 commits and created 9 repositories. I have been using C a lot this months because my class I am currently taking is in C. This has inspired me to write a lot of my new side projects in C. At least the ones where C makes sense as a language.\nThis month, I made an October themed fetch program. JakeRoggenbuck/spookyfetch\nAuto Clock Speed 0.1.10 # In this release, we closed and completed 76 issues. This was a relatively large release with tons of new features and lots of bug fixes.\nRelease can be found here\nAlgorithms # A friend challenged me to make a program to find anagrams in a list of words that has a better time complexity than quadratic.\nMy project can be found here in both python and rust.\nThe algorithm is most definitely better than quadratic O(n^2)\nIt loops over each word once. (Linear) and sorts the characters in the word. (Does not effect complexity) It then checks if this sorted string is a key in a dictionary. (get is O(1)) If it didn\u0026rsquo;t exist, it creates a new key value, with the word as the value and the sorted string as the key. (insert is O(1)) If it did exist, it appends the new word that matches the sorted value. (insert is O(1)) Because the only thing in the program that effects complexity is the linear loop over the words. This code has linear time complexity.\nTested with wordlist of 172787 lines long from - https://github.com/dwyl/english-words ============================================== Starting Rust real\t0m0.153s user\t0m0.133s sys\t0m0.020s ----------------- Starting Python real\t0m0.197s user\t0m0.176s sys\t0m0.020s ----------------- ","date":"31 October 2022","permalink":"/devlogs/october-2022/","section":"Dev Logs","summary":"Update # In October I did 146 commits and created 9 repositories.","title":"October 2022"},{"content":" clib-redef # A rewrite of the standard libraries in C for learning purposes.\nFunctions # strcmp strcpy ","date":"21 October 2022","permalink":"/projects/clib-redef/","section":"Projects","summary":"clib-redef # A rewrite of the standard libraries in C for learning purposes.","title":"clib-redef"},{"content":" homepage # Simple firefox homepage\n","date":"4 October 2022","permalink":"/projects/homepage/","section":"Projects","summary":"homepage # Simple firefox homepage","title":"homepage"},{"content":" spookyfetch # Fetch tool but spooky for hacktoberfest and hacktoberfest github topic\nInstall # go install github.com/jakeroggenbuck/spookyfetch@latest Display automatically in October # This code will run on shell startup and check if it\u0026rsquo;s October and display the spookyfetch if it is. I put this in my .bashrc file.\n# Get the date MONTH=$(date \u0026#34;+%m\u0026#34;) MONTH=10#${MONTH} # Check if it\u0026#39;s October if [[ $MONTH -eq 10 ]]; then SPOOKY=1 fi # Check if the October flag was set if [[ $SPOOKY -eq 1 ]]; then # Check if the spookyfetch program is installed if [[ $(which spookyfetch \u0026amp; \u0026gt;/dev/null 2\u0026gt;\u0026amp;1) ]]; then spookyfetch else echo \u0026#34;Install spookyfetch and add it to your PATH\u0026#34; fi fi ","date":"4 October 2022","permalink":"/projects/spookyfetch/","section":"Projects","summary":"spookyfetch # Fetch tool but spooky for hacktoberfest and hacktoberfest github topic","title":"spookyfetch"},{"content":" Anagram finder # Method of finding anagrams\nTested with wordlist of 172787 lines long from - https://github.com/dwyl/english-words ============================================== Starting Rust real\t0m0.153s user\t0m0.133s sys\t0m0.020s ----------------- Starting Python real\t0m0.197s user\t0m0.176s sys\t0m0.020s ----------------- ","date":"3 October 2022","permalink":"/projects/anagram/","section":"Projects","summary":"Anagram finder # Method of finding anagrams","title":"anagram"},{"content":" T3 research # I finished the first draft of my collaborative paper, titled Observations on Every Third Digit of the Thue-Morse Sequence. It is still waiting peer review, but all of the content is finished.\nAbstract # We investigate some interesting properties of the sequence made up of every third term of the Thue-Morse sequence, and consider other similar sequences.\nThe paper can be found at jr0.org/research/t3.\nFollow up paper # The same people involved in t3 will continue to work on a follow up paper that takes a similar problems into consideration. This paper will likely be published in 4 or 5 months.\nAuto Clock Speed # This month, I fixed 7 issues in auto clock speed\nStarting college # This month, I started my freshman year at UC Davis. I am taking a Math class, CS class, and a Philosophy class.\nIn the CS class, we will be using C, so I will probably do a lot of personal projects in C for the next few months.\nOne of those such projects is jimbot - a discord bot written in C.\nI will keep doing what is now undergraduate research in the areas of mathematics, computer science, and computational biology.\n","date":"30 September 2022","permalink":"/devlogs/september-2022/","section":"Dev Logs","summary":"T3 research # I finished the first draft of my collaborative paper, titled Observations on Every Third Digit of the Thue-Morse Sequence.","title":"September 2022"},{"content":" jimbot # Discord bot written in C to track milestones\n","date":"29 September 2022","permalink":"/projects/jimbot/","section":"Projects","summary":"jimbot # Discord bot written in C to track milestones","title":"jimbot"},{"content":" find-slope-with-secant # Find the slope at a point of a function with the secant method\n","date":"28 September 2022","permalink":"/projects/find-slope-with-secant/","section":"Projects","summary":"find-slope-with-secant # Find the slope at a point of a function with the secant method","title":"find-slope-with-secant"},{"content":" Day # A daily screen that shows the date and tasks for the day\nBackend # Using Gin-Gonic and Go\nFrontend # Using Yew and Rust\nColor Scheme # black: #000000; oxford-blue: #14213d; orange-web: #fca311; platinum: #e5e5e5; white: #ffffff; ","date":"19 September 2022","permalink":"/projects/day/","section":"Projects","summary":"Day # A daily screen that shows the date and tasks for the day","title":"day"},{"content":"","date":"7 September 2022","permalink":"/tags/clojure/","section":"Tags","summary":"","title":"Clojure"},{"content":" colorgradient-clj # Make a color gradient in Clojure!\nPython version colorgradient Rust version colorgradient-rs Julia version colorgradient-julia C version colorgradient-c Go version colorgradient-go Clojure version colorgradient-clj ","date":"7 September 2022","permalink":"/projects/colorgradient-clj/","section":"Projects","summary":"colorgradient-clj # Make a color gradient in Clojure!","title":"colorgradient-clj"},{"content":" jr0.org # Currently, Hugo to make this static website.\nNew # Recently (March 2022), I added a page for my new Dev Log, jr0.org/devlogs.\nHistory # This website has been up since May 26, 2019 in some form or another.\nCurrent # Prior # Prior # First # ","date":"7 September 2022","permalink":"/projects/jr0-source/","section":"Projects","summary":"jr0.","title":"jr0-source"},{"content":" Predict places that will likely have forest fires!\nFrontend (HTML/JS) # Backend (FastAPI) # ","date":"4 September 2022","permalink":"/projects/future-fire-finder/","section":"Projects","summary":"Predict places that will likely have forest fires!","title":"future-fire-finder"},{"content":" bloom-filter-rs # View blog post at jr0.org/posts/bloom-filters\nIntro # A bloom filter is a data structure that allows you to quickly identify if some data has been previously added to the structure. What makes a bloom filter unique is that is that it gives up full accuracy for huge speed boost. A bloom filter has small false positive rate, and this rate can be decreased by using more memory and more hash algorithms, however you can find an optimal amount of memory and hash algorithm count to achieve great speed while still maintaining lower memory than a normal list. This specific implementation uses three different hashing algorithms.\nUse cases # Bloom filters are very convenient for many different use cases.\nMy favorite application is for checking if a username or unique id exists somewhere. Bloom filters have very low memory usage as well as being fast, so for a solution that doesn\u0026rsquo;t need 100% accuracy and can get away with something close to 99%, then a bloom filter might be the correct structure.\nImplementation # We will define a structure in Rust to represent the bloom filter.\nstruct BloomFilter { size: usize, hash_count: i8, bitvector: BitVec, } Bloom filters usually have two traits (methods) associated with the structure.\nadd an item to the structure fn add(\u0026amp;mut self, value: String); check if an item likely exists in the structure fn check(\u0026amp;self, value: String) -\u0026gt; bool; We define these traits for the structure by \u0026ldquo;Implementing them like this\u0026rdquo;.\ntrait Filter { fn add(\u0026amp;mut self, value: String); fn check(\u0026amp;self, value: String) -\u0026gt; bool; fn hash(\u0026amp;self, s: String, i: usize) -\u0026gt; i32; } impl Filter for BloomFilter { fn add(\u0026amp;mut self, value: String) { // ... } fn check(\u0026amp;self, value: String) -\u0026gt; bool { // ... } fn hash(\u0026amp;self, s: String, i: usize) -\u0026gt; i32 { // ... } } For the add trait, we need to call each hash function for the value given to get a likely unique set of keys for the value.\nfn add(\u0026amp;mut self, value: String) { for x in 0..self.hash_count { let v = self.hash(value.clone(), x.try_into().unwrap()); let k = v as usize % self.size; self.bitvector.set(k, true); } } We need to do something similar to check if a value has been added.\nfn check(\u0026amp;self, value: String) -\u0026gt; bool { let mut acc = 0; for x in 0..self.hash_count { let v = self.hash(value.clone(), x.try_into().unwrap()); let k = v as usize % self.size; if self.bitvector.get(k).unwrap_or(false) { acc += 1; } } return acc \u0026gt;= self.hash_count; } The hash function is just a collection of the other hash functions.\nfn hash(\u0026amp;self, s: String, i: usize) -\u0026gt; i32 { let functions: [\u0026amp;dyn Fn(String) -\u0026gt; i32; 3] = [\u0026amp;hash_1, \u0026amp;hash_2, \u0026amp;hash_3]; return functions[i](s); } Here are the other hash functions.\nfn hash_1(s: String) -\u0026gt; i32 { let mut hash = 0; let size = s.len(); for i in 0..size { hash = hash + (s.chars().nth(i)).unwrap() as i32 - 0x30; } hash } fn hash_2(s: String) -\u0026gt; i32 { let mut hash = 7; let size = s.len(); for i in 0..size { hash = (hash * 31 + (s.chars().nth(i)).unwrap() as i32 - 0x30) % size as i32; } hash % size as i32 } fn hash_3(s: String) -\u0026gt; i32 { (hash_2(s) + 7) * 3 } All together, it should look like this.\nfn hash_1(s: String) -\u0026gt; i32 { let mut hash = 0; let size = s.len(); for i in 0..size { hash = hash + (s.chars().nth(i)).unwrap() as i32 - 0x30; } hash } fn hash_2(s: String) -\u0026gt; i32 { let mut hash = 7; let size = s.len(); for i in 0..size { hash = (hash * 31 + (s.chars().nth(i)).unwrap() as i32 - 0x30) % size as i32; } hash % size as i32 } fn hash_3(s: String) -\u0026gt; i32 { (hash_2(s) + 7) * 3 } struct BloomFilter { size: usize, hash_count: i8, bitvector: BitVec, } trait Filter { fn add(\u0026amp;mut self, value: String); fn check(\u0026amp;self, value: String) -\u0026gt; bool; fn hash(\u0026amp;self, s: String, i: usize) -\u0026gt; i32; } impl Filter for BloomFilter { fn add(\u0026amp;mut self, value: String) { for x in 0..self.hash_count { let v = self.hash(value.clone(), x.try_into().unwrap()); let k = v as usize % self.size; self.bitvector.set(k, true); } } fn check(\u0026amp;self, value: String) -\u0026gt; bool { let mut acc = 0; for x in 0..self.hash_count { let v = self.hash(value.clone(), x.try_into().unwrap()); let k = v as usize % self.size; if self.bitvector.get(k).unwrap_or(false) { acc += 1; } } return acc \u0026gt;= self.hash_count; } fn hash(\u0026amp;self, s: String, i: usize) -\u0026gt; i32 { let functions: [\u0026amp;dyn Fn(String) -\u0026gt; i32; 3] = [\u0026amp;hash_1, \u0026amp;hash_2, \u0026amp;hash_3]; return functions[i](s); } } Other types of searches # For testing purposes, we can use two different types of searches to compare against the bloom filter.\nLinear Search # Linear search iterates through the array, checking if it exists. This has a linear time complexity O(n) and is not ideal for this and many other use cases.\n/// Search for the term using linear search fn linear_search(array: \u0026amp;[String], term: String) -\u0026gt; bool { for c in array { if c == term.as_str() { return true; } } return false; } Bogo Search # Bogo search is an algorithm that was designed to be purposefully bad. This algorithm has a time complexity of factorial time O(n!). This algorithm should never ever be used.\nIt essentially picks a random number to use at an index to check if the item is at that index. If it\u0026rsquo;s not, it repeats.\n/// Search for the term using the worst search algorithm, bogo search fn bogo_search(array: \u0026amp;[String], term: String) -\u0026gt; bool { let mut num: usize; loop { num = thread_rng().gen_range(0..array.len()); if array[num] == term.as_str() { return true; } } } Testing and Setup # The rest of the code in the project is for setting up the data to be searched and testing of the search algorithms.\nfn fill_array_and_bloom_filter(num_vec: \u0026amp;mut [String], bf: \u0026amp;mut BloomFilter) -\u0026gt; Result\u0026lt;()\u0026gt; { let file = File::open(\u0026#34;english-words/words.txt\u0026#34;)?; let reader = BufReader::new(file); let mut index = 0; for line in reader.lines() { if index \u0026lt; num_vec.len() { let l = line?; // Add word to array num_vec[index] = l.clone(); // Add word to bloom filter bf.add(l); } index += 1; } Ok(()) } fn main() { // Set up bloom filter let mut bf = BloomFilter { bitvector: BitVec::from_elem(10000, false), hash_count: 3, size: 10000, }; // Set up num vec let mut num_vec: Vec\u0026lt;String\u0026gt; = vec![String::new(); 17000]; fill_array_and_bloom_filter(\u0026amp;mut num_vec, \u0026amp;mut bf).unwrap(); num_vec.shuffle(\u0026amp;mut thread_rng()); let length = num_vec.len(); // Pick random term let mut rng = rand::thread_rng(); let x: usize = rng.gen_range(0..num_vec.len()); let term: String = num_vec[x].clone(); println!(\u0026#34;The randomly selected term is \u0026#39;{term}\u0026#39;\u0026#34;); // Calculate all of the hashes for the term let one = hash_1(term.clone()); let two = hash_2(term.clone()); let three = hash_3(term.clone()); println!(\u0026#34;The hashes for \u0026#39;{term}\u0026#39; are {one}, {two}, {three}\\n\u0026#34;); println!( \u0026#34;This will test how fast the word \u0026#39;{term}\u0026#39; can be found in the array of {length} words.\u0026#34; ); // Test the time it takes for linear search let before = Instant::now(); for _ in 0..100 { linear_search(\u0026amp;mut num_vec, term.clone()); } println!(\u0026#34;Elapsed time for linear_search: {:.2?}\u0026#34;, before.elapsed()); // Test the time it takes for bloom filter check let before = Instant::now(); for _ in 0..100 { bf.check(term.clone()); } println!( \u0026#34;Elapsed time for bloom filter check: {:.2?}\u0026#34;, before.elapsed() ); // Test the time is takes for bogo search let before = Instant::now(); for _ in 0..100 { bogo_search(\u0026amp;mut num_vec, term.clone()); } println!(\u0026#34;Elapsed time for bogo_search: {:.2?}\u0026#34;, before.elapsed()); } Results # The randomly selected term is \u0026#39;amicabilities\u0026#39; The hashes for \u0026#39;amicabilities\u0026#39; are 736, 11, 54 This will test how fast the word \u0026#39;amicabilities\u0026#39; can be found in the array of 17000 words. Elapsed time for linear_search: 220.81ms Elapsed time for bloom filter check: 3.61ms Elapsed time for bogo_search: 4.64s The randomly selected term is \u0026#39;Aldermaston\u0026#39; The hashes for \u0026#39;Aldermaston\u0026#39; are 618, 0, 21 This will test how fast the word \u0026#39;Aldermaston\u0026#39; can be found in the array of 17000 words. Elapsed time for linear_search: 245.29ms Elapsed time for bloom filter check: 2.69ms Elapsed time for bogo_search: 5.53s The randomly selected term is \u0026#39;Achille\u0026#39; The hashes for \u0026#39;Achille\u0026#39; are 354, 1, 24 This will test how fast the word \u0026#39;Achille\u0026#39; can be found in the array of 17000 words. Elapsed time for linear_search: 273.41ms Elapsed time for bloom filter check: 1.30ms Elapsed time for bogo_search: 6.68s Setting up the project # Cloning # git clone https://github.com/JakeRoggenbuck/bloom-filter-rs.git cd bloom-filter-rs Words Submodule # git submodule init git submodule update ","date":"3 September 2022","permalink":"/projects/bloom-filter-rs/","section":"Projects","summary":"bloom-filter-rs # View blog post at jr0.","title":"bloom-filter-rs"},{"content":" Dataframe Function Speed Analysis # Testing a bunch of ways to apply a function to each item in a dataframe.\nGoal # Find the optimal way to apply different types of functions to a large dataframe.\nResults # Each test has been run 5 times, timed and averaged with the functions time_function, and time_average respectively.\nError bars are present, however they are very small (in the hundredths of seconds).\n","date":"3 September 2022","permalink":"/projects/dataframe-function-speed-analysis/","section":"Projects","summary":"Dataframe Function Speed Analysis # Testing a bunch of ways to apply a function to each item in a dataframe.","title":"dataframe-function-speed-analysis"},{"content":" Work # I just finished a month of work at Lucid Automation. I have been working with Go using Gin-Gonic and React JS. I have also been working with Keras and Tensorflow in Python. It has been an amazing experience and I have learned tons already.\nHackathons # In this month, I also did two hackathons, Plant Here and Future Fire Finder.\n1. Plant Here 🌿 # The first was a project called Plant here. Its goal is to help with food insecuritis by finding the best places on a map to plant certain types of food, ones with the correct amount of rain and soil pH.\nThe website can be found at https://PlantHere.org\n2. Future Fire Finder 🔥 # Predict places that will likely have forest fires.\n","date":"31 August 2022","permalink":"/devlogs/august-2022/","section":"Dev Logs","summary":"Work # I just finished a month of work at Lucid Automation.","title":"August 2022"},{"content":" bloom-filters # Bloom filter written in C\nView the blog post! https://jr0.org/posts/bloom-filters/\nIntro # A bloom filter is a data structure that allows you to quickly identify if some data has been previously added to the structure. What makes a bloom filter unique is that is that it gives up full accuracy for huge speed boost. A bloom filter has small false positive rate, and this rate can be decreased by using more memory and more hash algorithms, however you can find an optimal amount of memory and hash algorithm count to achieve great speed while still maintaining lower memory than a normal list. This specific implementation uses three different hashing algorithms.\nUse cases # Bloom filters are very convenient for many different use cases.\nMy favorite application is for checking if a username or unique id exists somewhere. Bloom filters have very low memory usage as well as being fast, so for a solution that doesn\u0026rsquo;t need 100% accuracy and can get away with something close to 99%, then a bloom filter might be the correct structure.\n\u0026hellip;\nView the rest of the post at https://jr0.org/posts/bloom-filters/\n","date":"25 August 2022","permalink":"/projects/bloom-filters/","section":"Projects","summary":"bloom-filters # Bloom filter written in C","title":"bloom-filters"},{"content":" Plant Here! # This simple web application (our submission to the 2022 Amazon Sustainability Data Initiaitve Hackathon) allows users to determine what crops would thrive best in their region based on rainfall data and soil pH analyzed with a simple machine learning model.\nFrontend (ReactJS) # Frontend is a very lightweight single webpage built with HTML/CSS, Vanilla JS, and Petite Vue.\nBackend (FastAPI) # Backend is written in FastAPI (more details soon).\nSetup Instructions # Install the required Python packages pip install -r requirements.txt Run the server inside the backend directory python3 main.py And you will see this! INFO: Started server process [18369] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit) Additional Credits # Floating plant icon created by Andinur on www.flaticon.com.\n","date":"23 August 2022","permalink":"/projects/asdi-global-hackathon/","section":"Projects","summary":"Plant Here!","title":"ASDI-Global-Hackathon"},{"content":" Amazon Sustainability Data Initiative\n","date":"23 August 2022","permalink":"/projects/aws-asdi-fire-finder-backed/","section":"Projects","summary":"Amazon Sustainability Data Initiative","title":"aws-asdi-fire-finder-backed"},{"content":"","date":"16 August 2022","permalink":"/tags/assembly/","section":"Tags","summary":"","title":"Assembly"},{"content":" one-time-pad-generator-asm # ","date":"16 August 2022","permalink":"/projects/one-time-pad-generator-asm/","section":"Projects","summary":" one-time-pad-generator-asm # ","title":"one-time-pad-generator-asm"},{"content":" Rust Command Line Chat (RCLC) # Structure # Install # The ~/.rclc/ dir needs to be created. So do the two FIFOs. Do this by running the following.\n./scripts/install.sh Uninstall # ./scripts/uninstall.sh ","date":"8 August 2022","permalink":"/projects/rclc/","section":"Projects","summary":"Rust Command Line Chat (RCLC) # Structure # Install # The ~/.","title":"rclc"},{"content":" hyper-minimal-loaders # A hyper minimal loader bar only 1.9 KB source.\nuse hyper_minimal_loaders::{Count, Counter}; use std::{thread, time}; fn main() { let mut counter = Count::default(); loop { counter.show(); counter.tick(); if counter.finished() { break; } thread::sleep(time::Duration::from_millis(100)); } } What is hyper-minimal-loaders? # It\u0026rsquo;s just a simple percentage loader\nWhy? # Sometimes you are making a program that will take hours to run, say you are computing large values of PI or something similar, and you want to know how long a certain operation will take. You may not necessarily want to print a bunch of info about the progress, especially every operation, because that would slow down the computation. You simply want to know if it\u0026rsquo;s going to take days, weeks, or months to complete. This was the type of thing that I needed when doing math research for t3 paper.\nHere is an example of me implementing one of these super simple loaders written directly in the code to compute these large numbers. I later decided to make this library so that I could abstract the logic outside of the math computation code, and instead use my own library.\nlet one_percent = MAX / 100; let mut percents_done = 0; loop { if t_location \u0026gt; one_percent * percents_done { percents_done += 1; print!(\u0026#34;\\x1b[1A\\x1b[2K\u0026#34;); println!(\u0026#34;{percents_done}%\u0026#34;); } // -- snip -- source found at https://github.com/JakeRoggenbuck/T3-Paper-Code/blob/main/rl-3-ratio/src/main.rs if t_location \u0026gt; MAX { break; } last = x; } print!(\u0026#34;\\x1b[1A\\x1b[2K\u0026#34;); println!(\u0026#34;{percents_done}%\u0026#34;); } ","date":"6 August 2022","permalink":"/projects/hyper-minimal-loaders/","section":"Projects","summary":"hyper-minimal-loaders # A hyper minimal loader bar only 1.","title":"hyper-minimal-loaders"},{"content":" JEC-rs # Jabacat\u0026rsquo;s Easy Config\nJEC-py | JEC-rs | JEC-go | JEC-c | JEC-c++ | JEC-zig | JEC-ts\nAPI # ConfigFile - exists - remove - create - from_home ConfigDir - exists - remove - create - from_home Usage # let conf = ConfigFile { path: \u0026#34;./test.yml\u0026#34;.to_string(), }; if !conf.exists() { conf.create(); } conf.remove(); let conf = ConfigDir { path: \u0026#34;./config/\u0026#34;.to_string(), }; if !conf.exists() { conf.create(); } conf.remove(); let conf = ConfigFile::from_home(\u0026#34;./test.yml\u0026#34;.to_string()); let conf = ConfigDir::from_home(\u0026#34;./config/\u0026#34;.to_string()); ","date":"3 August 2022","permalink":"/projects/jec-rs/","section":"Projects","summary":"JEC-rs # Jabacat\u0026rsquo;s Easy Config","title":"JEC-rs"},{"content":" JEC-py # Jabacat\u0026rsquo;s Easy Config\nJEC-py | JEC-rs | JEC-go | JEC-c | JEC-c++ | JEC-zig | JEC-ts\nAPI # ConfigFile - from_home - exists - remove - create ConfigDir - from_home - exists - remove - create Usage # conf_1 = ConfigFile(\u0026#34;./config.yml\u0026#34;) conf_2 = ConfigFile.from_home(\u0026#34;./config.yml\u0026#34;) dir_1 = ConfigDir(\u0026#34;./config/\u0026#34;) dir_2 = ConfigDir.from_home(\u0026#34;./config/\u0026#34;) if not dir_1.exists(): dir_1.create() dir_1.remove() ","date":"2 August 2022","permalink":"/projects/jec-py/","section":"Projects","summary":"JEC-py # Jabacat\u0026rsquo;s Easy Config","title":"JEC-py"},{"content":" JEC-go # Jabacat\u0026rsquo;s Easy Config\nJEC-py | JEC-rs | JEC-go | JEC-c++ | JEC-kt | JEC-c | JEC-zig | JEC-ts\nAPI # ConfigFile - Exists - Remove - Create - FromHome ConfigDir - Exists - Remove - Create - FromHome Usage # conf := ConfigFile{\u0026#34;./test.conf\u0026#34;} if !conf.Exists() { conf.Create() } dir := ConfigDir{\u0026#34;./config/\u0026#34;} if !dir.Exists() { dir.Create() } conf.Remove() dir.Remove() conf = ConfigFile{\u0026#34;\u0026#34;}.FromHome(\u0026#34;./test.conf\u0026#34;) strings.Contains(conf.path, \u0026#34;home\u0026#34;) // true at /home/user/test.conf ","date":"1 August 2022","permalink":"/projects/jec-go/","section":"Projects","summary":"JEC-go # Jabacat\u0026rsquo;s Easy Config","title":"JEC-go"},{"content":"This month, I made 231 commits in 30 different repositories. One of my favorite projects was BestNextStep which was a project that I thought of many years ago but was not able to fully develop until the last few years. I have also been working on a math paper with the current (tentative to change) title of Observations of Every Third Digit of the Thue-Morse Sequence. This month I created 11 repositories, two are cli applications in Go. They are timely for keeping track of hours for work and port-checker for checking open ports on a machine. Some things that I have been testing is Svelte, Graphql, gin-gonic. I will be starting work on August first as a Software Developer and I am very excited.\n","date":"31 July 2022","permalink":"/devlogs/july-2022/","section":"Dev Logs","summary":"This month, I made 231 commits in 30 different repositories.","title":"July 2022"},{"content":" Port Checker # Check for open tcp and tcp6 ports and their process info for server debugging\nInstall # git clone https://github.com/JakeRoggenbuck/port-checker.git cd port-checker go install ","date":"31 July 2022","permalink":"/projects/port-checker/","section":"Projects","summary":"Port Checker # Check for open tcp and tcp6 ports and their process info for server debugging","title":"port-checker"},{"content":" Timely # Keep track of work hour in the command line\nWhy # If you work remotely and need to log a certain amount of hours a week and may have a hard time keeping track, timely can help you track your start and end time of work sessions in the terminal.\nUsage # Usage of timely: -check Set to check -inline No newline after print -no-color No color output -no-word No word in output -off Set to off -toggle Set to toggle -work Set to work Add to prompt # In your bashrc or similar\nPS1=\u0026#34;$(timely -check -inline) \u0026#34; Setup # Create ~/.local/share/timely/ and ~/.local/share/timely/state\nmkdir -p ~/.local/share/timely/ printf off ~/.local/share/timely/state printf off ~/.local/share/timely/times Outputs # ","date":"31 July 2022","permalink":"/projects/timely/","section":"Projects","summary":"Timely # Keep track of work hour in the command line","title":"timely"},{"content":" structured-data-format-SoC-first-week # Summer Of Code (SoC) First Week - Structured Data Format\nThis week\u0026rsquo;s challenge # Build a new and unique structured data format similar to JSON or TOML with at least two intentional design decisions that improve upon some aspect of an existing data structure format.\nHow to participate # Fork this repo and start your work in your fork of the project! Projects are due on Sunday the 31st at 9pm PDT. Requirements # Write two paragraphs about your new data format (including two intentional design decisions and why) Give three examples of this data format being used Make a basic interpreter for this new file format and a library in any language with at least two functions ( loads, dumps). Optionally add load and dump (More info below) Have fun and be creative, make this a real thing that you will use in the future # Loads string of your new format into a native object obj = myformat.loads(\u0026#34;text\u0026#34;) # Write a string of your format from native object str = myformat.dumps(obj) Resources # https://toml.io/en/ https://yaml.org/ https://www.json.org/json-en.html https://www.geeksforgeeks.org/json-load-in-python/ https://www.geeksforgeeks.org/json-dump-in-python/ https://www.geeksforgeeks.org/json-loads-in-python/ https://www.geeksforgeeks.org/json-dumps-in-python/ What is Summer of Code? # Summer of Code is an initiative to encourage people to build cool and useful software over the summer. And most importantly to learn and have fun!!\n","date":"25 July 2022","permalink":"/projects/structured-data-format-soc-first-week/","section":"Projects","summary":"structured-data-format-SoC-first-week # Summer Of Code (SoC) First Week - Structured Data Format","title":"structured-data-format-SoC-first-week"},{"content":" Area API (Example Project) # area-api is a project to teach how to create a rest api using Gin Gonic\n","date":"12 July 2022","permalink":"/projects/area-api/","section":"Projects","summary":"Area API (Example Project) # area-api is a project to teach how to create a rest api using Gin Gonic","title":"area-api"},{"content":" Observations on Every Third Digit of the Thue-Morse Sequence # Adam Hutchings | Henry Hutchings | Jake Roggenbuck Abstract # We investigate some interesting properties of the sequence made up of every third term of the Thue-Morse sequence, and consider other similar sequences.\nT3.pdf ","date":"1 July 2022","permalink":"/projects/t3-paper-code/","section":"Projects","summary":"Observations on Every Third Digit of the Thue-Morse Sequence # Adam Hutchings | Henry Hutchings | Jake Roggenbuck Abstract # We investigate some interesting properties of the sequence made up of every third term of the Thue-Morse sequence, and consider other similar sequences.","title":"T3-Paper-Code"},{"content":"","date":"1 July 2022","permalink":"/tags/tex/","section":"Tags","summary":"","title":"TeX"},{"content":"This month I have committed to keeping up three system languages (Go, Rust, C) to learn more about backend tools and system programs. I also keep up my skills with Python because it is a great language to know. I made a library called Sense and wrote it in Go, Rust, C, and Python. I also wrote a preprocesor for Go in Go called Macaroon. I also wrote a study tool called study-cli in Go as well. I also continued to work on auto-clock-speed, with 1.0 on its way.\n","date":"30 June 2022","permalink":"/devlogs/june-2022/","section":"Dev Logs","summary":"This month I have committed to keeping up three system languages (Go, Rust, C) to learn more about backend tools and system programs.","title":"June 2022 - Go, C, and Rust!"},{"content":" sense-go # Sense the directories around you, are they git repositories, what language, etc.\nLanguages # 🐍 sense-py || 🦀 sense-rs || 🐹 gosense || 🇨 sense-c\nWhy? # Why so many langs? # Because I write projects pretty regularly in all of these languages and want a consistent API and available library support for all of them.\nAPI # Git # has_git(path string) bool is_local_git(path string) bool Language # get_lang(path string) Lang Example # func main() { lang := get_lang(\u0026#34;./\u0026#34;) fmt.Println(lang.String()) git := has_git(\u0026#34;./\u0026#34;) if git { fmt.Println(\u0026#34;Is git!\u0026#34;) } else { fmt.Println(\u0026#34;Is not git.\u0026#34;) } local := is_local_git(\u0026#34;./\u0026#34;) if local { fmt.Println(\u0026#34;Is local!\u0026#34;) } else { fmt.Println(\u0026#34;Is not local.\u0026#34;) } } Supported langs # Python JavaScript Rust Java Go TypeScript C CPP Contributing # If you would like to add features or language support, that would be amazing!\n","date":"29 June 2022","permalink":"/projects/gosense/","section":"Projects","summary":"sense-go # Sense the directories around you, are they git repositories, what language, etc.","title":"gosense"},{"content":" sense-c # Sense the directories around you, are they git repositories, what language, etc.\nLanguages # 🐍 sense-py || 🦀 sense-rs || 🐹 gosense || 🇨 sense-c\nWhy? # Why so many langs? # Because I write projects pretty regularly in all of these languages and want a consistent API and available library support for all of them.\nAPI # Git # int has_git(char *path); int is_local_git(char *path); git.h\nLanguage # enum LANG get_lang(char *path); Example # int main() { int lang = get_lang(\u0026#34;./\u0026#34;); printf(\u0026#34;%s\\n\u0026#34;, get_lang_name(lang)); int git = has_git(\u0026#34;./\u0026#34;); if (git) { printf(\u0026#34;Is git!\\n\u0026#34;); } else { printf(\u0026#34;Is not git.\\n\u0026#34;); } int local = is_local_git(\u0026#34;./\u0026#34;); if (local) { printf(\u0026#34;Is local!\\n\u0026#34;); } else { printf(\u0026#34;Is not local.\\n\u0026#34;); } return 0; } Supported langs # Python JavaScript Rust Java Go TypeScript C CPP Contributing # If you would like to add features or language support, that would be amazing!\n","date":"29 June 2022","permalink":"/projects/sense-c/","section":"Projects","summary":"sense-c # Sense the directories around you, are they git repositories, what language, etc.","title":"sense-c"},{"content":" sense-py # Sense the directories around you, are they git repositories, what language, etc.\nLanguages # 🐍 sense-py || 🦀 sense-rs || 🐹 gosense || 🇨 sense-c\nWhy? # Why so many langs? # Because I write projects pretty regularly in all of these languages and want a consistent API and available library support for all of them.\nAPI # Git # has_git(path: str) -\u0026gt; bool is_local_git(path: str) -\u0026gt; bool Language # get_lang(path: str) -\u0026gt; Lang Example # def main(): language = lang.get_lang(\u0026#34;./\u0026#34;) print(language) has_git_dir = git.has_git(\u0026#34;./\u0026#34;) if has_git_dir: print(\u0026#34;Is git!\u0026#34;) else: print(\u0026#34;Is not git.\u0026#34;) local = git.is_local_git(\u0026#34;./\u0026#34;) if local: print(\u0026#34;Is local!\u0026#34;) else: print(\u0026#34;Is not local.\u0026#34;) Supported langs # Python JavaScript Rust Java Go TypeScript C CPP Contributing # If you would like to add features or language support, that would be amazing!\n","date":"29 June 2022","permalink":"/projects/sense-py/","section":"Projects","summary":"sense-py # Sense the directories around you, are they git repositories, what language, etc.","title":"sense-py"},{"content":" sense-rs # Sense the directories around you, are they git repositories, what language, etc.\nLanguages # 🐍 sense-py || 🦀 sense-rs || 🐹 gosense || 🇨 sense-c\nWhy? # Why so many langs? # Because I write projects pretty regularly in all of these languages and want a consistent API and available library support for all of them.\nAPI # Git # has_git(path: String) -\u0026gt; bool; is_local_git(path: String) -\u0026gt; bool; Language # get_lang(path: String) -\u0026gt; Lang; Example # fn main() { let language = get_lang(\u0026#34;./\u0026#34;.to_string()); println!(\u0026#34;{}\u0026#34;, language); let git_dir = has_git(\u0026#34;./\u0026#34;.to_string()); if git_dir { println!(\u0026#34;Is git!\u0026#34;); } else { println!(\u0026#34;Is not git.\u0026#34;); } let local = is_local_git(\u0026#34;./\u0026#34;.to_string()); if local { println!(\u0026#34;Is local!\u0026#34;); } else { println!(\u0026#34;Is not local.\u0026#34;); } } Supported langs # Python JavaScript Rust Java Go TypeScript C CPP Contributing # If you would like to add features or language support, that would be amazing!\n","date":"29 June 2022","permalink":"/projects/sense-rs/","section":"Projects","summary":"sense-rs # Sense the directories around you, are they git repositories, what language, etc.","title":"sense-rs"},{"content":" Cosmic Ray Detector # If I find a cosmic ray with this, I will publish a paper about it.\nBuild # ./scripts/build.sh Install # ./scripts/install.sh ","date":"24 June 2022","permalink":"/projects/cosmic-ray-detector/","section":"Projects","summary":"Cosmic Ray Detector # If I find a cosmic ray with this, I will publish a paper about it.","title":"cosmic-ray-detector"},{"content":" Diri-c # DIRectory Info tool written in C that lets you look at a summary of a directory and which projects are using a version control system and if they are connected to a remote control system.\nWhy? # because\u0026hellip; I have too many directories\nSome of these are local git repos with github remotes, some are git repos with no remotes, and some are not using git at all.\nImage # Build # ./scripts/build.sh Install # ./scripts/install.sh ","date":"22 June 2022","permalink":"/projects/diri-c/","section":"Projects","summary":"Diri-c # DIRectory Info tool written in C that lets you look at a summary of a directory and which projects are using a version control system and if they are connected to a remote control system.","title":"diri-c"},{"content":" CoveRs # Code coverage for Rust in Rust\n","date":"20 June 2022","permalink":"/projects/covers/","section":"Projects","summary":"CoveRs # Code coverage for Rust in Rust","title":"covers"},{"content":" CVE Binary Tool quick start / README # The CVE Binary Tool is a free, open source tool to help you find known vulnerabilities in software, using data from the National Vulnerability Database (NVD) list of Common Vulnerabilities and Exposures (CVEs).\nThe tool has two main modes of operation:\nA binary scanner which helps you determine which packages may have been included as part of a piece of software. There are around 100 checkers which focus on common, vulnerable open source components such as openssl, libpng, libxml2 and expat. Tools for scanning known component lists in various formats, including .csv, several linux distribution package lists, language specific package scanners and several Software Bill of Materials (SBOM) formats. It is intended to be used as part of your continuous integration system to enable regular vulnerability scanning and give you early warning of known issues in your supply chain.\nFor more details, see our documentation or this quickstart guide\nCVE Binary Tool quick start / README Installing CVE Binary Tool Most popular usage options Finding known vulnerabilities using the binary scanner Finding known vulnerabilities in a list of components Scanning an SBOM file for known vulnerabilities Using the tool offline Output Options Full option list Configuration Using CVE Binary Tool in GitHub Actions Binary checker list Language Specific checkers Java Javascript Python Limitations Requirements Feedback \u0026amp; Contributions Security Issues Installing CVE Binary Tool # CVE Binary Tool can be installed using pip:\npip install cve-bin-tool You can also do pip install --user -e . to install a local copy which is useful if you\u0026rsquo;re trying the latest code from the cve-bin-tool github or doing development. The Contributor Documentation covers how to set up for local development in more detail.\nMost popular usage options # Finding known vulnerabilities using the binary scanner # To run the binary scanner on a directory or file:\ncve-bin-tool \u0026lt;directory/file\u0026gt; Note that this option will also use any language specific checkers to find known vulnerabilities in components.\nFinding known vulnerabilities in a list of components # To scan a comma-delimited (CSV) or JSON file which lists dependencies and versions:\ncve-bin-tool --input-file \u0026lt;filename\u0026gt; Note that the --input-file option can also be used to add extra triage data like remarks, comments etc. while scanning a directory so that output will reflect this triage data and you can save time of re-triaging (Usage: cve-bin-tool -i=test.csv /path/to/scan). A VEX file (which may be created using the --vex command line option) can also be used as a triage file. A VEX file is detected if the file suffix is \u0026lsquo;.vex\u0026rsquo;.\nScanning an SBOM file for known vulnerabilities # To scan a software bill of materials file (SBOM):\ncve-bin-tool --sbom \u0026lt;sbom_filetype\u0026gt; --sbom-file \u0026lt;sbom_filename\u0026gt; Valid SBOM types are SPDX, CycloneDX, and SWID.\nUsing the tool offline # Specifying the --offline option when running a scan ensures that cve-bin-tool doesn\u0026rsquo;t attempt to download the latest database files or to check for a newer version of the tool.\nNote that you will need to obtain a copy of the vulnerability data before the tool can run in offline mode. The offline how-to guide contains more information on how to set up your database.\nOutput Options # The CVE Binary Tool provides console-based output by default. If you wish to provide another format, you can specify this and a filename on the command line using --format. The valid formats are CSV, JSON, console, HTML and PDF. The output filename can be specified using the --output-file flag.\nThe reported vulnerabilities can additionally be reported in the Vulnerability Exchange (VEX) format by specifying --vex command line option. The generated VEX file can then be used as an --input-file to support a triage process.\nIf you wish to use PDF support, you will need to install the reportlab library separately.\nIf you intend to use PDF support when you install cve-bin-tool you can specify it and report lab will be installed as part of the cve-bin-tool install:\npip install cve-bin-tool[PDF] If you\u0026rsquo;ve already installed cve-bin-tool you can add reportlab after the fact using pip:\npip install --upgrade reportlab Note that reportlab was taken out of the default cve-bin-tool install because it has a known CVE associated with it ( CVE-2020-28463). The cve-bin-tool code uses the recommended mitigations to limit which resources added to PDFs, as well as additional input validation. This is a bit of a strange CVE because it describes core functionality of PDFs: external items, such as images, can be embedded in them, and thus anyone viewing a PDF could load an external image (similar to how viewing a web page can trigger external loads). There\u0026rsquo;s no inherent \u0026ldquo;fix\u0026rdquo; for that, only mitigations where users of the library must ensure only expected items are added to PDFs at the time of generation.\nSince users may not want to have software installed with an open, unfixable CVE associated with it, we\u0026rsquo;ve opted to make PDF support only available to users who have installed the library themselves. Once the library is installed, the PDF report option will function.\nFull option list # Usage: cve-bin-tool \u0026lt;directory/file to scan\u0026gt;\noptional arguments: -h, --help show this help message and exit -e, --exclude exclude path while scanning -V, --version show program's version number and exit --disable-version-check skips checking for a new version --detailed display detailed report --disable-validation-check skips checking xml files against schema --offline operate in offline mode\tCVE Data Download: -n {json,api}, --nvd {json,api} choose method for getting CVE lists from NVD -u {now,daily,never,latest}, --update {now,daily,never,latest} update schedule for NVD database (default: daily) --nvd-api-key NVD_API_KEY specify NVD API key (used to improve NVD rate limit) Input: directory directory to scan -i INPUT_FILE, --input-file INPUT_FILE provide input filename -C CONFIG, --config CONFIG provide config file -L PACKAGE_LIST, --package-list PACKAGE_LIST provide package list --sbom {spdx,cyclonedx,swid} specify type of software bill of materials (sbom) (default: spdx) --sbom-file SBOM_FILE provide sbom filename Output: -q, --quiet suppress output -l {debug,info,warning,error,critical}, --log {debug,info,warning,error,critical} log level (default: info) -o OUTPUT_FILE, --output-file OUTPUT_FILE provide output filename (default: output to stdout) --html-theme HTML_THEME provide custom theme directory for HTML Report -f {csv,json,console,html,pdf}, --format {csv,json,console,html,pdf} update output format (default: console) -c CVSS, --cvss CVSS minimum CVSS score (as integer in range 0 to 10) to report (default: 0) -S {low,medium,high,critical}, --severity {low,medium,high,critical} minimum CVE severity to report (default: low) --report Produces a report even if there are no CVE for the respective output format -A [\u0026lt;distro_name\u0026gt;-\u0026lt;distro_version_name\u0026gt;], --available-fix [\u0026lt;distro_name\u0026gt;-\u0026lt;distro_version_name\u0026gt;] Lists available fixes of the package from Linux distribution -b [\u0026lt;distro_name\u0026gt;-\u0026lt;distro_version_name\u0026gt;], --backport-fix [\u0026lt;distro_name\u0026gt;-\u0026lt;distro_version_name\u0026gt;] Lists backported fixes if available from Linux distribution --affected-versions Lists versions of product affected by a given CVE (to facilitate upgrades) --vex VEX Provide vulnerability exchange (vex) filename Merge Report: -a INTERMEDIATE_PATH, --append INTERMEDIATE_PATH provide path for saving intermediate report -t TAG, --tag TAG provide a tag to differentiate between multiple intermediate reports -m INTERMEDIATE_REPORTS, --merge INTERMEDIATE_REPORTS comma separated intermediate reports path for merging -F TAGS, --filter TAGS comma separated tags to filter out intermediate reports Checkers: -s SKIPS, --skips SKIPS comma-separated list of checkers to disable -r RUNS, --runs RUNS comma-separated list of checkers to enable Database Management: --export EXPORT export database filename --import IMPORT import database filename Deprecated: -x, --extract autoextract compressed files CVE Binary Tool autoextracts all compressed files by default now For further information about all of these options, please see the CVE Binary Tool user manual.\nNote: For backward compatibility, we still support csv2cve command for producing CVEs from csv but we recommend using the --input-file command going forwards.\n-L or --package-list option runs a CVE scan on installed packages listed in a package list. It takes a python package list (requirements.txt) or a package list of packages of systems that has dpkg, pacman or rpm package manager as an input for the scan. This option is much faster and detects more CVEs than the default method of scanning binaries.\nYou can get a package list of all installed packages in\na system using dpkg package manager by running dpkg-query -W -f '${binary:Package}\\n' \u0026gt; pkg-list a system using pacman package manager by running pacman -Qqe \u0026gt; pkg-list a system using rpm package manager by running rpm -qa --queryformat '%{NAME}\\n' \u0026gt; pkg-list in the terminal and provide it as an input by running cve-bin-tool -L pkg-list for a full package scan.\nConfiguration # You can use --config option to provide configuration file for the tool. You can still override options specified in config file with command line arguments. See our sample config files in the test/config\nUsing CVE Binary Tool in GitHub Actions # If you want to integrate cve-bin-tool as a part of your github action pipeline. You can checkout our example github action.\nBinary checker list # The following checkers are available for finding components in binary files:\nAvailable checkers accountsservice avahi bash bind binutils bolt bubblewrap busybox bzip2 commons_compress cronie cryptsetup cups curl dbus dnsmasq dovecot dpkg enscript expat ffmpeg freeradius ftp gcc gimp glibc gnomeshell gnupg gnutls gpgme gstreamer gupnp haproxy hdf5 hostapd hunspell icecast icu irssi jacksondatabind kbd kerberos kexectools libarchive libbpg libdb libebml libgcrypt libical libjpeg_turbo liblas libnss librsvg libseccomp libsndfile libsolv libsoup libsrtp libssh2 libtiff libvirt libvncserver libxslt lighttpd logrotate lua mariadb mdadm memcached mtr mysql nano ncurses nessus netpbm nginx node ntp open_vm_tools openafs openjpeg openldap openssh openssl openswan openvpn p7zip pcsc_lite pigz png polarssl_fedora poppler postgresql pspp python qt radare2 rsyslog rust samba sane_backends sqlite strongswan subversion sudo syslogng systemd tcpdump trousers varnish webkitgtk wireshark wpa_supplicant xerces xml2 zlib zsh All the checkers can be found in the checkers directory, as can the instructions on how to add a new checker. Support for new checkers can be requested via GitHub issues.\nLanguage Specific checkers # A number of checkers are available for finding vulnerable components in specific language packages.\nJava # The scanner examines the pom.xml file within a Java package archive to identify Java components. The package names and versions within the archive are used to search the database for vulnerabilities.\nJAR, WAR and EAR archives are supported.\nJavascript # The scanner examines the package-lock.json file within a javascript application to identify components. The package names and versions are used to search the database for vulnerabilities.\nPython # The scanner examines the PKG-INFO and METADATA files for an installed Python package to extract the component name and version which are used to search the database for vulnerabilities.\nThe tool supports the scanning of the contents of any Wheel package files (indicated with a file extension of .whl) and egg package files (indicated with a file extension of .egg).\nThe --package-list option can be used with a Python dependencies file requirements.txt to find the vulnerabilities in the list of components.\nLimitations # This scanner does not attempt to exploit issues or examine the code in greater detail; it only looks for library signatures and version numbers. As such, it cannot tell if someone has backported fixes to a vulnerable version, and it will not work if library or version information was intentionally obfuscated.\nThis tool is meant to be used as a quick-to-run, easily-automatable check in a non-malicious environment so that developers can be made aware of old libraries with security issues that have been compiled into their binaries.\nThe tool does not guarantee that any vulnerabilities reported are actually present or exploitable, neither is it able to find all present vulnerabilities with a guarantee.\nUsers can add triage information to reports to mark issues as false positives, false negatives, indicate that the risk has been mitigated by configuration/usage changes, and so on.\nTriage details can be re-used on other projects so, for example, triage on a Linux base image could be applied to multiple containers using that image.\nFor more information and usage of triage information with the tool kindly have a look here.\nIf you are using the binary scanner capabilities, be aware that we only have a limited number of binary checkers (see table above) so we can only detect those libraries. Contributions of new checkers are always welcome! You can also use an alternate way to detect components (for example, a bill of materials tool such as tern) and then use the resulting list as input to cve-bin-tool to get a more comprehensive vulnerability list.\nThe tool uses a vulnerability database in order to detect the present vulnerabilities, in case the database is not frequently updated (specially if the tool is used in offline mode), the tool would be unable to detect any newly discovered vulnerabilities. Hence it is highly advised to keep the database updated.\nRequirements # To use the auto-extractor, you may need the following utilities depending on the type of file you need to extract. The utilities below are required to run the full test suite on Linux:\nfile strings tar unzip rpm2cpio cpio ar cabextract Most of these are installed by default on many Linux systems, but cabextract and rpm2cpio in particular might need to be installed.\nOn windows systems, you may need:\nar 7z Expand pdftotext Windows has ar and Expand installed by default, but 7z in particular might need to be installed. If you want to run our test-suite or scan a zstd compressed file, We recommend installing this 7-zip-zstd fork of 7zip. We are currently using 7z for extracting jar, apk, msi, exe and rpm files.\nIf you get an error about building libraries when you try to install from pip, you may need to install the Windows build tools. The Windows build tools are available for free from https://visualstudio.microsoft.com/visual-cpp-build-tools/\nIf you get an error while installing brotlipy on Windows, installing the compiler above should fix it.\npdftotext is required for running tests. (users of cve-bin-tool may not need it, developers likely will.) The best approach to install it on Windows involves using conda (click here for further instructions).\nYou can check our CI configuration to see what versions of python we\u0026rsquo;re explicitly testing.\nFeedback \u0026amp; Contributions # Bugs and feature requests can be made via GitHub issues. Be aware that these issues are not private, so take care when providing output to make sure you are not disclosing security issues in other products.\nPull requests are also welcome via git.\nNew contributors should read the contributor guide to get started. Folk who already have experience contributing to open source projects may not need the full guide but should still use the pull request checklist to make things easy for everyone. Security Issues # Security issues with the tool itself can be reported to Intel\u0026rsquo;s security incident response team via https://intel.com/security.\nIf in the course of using this tool you discover a security issue with someone else\u0026rsquo;s code, please disclose responsibly to the appropriate party.\n","date":"17 June 2022","permalink":"/projects/cve-bin-tool/","section":"Projects","summary":"CVE Binary Tool quick start / README # The CVE Binary Tool is a free, open source tool to help you find known vulnerabilities in software, using data from the National Vulnerability Database (NVD) list of Common Vulnerabilities and Exposures (CVEs).","title":"cve-bin-tool"},{"content":"","date":"17 June 2022","permalink":"/tags/java/","section":"Tags","summary":"","title":"Java"},{"content":" Lark - a parsing toolkit for Python # Lark is a parsing toolkit for Python, built with a focus on ergonomics, performance and modularity.\nLark can parse all context-free languages. To put it simply, it means that it is capable of parsing almost any programming language out there, and to some degree most natural languages too.\nWho is it for?\nBeginners: Lark is very friendly for experimentation. It can parse any grammar you throw at it, no matter how complicated or ambiguous, and do so efficiently. It also constructs an annotated parse-tree for you, using only the grammar and an input, and it gives you convienient and flexible tools to process that parse-tree.\nExperts: Lark implements both Earley(SPPF) and LALR(1), and several different lexers, so you can trade-off power and speed, according to your requirements. It also provides a variety of sophisticated features and utilities.\nWhat can it do?\nParse all context-free grammars, and handle any ambiguity gracefully Build an annotated parse-tree automagically, no construction code required. Provide first-rate performance in terms of both Big-O complexity and measured run-time (considering that this is Python ;) Run on every Python interpreter (it\u0026rsquo;s pure-python) Generate a stand-alone parser (for LALR(1) grammars) And many more features. Read ahead and find out!\nMost importantly, Lark will save you time and prevent you from getting parsing headaches.\nQuick links # Documentation @readthedocs Cheatsheet (PDF) Online IDE Tutorial for writing a JSON parser. Blog post: How to write a DSL with Lark Gitter chat Install Lark # $ pip install lark --upgrade Lark has no dependencies.\nSyntax Highlighting # Lark provides syntax highlighting for its grammar files (*.lark):\nSublime Text \u0026amp; TextMate vscode Intellij \u0026amp; PyCharm Vim Atom Clones # These are implementations of Lark in other languages. They accept Lark grammars, and provide similar utilities.\nLerche (Julia) - an unofficial clone, written entirely in Julia. Lark.js (Javascript) - a port of the stand-alone LALR(1) parser generator to Javascsript. Hello World # Here is a little program to parse \u0026ldquo;Hello, World!\u0026rdquo; (Or any other similar phrase):\nfrom lark import Lark l = Lark(\u0026#39;\u0026#39;\u0026#39;start: WORD \u0026#34;,\u0026#34; WORD \u0026#34;!\u0026#34; %import common.WORD // imports from terminal library %ignore \u0026#34; \u0026#34; // Disregard spaces in text \u0026#39;\u0026#39;\u0026#39;) print( l.parse(\u0026#34;Hello, World!\u0026#34;) ) And the output is:\nTree(start, [Token(WORD, \u0026#39;Hello\u0026#39;), Token(WORD, \u0026#39;World\u0026#39;)]) Notice punctuation doesn\u0026rsquo;t appear in the resulting tree. It\u0026rsquo;s automatically filtered away by Lark.\nFruit flies like bananas # Lark is great at handling ambiguity. Here is the result of parsing the phrase \u0026ldquo;fruit flies like bananas\u0026rdquo;:\nRead the code here, and see more examples here.\nList of main features # Builds a parse-tree (AST) automagically, based on the structure of the grammar Earley parser Can parse all context-free grammars Full support for ambiguous grammars LALR(1) parser Fast and light, competitive with PLY Can generate a stand-alone parser ( read more) EBNF grammar Unicode fully supported Automatic line \u0026amp; column tracking Interactive parser for advanced parsing flows and debugging Grammar composition - Import terminals and rules from other grammars Standard library of terminals (strings, numbers, names, etc.) Import grammars from Nearley.js ( read more) Extensive test suite Type annotations (MyPy support) And much more! See the full list of features here\nComparison to other libraries # Performance comparison # Lark is fast and light (lower is better)\nCheck out the JSON tutorial for more details on how the comparison was made.\nFor a more thorough and objective comparison, checkout the Python Parsing Benchmarks repo.\nFeature comparison # Library Algorithm Grammar Builds tree? Supports ambiguity? Can handle every CFG? Line/Column tracking Generates Stand-alone Lark Earley/LALR(1) EBNF Yes! Yes! Yes! Yes! Yes! (LALR only) PLY LALR(1) BNF No No No No No PyParsing PEG Combinators No No No* No No Parsley PEG EBNF No No No* No No Parsimonious PEG EBNF Yes No No* No No ANTLR LL(*) EBNF Yes No Yes? Yes No (* PEGs cannot handle non-deterministic grammars. Also, according to Wikipedia, it remains unanswered whether PEGs can really parse all deterministic CFGs)\nProjects using Lark # Poetry - A utility for dependency management and packaging tartiflette - a GraphQL server by Dailymotion PyQuil - Python library for quantum programming using Quil Preql - An interpreted relational query language that compiles to SQL Hypothesis - Library for property-based testing mappyfile - a MapFile parser for working with MapServer configuration synapse - an intelligence analysis platform Datacube-core - Open Data Cube analyses continental scale Earth Observation data through time SPFlow - Library for Sum-Product Networks Torchani - Accurate Neural Network Potential on PyTorch Command-Block-Assembly - An assembly language, and C compiler, for Minecraft commands EQL - Event Query Language Fabric-SDK-Py - Hyperledger fabric SDK with Python 3.x required - multi-field validation using docstrings miniwdl - A static analysis toolkit for the Workflow Description Language pytreeview - a lightweight tree-based grammar explorer harmalysis - A language for harmonic analysis and music theory gersemi - A CMake code formatter MistQL - A query language for JSON-like structures Full list\nLicense # Lark uses the MIT license.\n(The standalone tool is under MPL2)\nContributors # Lark accepts pull-requests. See How to develop Lark\nBig thanks to everyone who contributed so far:\nSponsor # If you like Lark, and want to see us grow, please consider sponsoring us!\nContact the author # Questions about code are best asked on gitter or in the issues.\nFor anything else, I can be reached by email at erezshin at gmail com.\n\u0026ndash; Erez\n","date":"17 June 2022","permalink":"/projects/lark/","section":"Projects","summary":"Lark - a parsing toolkit for Python # Lark is a parsing toolkit for Python, built with a focus on ergonomics, performance and modularity.","title":"lark"},{"content":" REST APIs in every language # A collection of many REST frameworks in many languages.\nPlease add more langs and frameworks!\nPython Fast API Flask Django Rust Rocket Actix Javascript Express Go net/http gin Ruby Sinatra C Add frameworks C++ Add frameworks Java No API Spark ","date":"17 June 2022","permalink":"/projects/rest-api-in-every-lang/","section":"Projects","summary":"REST APIs in every language # A collection of many REST frameworks in many languages.","title":"rest-api-in-every-lang"},{"content":" Daily Tasks # Teaching Svelte and FastAPI\nFrontend (Javascript with Svelte) # Backend (Python with FastAPI) # ","date":"16 June 2022","permalink":"/projects/daily-tasks/","section":"Projects","summary":"Daily Tasks # Teaching Svelte and FastAPI","title":"daily-tasks"},{"content":" sqlite-cpp # Part of a learning project for an app with a C++ backend.\n","date":"16 June 2022","permalink":"/projects/sqlite-cpp/","section":"Projects","summary":"sqlite-cpp # Part of a learning project for an app with a C++ backend.","title":"sqlite-cpp"},{"content":" autoclockspeed.github.io # ","date":"14 June 2022","permalink":"/projects/autoclockspeed.github.io/","section":"Projects","summary":"autoclockspeed.","title":"autoclockspeed.github.io"},{"content":" study-cli # CLI tool to quiz and learn a question dataset. This can be used for memorizing any sort of multiple choice compliant quizzes. Similar to flashcards, except no need to write out flash cards, or carry them with you. Also, study-cli can be set to random ask if you\u0026rsquo;d like to study at random times.\nQuestion Set Schema # type Single struct { Name string `json:\u0026#34;name\u0026#34;` Correct int `json:\u0026#34;correct\u0026#34;` Answers []string `json:\u0026#34;answers\u0026#34;` Question string `json:\u0026#34;question\u0026#34;` } [ { \u0026#34;name\u0026#34;: \u0026#34;question one\u0026#34;, \u0026#34;correct\u0026#34;: 0, \u0026#34;answers\u0026#34;: [ \u0026#34;A. yes\u0026#34;, \u0026#34;B. no\u0026#34; ], \u0026#34;question\u0026#34;: \u0026#34;Which of the two options is a three letter world?\u0026#34;, } ] TODO # Add dataset manager ","date":"13 June 2022","permalink":"/projects/study-cli/","section":"Projects","summary":"study-cli # CLI tool to quiz and learn a question dataset.","title":"study-cli"},{"content":" Macaroon - Go Macros \u0026amp; Preprocessor # Add C style macros to code in Go. Not to be confused with go-macaroon/macaroon about \u0026ldquo;Cookies with Contextual Caveats for Decentralized Authorization in the Cloud\u0026rdquo;.\nQuick Start # Use the set keyword to define the value of a simple macro.\nBefore # package main #set NAME \u0026#34;jake\u0026#34; func main() { fmt.Println(NAME) } After # package main func main() { fmt.Println(\u0026#34;jake\u0026#34;) } Running # macaroon main.gomac \u0026amp;\u0026amp; go build ","date":"10 June 2022","permalink":"/projects/macaroon/","section":"Projects","summary":"Macaroon - Go Macros \u0026amp; Preprocessor # Add C style macros to code in Go.","title":"macaroon"},{"content":" auth-upload-rest-go # Authenticated REST server written in Go.\nUsage # Upload files to a dedicated server for file sharing, etc.\nRunning # # Run the server cd backend go run . # Open the index.html with firefox - you can use whatever browser you like firefox index.html Images # ","date":"8 June 2022","permalink":"/projects/auth-upload-rest-go/","section":"Projects","summary":"auth-upload-rest-go # Authenticated REST server written in Go.","title":"auth-upload-rest-go"},{"content":" better-scripts # Scripts that are better than the ones I wrote a long time ago\nDocs # Docs for each script can be found it its directory.\nInstall # sh ./install.sh Build # Installing will write the scripts to build/\nGoals # The goal with this repository is to have better documentation, readability, and functionality than the old repository.\n","date":"7 June 2022","permalink":"/projects/better-scripts/","section":"Projects","summary":"better-scripts # Scripts that are better than the ones I wrote a long time ago","title":"better-scripts"},{"content":" colorgradient-go # Colorgradient learning project in Go\nPython version colorgradient Rust version colorgradient-rs Julia version colorgradient-julia C version colorgradient-c Go version colorgradient-go Clojure version colorgradient-clj ","date":"5 June 2022","permalink":"/projects/colorgradient-go/","section":"Projects","summary":"colorgradient-go # Colorgradient learning project in Go","title":"colorgradient-go"},{"content":" Stack Overflow Search IntelliJ # IntelliJ plugin to search code on stackoverflow, inspired by @JakeRoggenbuck\u0026rsquo;s Stack Overflow Search Vim\nWork in progess\n","date":"4 June 2022","permalink":"/projects/stackoverflow-search-intellij/","section":"Projects","summary":"Stack Overflow Search IntelliJ # IntelliJ plugin to search code on stackoverflow, inspired by @JakeRoggenbuck\u0026rsquo;s Stack Overflow Search Vim","title":"stackoverflow-search-intellij"},{"content":" Stack Overflow Search Vim # Inspired by a conversation with @Shuzhengz. Still a work in progress.\nTask List # Function to return currently highlighted text Function to open xdg default browser with stackoverflow query Fix search function to escape spaces in search Make keybind for function that does both ","date":"2 June 2022","permalink":"/projects/stackoverflow-search-vim/","section":"Projects","summary":"Stack Overflow Search Vim # Inspired by a conversation with @Shuzhengz.","title":"stackoverflow-search-vim"},{"content":"","date":"2 June 2022","permalink":"/tags/vim-script/","section":"Tags","summary":"","title":"Vim Script"},{"content":"This month I had my finals and AP tests, all of which where a lot of work, but I still worked on some project and organized the first annual Da Vinci Game Jam for my school. Links to the resources for the workshops can be found here. The game jam was an overall success and we had many great game entries. This month I also worked on auto clock speed some more, adding a universal interface to all of the commands and other issues. I created a total of 44 commits on that project this month. GitHub summarizes this month with \u0026lsquo;Created 238 commits in 26 repositories\u0026rsquo;, I also created 13 repositories this month, two for the game jam, one related to a possible future auto-clock-speed side project, two about my statistics class, two about my link shortener project, two related to JABACAT (Group of programmers), and one as a tool to help view csv files that I might release sometime next month.\n","date":"31 May 2022","permalink":"/devlogs/may-2022/","section":"Dev Logs","summary":"This month I had my finals and AP tests, all of which where a lot of work, but I still worked on some project and organized the first annual Da Vinci Game Jam for my school.","title":"May 2022 - Da Vinci Game Jam \u0026 More Auto Clock Speed"},{"content":" ocean-game-js # ","date":"24 May 2022","permalink":"/projects/ocean-game-js/","section":"Projects","summary":" ocean-game-js # ","title":"ocean-game-js"},{"content":" ocean-game-py # Simple ocean game demo for DV Game Jam\nSlides # View the slides pygame_slides.pdf\nImage # ","date":"23 May 2022","permalink":"/projects/ocean-game-py/","section":"Projects","summary":"ocean-game-py # Simple ocean game demo for DV Game Jam","title":"ocean-game-py"},{"content":" statistical-tests-rs # Statistical Tests for Rust\nExamples # use statistical_tests_rs::mean; fn main() { // Get the mean of an array let array: [f64; 4] = [3.4, 6.7, 2.3, 1.1]; let m = mean(\u0026amp;array); println!(\u0026#34;{}\u0026#34;, m); } use statistical_tests_rs::{GetStatistics, SampleStatistics}; fn main() { // Get the statistics of a sample let samp = SampleStatistics::from_array(\u0026amp;array); println!( \u0026#34;Sample Mean: {}, Sample Standard Deviation: {}\u0026#34;, samp.sample_mean, samp.standard_error ); } ","date":"22 May 2022","permalink":"/projects/statistical-tests-rs/","section":"Projects","summary":"statistical-tests-rs # Statistical Tests for Rust","title":"statistical-tests-rs"},{"content":" link-cli # Utility to manage link.jr0.org\n","date":"17 May 2022","permalink":"/projects/link-cli/","section":"Projects","summary":"link-cli # Utility to manage link.","title":"link-cli"},{"content":" chi-squared-c # Calculate chi squared\nEquation # Χ^2 = ((O - E)^2) / E ","date":"4 May 2022","permalink":"/projects/chi-squared-c/","section":"Projects","summary":"chi-squared-c # Calculate chi squared","title":"chi-squared-c"},{"content":"On April 21st, the whole Local List team (Me and 4 others) got to present our business to our town\u0026rsquo;s chamber of commerce. Our idea and demo was met with lots of excitement and great feedback. The day after, we got to pitch our idea to more business owners during our school\u0026rsquo;s business project\u0026rsquo;s night event. We won the event\u0026rsquo;s Judges award for our idea and presentation. The current site can be found at thelocallist.org. Our judges invited us to a city event where we will have a booth and present to the public. This month, I also made two projects ( derive, and integrate) in C that implement ideas from calculus. I also started using Svelte for a mindfulness tracker app that I might publish in a few weeks.\n","date":"30 April 2022","permalink":"/devlogs/april-2022/","section":"Dev Logs","summary":"On April 21st, the whole Local List team (Me and 4 others) got to present our business to our town\u0026rsquo;s chamber of commerce.","title":"April 2022- Local List Business Pitch"},{"content":" c-select # ncurses selection tool for tui based projects.\n","date":"25 April 2022","permalink":"/projects/c-select/","section":"Projects","summary":"c-select # ncurses selection tool for tui based projects.","title":"c-select"},{"content":" integrate # Build # Run ./scripts/build.sh Config # You can replace this function with anything you would like to find the antiderivative of.\ndouble function(double x) { // f(x) = x^3 + 4x + 2 return (pow(x, 3) + 4 * x) + 2; } Run # Run ./cmake/derive \u0026gt; data/out.csv Run python3 plotting/main.py ","date":"18 April 2022","permalink":"/projects/integrate/","section":"Projects","summary":"integrate # Build # Run .","title":"integrate"},{"content":" approximate-pi # This program creates a bunch of points in a square, then draws a circle with the same radius. Then it checks which points are inside the circle. The ratio of inside to out should be around pi/4, therefor multiplying that by 4 should result in an approximation of pi.\nFavorite bit of code # int calculate_single() { int s; s = rand() % DIAMETER; return pow(s, 2) + RADIUS; } int calculate_point() { int x = calculate_single(); int y = calculate_single(); return x + y \u0026lt;= pow(DIAMETER, 2); } Run # Run ./cmake/approximate_pi Build # Run ./scripts/build.sh Results # 787961 inside the circle, 212039 outside. 3.151844 787271 inside the circle, 212729 outside. 3.149084 ","date":"17 April 2022","permalink":"/projects/approximate-pi/","section":"Projects","summary":"approximate-pi # This program creates a bunch of points in a square, then draws a circle with the same radius.","title":"approximate-pi"},{"content":" derive # Calculate the derivative of a function non-algebraically\nBuild # Run ./scripts/build.sh Config # You can replace this function with anything you would like to find the derivative of.\ndouble function(double x) { // f(x) = x^3 + 4x + 2 return (pow(x, 3) + 4 * x) + 2; } Run # Run ./cmake/derive \u0026gt; data/out.csv Run python3 plotting/main.py ","date":"17 April 2022","permalink":"/projects/derive/","section":"Projects","summary":"derive # Calculate the derivative of a function non-algebraically","title":"derive"},{"content":" colorgradient-c # Colorgradient learning project in C\nPython version colorgradient Rust version colorgradient-rs Julia version colorgradient-julia C version colorgradient-c Go version colorgradient-go Clojure version colorgradient-clj ","date":"11 April 2022","permalink":"/projects/colorgradient-c/","section":"Projects","summary":"colorgradient-c # Colorgradient learning project in C","title":"colorgradient-c"},{"content":" termcolor-c # Simple color printing in C\nExample # #include \u0026#34;termcolor.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; int main() { cprint(\u0026#34;Passed\\n\u0026#34;, FG_GREEN); cprint(\u0026#34;Failed\\n\u0026#34;, FG_RED); char *msg = \u0026#34;Hello!!\u0026#34;; char *new; color(msg, new, FG_BLUE); printf(\u0026#34;%s\\n\u0026#34;, new); return 0; } Quick start # Add termcolor as a dependency # git submodule add git@github.com:JakeRoggenbuck/termcolor-c.git For CMakeLists.txt (if you are using that) # -add_executable(project_name ${SOURCES}) +add_executable(project_name ${SOURCES} termcolor-c/src/termcolor.c) +include_directories(${PROJECT_SOURCE_DIR}/termcolor-c/src) Add the color printing code # #include \u0026#34;termcolor.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; int main() { cprint(\u0026#34;Hello World!\\n\u0026#34;, FG_GREEN); return 0; } Why # I made this just in case I start writing a bunch of CLI stuff in C in the next few months before college, and during college as well. So far, here are my favorites:\ndiri-c cosmic-ray-detector ","date":"11 April 2022","permalink":"/projects/termcolor-c/","section":"Projects","summary":"termcolor-c # Simple color printing in C","title":"termcolor-c"},{"content":"This month, I registered the domains finditlocal.org and finditlocal.us for a company called Local List I started in connection with my Econ class. I created a frontend using React and a backend using Django. Currently, both are hosted and communicating to display local products at those domains. I am really excited for this project and I even printed business cards and stickers for the project to share with classmates. I feel like this project is the latest culmination of my skills in frontend, backend, and system architecture. The part I am most proud of is the architecture and design decisions to allow for further development and higher traffic as the project grows. I also worked more on my yet to be released (as of March 31st 2022) runtime-analysis-paper, including a new idea I thought of regarding error handling.\nfinditlocal\n","date":"31 March 2022","permalink":"/devlogs/march-2022/","section":"Dev Logs","summary":"This month, I registered the domains finditlocal.","title":"March 2022 - Starting Local List"},{"content":" The Algorithms - Rust All algorithms implemented in Rust - for education List of Algorithms # See our directory for easier navigation and a better overview of the project.\nContributing # Read through our Contribution Guidelines before you contribute.\n","date":"31 March 2022","permalink":"/projects/rust/","section":"Projects","summary":"The Algorithms - Rust All algorithms implemented in Rust - for education List of Algorithms # See our directory for easier navigation and a better overview of the project.","title":"Rust"},{"content":" A command-line Git information tool written in Rust 日本語 | فارسی | 简体中文 | Русский | Español Onefetch is a command-line Git information tool written in Rust that displays project information and code statistics for a local Git repository directly on your terminal. The tool is completely offline - no network access is required.\nBy default, the repo\u0026rsquo;s information is displayed alongside the dominant language\u0026rsquo;s logo, but you can further configure onefetch to instead use an image - on supported terminals -, a text input or nothing at all.\nIt automatically detects open source licenses from texts and provides the user with valuable information like code distribution, pending changes, number of dependencies (by package manager), top contributors (by number of commits), size on disk, creation date, LOC (lines of code), etc.\nOnefetch can be configured via command-line flags to display exactly what you want, the way you want it to: you can customize ASCII/Text formatting, disable info lines, ignore files \u0026amp; directories, output in multiple formats (Json, Yaml), etc.\nAs of now, onefetch supports more than 50 different programming languages; if your language of choice isn\u0026rsquo;t supported: Open up an issue and support will be added.\nContributions are very welcome! See CONTRIBUTING for more info.\nMore: [ Wiki] [ Installation] [ Getting Started] # ","date":"24 March 2022","permalink":"/projects/onefetch/","section":"Projects","summary":"A command-line Git information tool written in Rust 日本語 | فارسی | 简体中文 | Русский | Español Onefetch is a command-line Git information tool written in Rust that displays project information and code statistics for a local Git repository directly on your terminal.","title":"onefetch"},{"content":" mac-shifter # ","date":"21 March 2022","permalink":"/projects/mac-shifter/","section":"Projects","summary":" mac-shifter # ","title":"mac-shifter"},{"content":"404: Not Found\n","date":"7 March 2022","permalink":"/projects/python-guides/","section":"Projects","summary":"404: Not Found","title":"Python-Guides"},{"content":" Vosk Speech Recognition Toolkit # Vosk is an offline open source speech recognition toolkit. It enables speech recognition for 20+ languages and dialects - English, Indian English, German, French, Spanish, Portuguese, Chinese, Russian, Turkish, Vietnamese, Italian, Dutch, Catalan, Arabic, Greek, Farsi, Filipino, Ukrainian, Kazakh, Swedish, Japanese, Esperanto. More to come.\nVosk models are small (50 Mb) but provide continuous large vocabulary transcription, zero-latency response with streaming API, reconfigurable vocabulary and speaker identification.\nSpeech recognition bindings implemented for various programming languages like Python, Java, Node.JS, C#, C++ and others.\nVosk supplies speech recognition for chatbots, smart home appliances, virtual assistants. It can also create subtitles for movies, transcription for lectures and interviews.\nVosk scales from small devices like Raspberry Pi or Android smartphone to big clusters.\nDocumentation # For installation instructions, examples and documentation visit Vosk Website.\n","date":"7 March 2022","permalink":"/projects/vosk-api/","section":"Projects","summary":"Vosk Speech Recognition Toolkit # Vosk is an offline open source speech recognition toolkit.","title":"vosk-api"},{"content":" yfin 0.1.2 # Yfin is the Official package manager for the Y-flat programming language. Yfin allows the user to install, upgrade, and uninstall packages. It also allows a user to initialize a package with the Y-flat package structure and files automatically generated. In future, Yfin will also allow users to publish packages.\nUsage # yfin \u0026lt;SUBCOMMAND\u0026gt; Flags # -h, --help Prints help information -V, --version Prints version information Subcommands # help Prints this message or the help of the given subcommand(s) init Initialize a package install Install from git repo url install-compiler Install compiler yfin install-compiler install-yflib Install yflib yfin install-yflib uninstall Uninstall package upgrade Install newer version of package Install Latest # If you have cargo on your machine, skip to step 3\nInstall rustup.rs.\nSetup rust\nrustup override set stable rustup update stable Install from crates\ncargo install --git https://github.com/JakeRoggenbuck/yfin New package # Create a new package with yfin init \u0026lt;name\u0026gt; or yfin init for the current directory. Here is what you will see in the directory.\nyf-package-example (main) λ tree . . ├── package.yml └── src └── lib.yf 1 directory, 3 files Join the discussion # Full (0.1.1) # Install Compiler (0.1.2) # ","date":"4 March 2022","permalink":"/projects/yfin/","section":"Projects","summary":"yfin 0.","title":"yfin"},{"content":"This months has been pretty interesting. I have learned several things like how to use Gatsby and GraphQL. I have been working on auto-clock-speed a lot, and I 0.1.8 was released! I have also been writing code for the Hermes project. This months I picked up the book \u0026lsquo;Design Patterns\u0026rsquo; and it\u0026rsquo;s great. Something that I have been using more often is React. Sometimes I forget how useful a good frontend framework can be. On an unrelated note, I have been learning more about computer science theory and software development history. Knowing what others did in the past is helpful when making decision.\n","date":"28 February 2022","permalink":"/devlogs/february-2022/","section":"Dev Logs","summary":"This months has been pretty interesting.","title":"February 2022 - Creating Dev Log Page"},{"content":" Hermes # Hermes is a project in C++ to help individuals with ALS communicate.\n","date":"4 February 2022","permalink":"/projects/hermes/","section":"Projects","summary":"Hermes # Hermes is a project in C++ to help individuals with ALS communicate.","title":"hermes"},{"content":" yflat-docs # Documentation for all parts of Y-flat.\nFiles # Y-flat description and examples\n","date":"4 February 2022","permalink":"/projects/yflat-docs/","section":"Projects","summary":"yflat-docs # Documentation for all parts of Y-flat.","title":"yflat-docs"},{"content":" Project Invent: Rube Goldberg Machine # ","date":"29 January 2022","permalink":"/projects/project-invent-rube-goldberg-machine/","section":"Projects","summary":" Project Invent: Rube Goldberg Machine # ","title":"project-invent-rube-goldberg-machine"},{"content":" Reduc # A lightweight general purpose window manager written in rust\n","date":"25 January 2022","permalink":"/projects/reduc/","section":"Projects","summary":"Reduc # A lightweight general purpose window manager written in rust","title":"Reduc"},{"content":" pascals-triangle # Exploration of pascals triangle\n","date":"14 January 2022","permalink":"/projects/pascals-triangle/","section":"Projects","summary":"pascals-triangle # Exploration of pascals triangle","title":"pascals-triangle"},{"content":"404: Not Found\n","date":"13 January 2022","permalink":"/projects/indirect-git-repo/","section":"Projects","summary":"404: Not Found","title":"indirect-git-repo"},{"content":" dark-discord # An actual dark theme for Discord\nRequirements # BetterDiscord is required to be installed\nBetterdiscordctl might be a handy tool to install BetterDiscord on Linux\nInstall Instructions # Install as a theme # Go to the Themes tab in Discord\u0026rsquo;s settings (BetterDiscord section) Click on the Open Theme Folder button Drag dark-discord.theme.css into the folder Go back to the Themes tab in Discord\u0026rsquo;s settings, enable the theme with the switch Install as a custom CSS # The custom CSS option can be useful if your install of betterdiscord is bugged or not supported fully\nTo use this option,\nGo to the Custom CSS tab in Discord\u0026rsquo;s settings Copy the content in dark-discord.scss ( here) Paste in the textbox Click on the save button, then the update button to apply Customize # You can customize this theme by changing the color hex codes in the file\n(Be careful, there\u0026rsquo;s a lot of them)\n","date":"4 January 2022","permalink":"/projects/dark-discord/","section":"Projects","summary":"dark-discord # An actual dark theme for Discord","title":"dark-discord"},{"content":"404: Not Found\n","date":"4 January 2022","permalink":"/projects/pgzero/","section":"Projects","summary":"404: Not Found","title":"pgzero"},{"content":" broken-package-yf # An example broken package\nInstall # yfin install jakeroggenbuck/broken-package-yf ","date":"22 December 2021","permalink":"/projects/broken-package-yf/","section":"Projects","summary":"broken-package-yf # An example broken package","title":"broken-package-yf"},{"content":" Diri # DIRectory Info tool that lets you look at a summary of a directory and which projects are using a version control system and if they are connected to a remote control system.\nNew C Version diri-c\nOne could even alias this to ls (Not recommended)\n","date":"21 December 2021","permalink":"/projects/directory-info/","section":"Projects","summary":"Diri # DIRectory Info tool that lets you look at a summary of a directory and which projects are using a version control system and if they are connected to a remote control system.","title":"directory-info"},{"content":" termcolor-yf # Terminal color library for the y-flat language\nInstall # yfin install jakeroggenbuck/termcolor-yf ","date":"21 December 2021","permalink":"/projects/termcolor-yf/","section":"Projects","summary":"termcolor-yf # Terminal color library for the y-flat language","title":"termcolor-yf"},{"content":" yfc # Overview # Compiler for the Y-flat programming language. This is part of the wider Y-flat programming language project, which includes the installer and other components in the future.\nY-flat programming language project Build # ./scripts/build.sh Run # ./cmake/yfc Internal documentation # General documentation\n","date":"20 December 2021","permalink":"/projects/yfc/","section":"Projects","summary":"yfc # Overview # Compiler for the Y-flat programming language.","title":"yfc"},{"content":" Spinners - 🛎 60+ Elegant terminal spinners for Rust # ❤️ Shameless plug # Charts, simple as a URL. No more server-side rendering pain, 1 url = 1 chart Keycloak Identity and Access Management (IAM) as a Service Install # See Cargo page\nUsage # use spinners; use spinners::{Spinner, Spinners}; use std:🧵:sleep; use std::time::Duration; fn main() { let sp = Spinner::new(Spinners::Dots9, \u0026#34;Waiting for 3 seconds\u0026#34;.into()); sleep(Duration::from_secs(3)); sp.stop(); } List of available spinners Documentation Example # cargo run --example cycle cargo run --example simple License # MIT © François-Guillaume Ribreau\n","date":"18 December 2021","permalink":"/projects/spinners/","section":"Projects","summary":"Spinners - 🛎 60+ Elegant terminal spinners for Rust # ❤️ Shameless plug # Charts, simple as a URL.","title":"spinners"},{"content":" yf-package-example # Install # yfin install jakeroggenbuck/yf-package-example ","date":"18 December 2021","permalink":"/projects/yf-package-example/","section":"Projects","summary":" yf-package-example # Install # yfin install jakeroggenbuck/yf-package-example ","title":"yf-package-example"},{"content":" on-logs-research # Exploration on the decimal places of logs from December 2021 - Present\n","date":"3 November 2021","permalink":"/projects/on-logs-research/","section":"Projects","summary":"on-logs-research # Exploration on the decimal places of logs from December 2021 - Present","title":"on-logs-research"},{"content":" Cardinal # Cardinal is a web server that serves data from the frc1678/server project to the frc1678/viewer project.\nUse # There are two main uses for Cardinal. Both involve serving data to the Viewer app. The most important data that is sent is from the frc1678/server project and is current match and competition data. The second type of data is auto generated testing data.\nSetup # Export the value DJANGO_SECRET_KEY before running\nThis key needs to be completely original This key should never be in git You can use this command to generate a key dd if=/dev/urandom bs=60 count=1 | base64 python3 manage.py migrate\nCreate and update the database python3 manage.py migrate python3 manage.py createsuperuser\nCreate the admin account used for making a auth token python3 manage.py createsuperuser Create an auth token\nRun the server Open the website at /admin Log in Create a new token Copy that token into the request Running # Make sure to finish with setup steps Start the server (test) ./scripts/testserver.sh Start the server (production) ./scripts/runserver.sh API Usage # Test Data Generator # # Format curl -X GET \u0026#34;https://cardinal.citruscircuits.org/cardinal/api/generate/\u0026lt;schema_name\u0026gt;/?format=json\u0026#34; # Example curl -X GET \u0026#34;https://cardinal.citruscircuits.org/cardinal/api/generate/calc_tba_team_schema/?format=json\u0026#34; # Example with count curl -X GET \u0026#34;https://cardinal.citruscircuits.org/cardinal/api/generate/calc_tba_team_schema/?format=json\u0026amp;count=10\u0026#34; Output # [ { \u0026#34;auto_high_balls_percent_inner\u0026#34;: 89.7359, \u0026#34;tele_high_balls_percent_inner\u0026#34;: 16.1893, \u0026#34;climb_all_success_avg_time\u0026#34;: 96.7847, \u0026#34;team_name\u0026#34;: \u0026#34;rNvKwQbEoMBnA\u0026#34;, \u0026#34;climb_percent_success\u0026#34;: 46.9206, \u0026#34;climb_all_successes\u0026#34;: 13, \u0026#34;climb_level_successes\u0026#34;: 8, \u0026#34;park_successes\u0026#34;: 16, \u0026#34;auto_line_successes\u0026#34;: 42, \u0026#34;team_number\u0026#34;: 4665 } ] Roadmap # 1. System architecture \u0026amp; framework decision # - Decide what framework we will use - Decide on the architecture (framework dependent) 2. Basic functioning system \u0026amp; initial tests # - Run the system on every developer's computer and troubleshoot - Get automatic testing working with Github actions 3. Automatic test data generator API # - Fully generate data the view can use to test from this service - Serve test data for Viewer to easily pull using REST standards 4. Serve current competition data # - Pull data from Mongodb about the current competition - Setup views to serve this data - Collect time deltas per user of when the last data was pulled - Setup database for time deltas - Create functionality to pull data that has been changed since last pull 5. Discuss specifics of hosting, etc. # - Where will we host this web server note: this isn't as time-sensitive as development 6. Help frontend integrate this API into Viewer. (Currently here) # - Create documentation on the API and it's proper use - Support the Viewer's developers in their implementation efforts 7. Full systems test # - Run the server with test data from scouts - Run this web server and pull data from it - Test Viewer and its ability to pull data ","date":"21 October 2021","permalink":"/projects/cardinal/","section":"Projects","summary":"Cardinal # Cardinal is a web server that serves data from the frc1678/server project to the frc1678/viewer project.","title":"Cardinal"},{"content":" game-2021 # This is the repository for the Davis Senior High School Gamedev game of the 2021-22 school year.\nBuild Instructions # Dependencies # This project wil be done with OpenGL, and as such has the dependencies of OpenGL and GLFW. If you are using macOS, the OpenGL component is not required as it comes with the installation.\nBuild # ./scripts/build.sh\nRun # ./scripts/run.sh\n","date":"8 October 2021","permalink":"/projects/game-2021/","section":"Projects","summary":"game-2021 # This is the repository for the Davis Senior High School Gamedev game of the 2021-22 school year.","title":"game-2021"},{"content":" car-lights # NeoPixel and Arduino code for car underglow.\n","date":"4 October 2021","permalink":"/projects/car-lights/","section":"Projects","summary":"car-lights # NeoPixel and Arduino code for car underglow.","title":"car-lights"},{"content":" nasa-spaceapps-2021 # Project for NASA Spaceapps 2021\n","date":"2 October 2021","permalink":"/projects/nasa-spaceapps-2021/","section":"Projects","summary":"nasa-spaceapps-2021 # Project for NASA Spaceapps 2021","title":"nasa-spaceapps-2021"},{"content":" pytest_assignments # ","date":"18 September 2021","permalink":"/projects/pytest_assignments/","section":"Projects","summary":" pytest_assignments # ","title":"pytest_assignments"},{"content":" pwm-dc-motor # ","date":"15 September 2021","permalink":"/projects/pwm-dc-motor/","section":"Projects","summary":" pwm-dc-motor # ","title":"pwm-dc-motor"},{"content":" project-announcements # Usage # from test.py\nfrom main import Repo if __name__ == \u0026#34;__main__\u0026#34;: repo = Repo(\u0026#34;jakeroggenbuck\u0026#34;, \u0026#34;stow-squid\u0026#34;) announcements = repo.get_announcements() for announcement in announcements: print(announcement) ","date":"11 September 2021","permalink":"/projects/project-announcements/","section":"Projects","summary":"project-announcements # Usage # from test.","title":"project-announcements"},{"content":" Yutimim-dictionary-parser # Usage # # Output to screen python3 main.py -i dict.txt -v # Output to file python3 main.py -i dict.txt -o dict.json Example # Input (Fake words) # auau- 1. te de (v.) eueue- 1. ta da jejj- 1. to da (v.) uueu- 1. to do (v.) Output # [ { \u0026#34;word\u0026#34;: \u0026#34;auau-\u0026#34;, \u0026#34;defs\u0026#34;: [\u0026#34;te de (v.)\u0026#34;] }, { \u0026#34;word\u0026#34;: \u0026#34;eueue-\u0026#34;, \u0026#34;defs\u0026#34;: [\u0026#34;ta da\u0026#34;] }, { \u0026#34;word\u0026#34;: \u0026#34;jejj-\u0026#34;, \u0026#34;defs\u0026#34;: [\u0026#34;to da (v.)\u0026#34;] }, { \u0026#34;word\u0026#34;: \u0026#34;uueu-\u0026#34;, \u0026#34;defs\u0026#34;: [\u0026#34;to do (v.)\u0026#34;] } ] ","date":"11 September 2021","permalink":"/projects/yutimim-dictionary-parser/","section":"Projects","summary":"Yutimim-dictionary-parser # Usage # # Output to screen python3 main.","title":"Yutimim-dictionary-parser"},{"content":" About # magic-identify implements a wrapper around both the python-magic module and the identify module to try and always return results, even if one fails. Finally, failing that it will try to guess some minimal shell script environments too (specifically looking at common keywords in malware droppers/downloaders that do not always have leading identification lines).\nIt returns two strings, one which may be a higher level text description and a second which should always be a mime-type identifier.\nExample module use: # import magicidentify c = magicidentify.MagicIdentify() print(c.identify(\u0026quot;/bin/ls\u0026quot;)) # ('application/x-pie-executable', 'application/x-pie-executable') Example CLI use: # # magic-identify /bin/ls /sbin/ifup /bin/ls: application/x-pie-executable, application/x-pie-executable using magic: application/x-pie-executable, application/x-pie-executable using identify: unknown, unknown /sbin/ifup: inode/symlink, inode/symlink using magic: inode/symlink, inode/symlink using identify: bash/shell, text/x-bash # magic-identify -q /bin/ls /sbin/ifup application/x-pie-executable inode/symlink Todo # Handle more boring cases from some outputs (text/inode) test suite Acknowledgements # The following wonderful github accounts have contributed to the code base:\n@JakeRoggenbuck ","date":"8 September 2021","permalink":"/projects/magic-identify/","section":"Projects","summary":"About # magic-identify implements a wrapper around both the python-magic module and the identify module to try and always return results, even if one fails.","title":"magic-identify"},{"content":" math # A collection of math notebooks that wrote and I used during my senior year of high school to study for tests and remember concepts.\nExample # Here is an example Power Rule.ipynb\n","date":"4 September 2021","permalink":"/projects/math/","section":"Projects","summary":"math # A collection of math notebooks that wrote and I used during my senior year of high school to study for tests and remember concepts.","title":"math"},{"content":" ","date":"2 September 2021","permalink":"/projects/shuzhengz/","section":"Projects","summary":" ","title":"Shuzhengz"},{"content":" Pony # Pony is an open-source, object-oriented, actor-model, capabilities-secure, high-performance programming language.\nStatus # Pony is still pre-1.0 and as such, semi-regularly introduces breaking changes. These changes are usually fairly easy to adapt to. Applications written in Pony are currently used in production environments.\nSupported platforms # Operating Systems # FreeBSD Linux macOS Windows 10 CPUs # Full support for 64-bit platforms x86 and ARM CPUs only Partial support for 32-bit platforms The arm and armhf architectures are tested via CI (Continuous Integration testing) More Information # Installation Building from source Docker images Editor support Resources # Learn more about Pony Start learning Pony Getting help Try Pony online Frequently Asked Questions Community Contributing # We welcome contributions to Pony. Please read through CONTRIBUTING.md for details on how to get started.\nLicense # Pony is distributed under the terms of the 2-Clause BSD License. See LICENSE for details.\n","date":"1 September 2021","permalink":"/projects/ponyc/","section":"Projects","summary":"Pony # Pony is an open-source, object-oriented, actor-model, capabilities-secure, high-performance programming language.","title":"ponyc"},{"content":"404: Not Found\n","date":"31 August 2021","permalink":"/projects/cpython/","section":"Projects","summary":"404: Not Found","title":"cpython"},{"content":" zipfs-law # ranked = sorted(RANK.items(), key=lambda x: x[1], reverse=True) most_used = \u0026#34;\\n\u0026#34; + \u0026#34;\u0026#34;.join([f\u0026#34;- {x}: {y}\\n\u0026#34; for x, y in ranked[:10]]) least_used = \u0026#34;\\n\u0026#34; + \u0026#34;\u0026#34;.join([f\u0026#34;- {x}: {y}\\n\u0026#34; for x, y in ranked[-10:]]) ","date":"31 August 2021","permalink":"/projects/zipfs-law/","section":"Projects","summary":"zipfs-law # ranked = sorted(RANK.","title":"zipfs-law"},{"content":" programming-languages # This is list of programming languages I have made for learning. In each project there were many key takeaways that can be learned from. I will continue to add information and lessons learned from each project. I will also add a description of each project and what it accomplished.\nComponent (2024) [ source] \u0026lt;- Latest # A programming language that compiles to x86-64 assembly for math using postfix notation\nhttps://github.com/JakeRoggenbuck/component Unit Language (2024) [ source] # A small programming language with postfix notation using TypeScript and Deno.\nhttps://github.com/JakeRoggenbuck/unit-language Basis (2023) [ source] # A math console language with a bunch of useful functions and constants\nhttps://github.com/JakeRoggenbuck/basis jai (2021) [ source] # Jai is a programming language\nhttps://github.com/JakeRoggenbuck/jai plrs (2021) [ source] (Lexer library) # The multi-tool of lexical analysis and tokenization. Make parsers in less time for many use cases.\nhttps://github.com/JakeRoggenbuck/plrs f09f (2021) [ source] # pronounced fonf; is a simple statically typed language with simple but powerful syntax\nhttps://github.com/JakeRoggenbuck/f09f https://github.com/JakeRoggenbuck/f09f-rs Description # This one was my first lexer written in C++ and was very fast and robust. This is also a project where I did the first reimplementation of a lexer for the same lang. The reimplementation was in Rust, so it was also my first lexer in Rust.\nmahou (2021) [ source] # Description # Mahou is my AP Computer Science create task for the 2020-21 year. Mahou is a source-to-source compiler, from a custom language to python. Mahou uses a lexer to find all of the tokens in the original source, then uses a parser to convert the source code into python.\nhttps://github.com/JakeRoggenbuck/mahou Paper (2021) [ source] # A simple programming language and its interpreter\nhttps://github.com/JakeRoggenbuck/Paper Planck (2020) [ source] # https://github.com/JakeRoggenbuck/pinter ice (2020) [ source] # https://github.com/JakeRoggenbuck/ice snow_script (2020) [ source] # A simple and high/low macro lang, similar to pogscript with way nicer syntax\nhttps://github.com/JakeRoggenbuck/snow_script calcLex (2020) [ source] # A test of lexing and yacc in python\nhttps://github.com/JakeRoggenbuck/calcLex cruz-lang (2020) [ source] # Cruz Lang is a simple explicit static typed language.\nhttps://github.com/JakeRoggenbuck/cruz-lang pogscript (2020) [ source] # A simple macro language\nhttps://github.com/JakeRoggenbuck/pogscript ","date":"28 August 2021","permalink":"/projects/programming-languages/","section":"Projects","summary":"programming-languages # This is list of programming languages I have made for learning.","title":"programming-languages"},{"content":" lesson-tester # Test python code in the browser. Wrap code with specific lesson tests for code completeness grading.\n","date":"27 August 2021","permalink":"/projects/lesson-tester/","section":"Projects","summary":"lesson-tester # Test python code in the browser.","title":"lesson-tester"},{"content":" 2021-software-general-homework # Assignments that crash immediately when run cannot be made up. All assignments will be due by the next lesson. Late assignments will be graded on a curve of 85% maximum. If you won\u0026rsquo;t be able to complete an assignment on time, slack Kate, Ludi, and/or Emily. We will not be telling you your exact grades. Instead, you will be given your grade on a \u0026ldquo;fail/needs work/pass\u0026rdquo; scale. If you have questions regarding grades, slack the graders in a DM. It is mandatory to redo homework until you receive a \u0026ldquo;pass\u0026rdquo;. You will be allowed to redo assignments for full credit only if rules 1 and 3 do not apply. You must ask or answer at least 2 pertinent questions during a lesson for full points on that day. All non-personal questions must be asked in #2_software_general or during worktime meetings. If you will not be able to attend a meeting, slack Ludi, Kate, and Emily least 24 hours prior to the meeting. 1. Functionality # Have you tested your code, including edge cases? Does your code accomplish the assigned task? 2. Concepts # Does your code use only concepts previously taught by us? Does your code use concepts relevant to the assignment? 3. Style # Does your code meet our style guidelines? Double quotes Proper indentation Appropriate usage of snake**case and CapWords Comment formatting Naming conventions (may be given in assignment) ","date":"23 August 2021","permalink":"/projects/2021-software-general-homework/","section":"Projects","summary":"2021-software-general-homework # Assignments that crash immediately when run cannot be made up.","title":"2021-software-general-homework"},{"content":" Vyxal - Terse, Elegant and Readable # Vyxal is the latest addition to the plethora of stack-based languages used for code golfing. But unlike its competitors, Vyxal has a special focus upon readability and elegancy. Indeed, the codepage has been specially chosen to be as mnemonic as possible. Further, constructs from practical languages (such as functions, lambdas and easy list manipulation) are present.\n(Vyxal is pronounced Vikesal)\nHow to use the interpreter: # python3 Vyxal.py \u0026lt;file\u0026gt; \u0026lt;flags (single string of flags)\u0026gt; \u0026lt;input(s)\u0026gt;\nFor a list of command-line flags: python3 Vyxal.py h\nData Types # There are 5 data types supported by Vyxal:\nNumbers (integers and reals/floats) Strings Lists Generators Functions Basic Operators # +-*/% perform addition, subtraction, multiplication, division and modulo respectively. , prints the top of the stack :_ duplicates the top of the stack and pops the top of the stack respectively ! pushes the length of the stack Syntax Constructs # If statements # [truthy_branch|falsey_branch] [truthy_branch] The if statement pops the top of the stack, and if it is truthy, executes the truthy branch. Otherwise, if a falsey branch is present, it will branch to execute that.\nFor loop # (variable|body) (body) The for loop pops the top of the stack and iterates through each item. If the value popped is an integer, it loops through the range [0, n). If variable is present, the iteration value is stored in that. Otherwise, the iteration value is stored in the context variable n.\nWhile loop # {condition|body} {body} The while loop repeats body until condition evaluates as true. If there is no explicit condition, 1 is used as the condition, meaning that {...} is an infinite loop.\nFunctions # @name|code; @name:number_of_arguments|code; @name:variable|code; @name:argument_list|code; @name; If code isn\u0026rsquo;t present, the function with name name is called. Otherwise, the function is defined. The arguments can be a combination of variables and numbers. Numbers tell the function how many items to pop from the main stack as arguments, and variables store a single value in the variable. Numbered arguments are pushed to the function\u0026rsquo;s stack \u0026ndash; functions operate on their own scoped stack with scoped variables (much like Python).\nFor example:\n@triple:1|3*; Takes 1 parameter and pushes it to the function\u0026rsquo;s stack\n@triple:value|←value 3*; Takes a single argument and stores it in variable value.\n@add_and_halve:1:rhs|←rhs +2/; Takes two arguments: pushes the first on to the stack and stores the second in variable rhs\n@average:*|W:L$∑$/; Takes however many arguments as defined by the first value popped from the stack. A function call of 2 3 3 3 @average; would take three arguments.\nLambdas # λarity|code; λcode; Where the @...; function stores the definition for infinite re-use, the lambda pushes a reference to the code inside it. This is similar to python\u0026rsquo;s lambdas, which are temporary functions, or literal functions (for lack of better word).\nThese can be applied using †. For example: 3 λ3*; † will result in 9. Lambdas are also useful for mapping/filtering/reducing a vector according to the lambda\u0026rsquo;s code.\nImplicit input and output # At the end of program execution (eof), if nothing has been printed (using , or other printing commands), the top value on the stack is automatically printed. If there isn\u0026rsquo;t enough values on the stack to perform an operation, implicit input is taken. If input is passed through command line arguments, then the input used is cycled. Input can be either through arguments or STDIN. STDIN is used if arguments aren\u0026rsquo;t avaliable. If no input is avaliable at all, 0 is returned. In functions (and lambdas), if implicit input is needed, the argument(s) passed are used as the input \u0026ldquo;list\u0026rdquo;. Commands # Vyxal has so many commands that it is impractical to list them all here. Here is the reference page ( markdown).\nExamples # Hello, World! # `Hello, World!` Try it Online!\n`ƈṡ, ƛ€! Try it Online!\nThe above program uses dictionary compression: words in a predefined list are indexed using a subjective base-162 literal.\nkH Try it Online!\nFizzbuzz # ₁ƛ₍₃₅kF½*∑⟇ Try it Online! Explanation\nPrime Checking # æ Try it Online!\nKL2= Try it Online!\nLinks # Repository Online Interpreter Tutorial Codepage Chat Room (SE Chat) Very Special Contributors (alphabetically sorted) # code-golf se user @2x-1 for helping me establish the fundamentals of Vyxal and being my first collaborator on this journey. @8dion8 for language suggestions and motivation in the MAWP discord group. @Amiller42 for pointing out bugs and fixing bugfixes @chunkybanana for also bug fixing and making PRs @detectivewasif for adding detail to the reference sheet. @Razetime for helping me with the online interpreter\u0026rsquo;s design @ysthakur for making the reference.md file and making an automated process to do so. ","date":"23 August 2021","permalink":"/projects/vyxal/","section":"Projects","summary":"Vyxal - Terse, Elegant and Readable # Vyxal is the latest addition to the plethora of stack-based languages used for code golfing.","title":"Vyxal"},{"content":" Jai # Our entry to the langjam\nRequirements: # Rust \u0026amp; Python\nPip requirements: run\npip install -r requirements.txt Build instructions # Build the lexer crate with\nmaturin build Install lexer with\npip install ./target/wheels/jai-0.1.1-* # Add a `--force-reinstall` if reinstalling Troubleshooting # If you get something saying maturin command not found after you install it via pip. Try using python3 -m pip to install it and python3 -m maturin to run it. If jai does not seem to change after editing the source, make sure to do the build instructions again and use --force-reinstall Test # Run build instructions run pytest Syntax # Variable # variable: type; variable: type = value; Functions # fn myfunc(num: int) -\u0026gt; int { return num + 10; } Types # Name details int an integer str a string ","date":"20 August 2021","permalink":"/projects/jai/","section":"Projects","summary":"Jai # Our entry to the langjam","title":"jai"},{"content":" Jam #1 # Welcome to the first langjam! We\u0026rsquo;re excited to get started.\nTheme # The theme of the jam is \u0026ldquo;first-class comments\u0026rdquo;.\nGetting started # To participate:\nfork this repo make a copy of the TEMPLATE directory rename the copy to your team name (for example, if your team is \u0026ldquo;foobazers\u0026rdquo; rename TEMPLATE to foobazers) update the TEAM file in the directory with the name of your team members. Team members should be listed by their GitHub username. The first username on the list of team members is the team captain (see below) send a PR to add your team to the repo Team names are \u0026ldquo;first come, first served\u0026rdquo; so if someone else takes your team name you\u0026rsquo;ll need to come up with a new one.\nOnce your team has a directory, you can submit your project.\nTeam names # Please keep your team names \u0026ldquo;family-friendly\u0026rdquo;. Sole discretion for what counts as \u0026ldquo;family friendly\u0026rdquo; is left up to JT, but if the name is safe/kid-friendly you should be fine.\nOne-person teams # If you are a single-person team, feel free to make up a team name or use your GitHub username as the team name, if it\u0026rsquo;s available.\nSubmitting your project # When you submit your project, send a PR that updates only your team\u0026rsquo;s directory and nothing else.\nIn the PR, submit:\nyour project\u0026rsquo;s source a README.md that describes how to build and use the project sample files written in your language Team captains # The team captains will be the ones responsible for sending the PR to submit your project.\nHelp, I\u0026rsquo;ve never used GitHub # You can read through the helpful hello world from GitHub. If you\u0026rsquo;ve never forked a repo, you can check out the GitHub forking tutorial.\nTimeline # The langjam begins at 7pm UK time 20th of August and runs for 48 hours. At 7pm UK time 22nd of August, we\u0026rsquo;ll end submissions for the jam. Make sure you have submitted your PR before this cut-off.\nJudging # Judging will take place after the jam has ended. Once judging has concluded, we\u0026rsquo;ll pick a set of winning projects that will become part of a video on the Systems with JT YouTube channel.\nYou\u0026rsquo;re welcome to try out other projects. We\u0026rsquo;ll have a way to leave comments so you can share your thoughts and vote on which you liked best, too.\nOther rules # General information about the langjam is available in the main langjam README.\nSocial media # If you like, you can tag your social media posts with #langjam2021 to help others find you.\n","date":"20 August 2021","permalink":"/projects/jam0001/","section":"Projects","summary":"Jam #1 # Welcome to the first langjam!","title":"jam0001"},{"content":" 🦑 stow-squid 0.1.2 # Stow your dotfiles\nInstall # git clone https://github.com/JakeRoggenbuck/stow-squid.git cd stow-squid cargo install --path . # Add config to ~/.config/stow-squid/stow-squid.toml mkdir -p ~/.config/stow-squid/ \u0026amp;\u0026amp; cp example-config.toml ~/.config/stow-squid/stow-squid.toml Verbs # For all verbs, the name is of a specific dotfile and is optional. Including a name will only run the verb on that dotfile. Without a name, it will run the verb on all the dots in the config.\nSave # This is to update your dotfiles (that are scattered around your machine) to your git repo\nstow-squid save \u0026lt;name\u0026gt; Deploy # This is to place all your dotfiles from your git repo to all the various places they might go\nstow-squid deploy \u0026lt;name\u0026gt; List # stow-squid list Config # # Git path gitpath = \u0026#34;/path/to/git/dir\u0026#34; # Structure # [[files]] # name = \u0026#34;dotfile-name\u0026#34; # origin = \u0026#34;path/to/file/from/git/repo\u0026#34; # deployed = \u0026#34;/path/to/where/the/file/is/placed\u0026#34; # Example # [[files]] # name = \u0026#34;bspwm\u0026#34; # origin = \u0026#34;/home/jake/Build/dotfiles/bspwm/bspwmrc\u0026#34; # deployed = \u0026#34;/home/jake/.config/bspwm/bspwmrc\u0026#34; # Add you first dotfile here [[files]] name = \u0026#34;\u0026#34; origin = \u0026#34;\u0026#34; deployed = \u0026#34;\u0026#34; Config Example # gitpath = \u0026#34;/home/jake/Build/dotfiles/\u0026#34; [[files]] name = \u0026#34;bspwm\u0026#34; origin = \u0026#34;/home/jake/Build/dotfiles/bspwm/bspwmrc\u0026#34; deployed = \u0026#34;/home/jake/.config/bspwm/bspwmrc\u0026#34; [[files]] name = \u0026#34;alacritty\u0026#34; origin = \u0026#34;/home/jake/Build/dotfiles/alacritty/alacritty.yml\u0026#34; deployed = \u0026#34;/home/jake/.config/alacritty/alacritty.yml\u0026#34; [[files]] name = \u0026#34;bashrc\u0026#34; origin = \u0026#34;/home/jake/Build/dotfiles/.bashrc\u0026#34; deployed = \u0026#34;/home/jake/.bashrc\u0026#34; [[files]] name = \u0026#34;sxhkd\u0026#34; origin = \u0026#34;/home/jake/Build/dotfiles/sxhkd/sxhkdrc\u0026#34; deployed = \u0026#34;/home/jake/.config/sxhkd/sxhkdrc\u0026#34; Help # USAGE: drop \u0026lt;verb\u0026gt; [dot] FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: \u0026lt;verb\u0026gt; \u0026lt;dot\u0026gt; ","date":"13 August 2021","permalink":"/projects/stow-squid/","section":"Projects","summary":"🦑 stow-squid 0.","title":"stow-squid"},{"content":"404: Not Found\n","date":"8 August 2021","permalink":"/projects/dragon/","section":"Projects","summary":"404: Not Found","title":"dragon"},{"content":" plrs # The multi-tool of lexical analysis and tokenization.\nInstall # pip install plrs Build # maturin build Examples # zipfs-law API # Global variables # EOF_TOKEN Classes # Tokens Settings Token - part - token - set_part - set_token - __str__ - __repr__ Lexer - new - char_forward - skip_over_char_set - next Functions # is_char_symbol is_char_operator is_char_whitespace is_char_numeric is_single_quote is_double_quote ends_token is_part_numeric tokenize ","date":"29 July 2021","permalink":"/projects/plrs/","section":"Projects","summary":"plrs # The multi-tool of lexical analysis and tokenization.","title":"plrs"},{"content":" Cutil # Cutil is a rewrite of the gnu core utils, also in C. Written by Adam Hutchings and Jake Roggenbuck.\nList # cat.c ls.c nl.c yes.c head.c help.c clear.c ","date":"20 July 2021","permalink":"/projects/cutil/","section":"Projects","summary":"Cutil # Cutil is a rewrite of the gnu core utils, also in C.","title":"cutil"},{"content":" pylex_lite # The test for plrs\n","date":"18 July 2021","permalink":"/projects/pylexlite/","section":"Projects","summary":"pylex_lite # The test for plrs","title":"pylexlite"},{"content":" least-squares-regression # View least-squares.ipynb # ","date":"17 July 2021","permalink":"/projects/least-squares-regression/","section":"Projects","summary":"least-squares-regression # View least-squares.","title":"least-squares-regression"},{"content":" colorgradient-julia # Python version colorgradient Rust version colorgradient-rs Julia version colorgradient-julia C version colorgradient-c Go version colorgradient-go Clojure version colorgradient-clj # Find the slope of two point find_slope(x1, y1, x2, y2) = (y2 - y1) / (x2 - x1) # Find the closest whole numbers on both sides of a x neighbors(x) = round(x), ceil(x) # Find the predicted value of y given a value x function find_y(x, known_x) # Check if the given value is exactly one in the known list if round(x) == x return known_x[x] end left_x, right_x = neighbors(x) left_y = known_x[left_x] right_y = known_x[right_x] slope = find_slope(left_x, left_y, right_x, right_y) return left_y + (slope * (x - left_x)) end ","date":"14 July 2021","permalink":"/projects/colorgradient-julia/","section":"Projects","summary":" colorgradient-julia # Python version colorgradient Rust version colorgradient-rs Julia version colorgradient-julia C version colorgradient-c Go version colorgradient-go Clojure version colorgradient-clj # Find the slope of two point find_slope(x1, y1, x2, y2) = (y2 - y1) / (x2 - x1) # Find the closest whole numbers on both sides of a x neighbors(x) = round(x), ceil(x) # Find the predicted value of y given a value x function find_y(x, known_x) # Check if the given value is exactly one in the known list if round(x) == x return known_x[x] end left_x, right_x = neighbors(x) left_y = known_x[left_x] right_y = known_x[right_x] slope = find_slope(left_x, left_y, right_x, right_y) return left_y + (slope * (x - left_x)) end ","title":"colorgradient-julia"},{"content":"","date":"14 July 2021","permalink":"/tags/julia/","section":"Tags","summary":"","title":"Julia"},{"content":" ACS Upstream - autoclockspeed.org - Our crates.io - ACS Github Org # A utility to check stats about your CPU, and auto regulate clock speeds to help with either performance or battery life. This program is designed for Linux and Intel laptops, although it should theoretically work on AMD systems and sometimes desktops as well. If you encounter any issues or bugs, please refer to the wiki to see if there is a solution.\nQuickstart # cargo install autoclockspeed Now you can run acs monit.\nView #install-latest-release for more info.\nGoals # First and foremost, this is a project to learn about Rust and Linux Secondly, try to improve upon AdnanHodzic\u0026rsquo;s already amazing auto-cpufreq Add options to display raw output of governors, clockspeed, turbo, battery, etc. for use in scripts or display panels like polybar. Want to help? Yay! Welcome! # Read our CONTRIBUTING.md for some helpful tips Find an issue - \u0026ldquo;good first issue\u0026rdquo; recommended Feel free to ask questions! Install Latest Release # If you have cargo on your machine, skip to step 3\nGo to rustup.rs to install rust.\nSetup rust\nrustup override set stable rustup update stable Clone the project and install\ngit clone https://github.com/JakeRoggenbuck/auto-clock-speed cargo install --path auto-clock-speed # This is needed to have the root version of acs match the local installed version sudo cp ~/.cargo/bin/acs /usr/bin/acs Note: If you receive error linker 'cc' not found, then you need to install a C compiler (gcc, cmake, etc.) first.\nNote: The latest release of acs can also be installed locally with the following\ncargo install autoclockspeed Tested Devices # Auto clock speed has been tested to work on the following devices. If you have a device that is not listed please submit a pull request.\nFunctionality Description Working All parts of ACS are fully functional, the computer has enough data to make decisions on governor changes and can be run in edit mode Mostly Working ACS is unable to understand some data from the computer however certain data (like battery life, battery condition, temperature etc) which is non essential in making governor decisions, is missing Partially Working ACS is able to mostly work, although with one or more significant caveat (i.e. jailbreak on Apple devices or root access on Android) Barely Working ACS is unable to be run in edit mode due to missing data from the system, monit mode may still work however functionality is limited. If you have a system that falls under this category please open an issue Borked ACS cannot find any useful data. Please open an issue Device Name Functionality Notes Dell XPS 13 9360 Working Dell Latitude 7480 Working Steam Deck Working Edit mode not necessary (use built in governor switcher) Thinkpad T400 Working Thinkpad T460 Working Thinkpad X230 Working Thinkpad X301 Working Thinkpad W540 Working ThinkPad X1 Carbon Gen 1 Working ThinkPad X1 Extreme Gen 1 Working Thinkpad P1 Gen 4 (Intel Core) Working Thinkpad P14 Gen 2 (AMD) Mostly Working See #443 OnePlus 9 Pro (Snapdragon 888 SoC) Partially Working Needs root access; compile from source through termux iPad Pro Gen 6 Borked Compiles with iSH, but cannot access any data In Action # New Interactive Mode # Systemd # In order to have auto-clock-speed start when you restart your computer you must follow these instructions\n# IMPORTANT: Modify the service file (acs.service) in the # project directory to include the path to the binary file # (usually /home/username/.cargo/bin/acs) # In the auto clock speed directory run this command to # move the service file into your systemd directory sudo cp acs.service /etc/systemd/system/ # Start and enable the service sudo systemctl start acs sudo systemctl enable acs # Check service is up and running systemctl status acs Systemctl command # The line after [Service] in acs.service is the command that will be run. You may want to add or remove arguments, mainly --quiet.\n[Unit] Description=Manages Clock Speed [Service] ExecStart=/home/your-user-here/.cargo/bin/acs run --no-animation --quiet [Install] WantedBy=multi-user.target Config # Using default config # WARN: Using default config. Create file \u0026#39;/etc/acs/acs.toml\u0026#39; for custom config or run \u0026#39;acs initconfig\u0026#39; to setup default config automatically. This warning recommends creating a config file, use the initconfig command to automatically create one for you!\nsudo acs initconfig This is an example config # also the default settings if no config is provided\n# acs.toml powersave_under = 20 overheat_threshold = 80 active_rules = [ \u0026#34;battery_percent_rule\u0026#34;, \u0026#34;lid_open_rule\u0026#34;, \u0026#34;ac_charging_rule\u0026#34;, \u0026#34;cpu_usage_rule\u0026#34; ] Turn Off # If you would like to turn off auto-clock-speed, here are the steps.\nNote: This should be done during testing of acs run mode.\n# Temporarily stop (only lasts until reboot) sudo systemctl stop acs # Permanently stop until turned on sudo systemctl disable acs Uninstall # Here is how to uninstall the binary and the systemctl service.\n# Remove local binary cargo uninstall acs # Remove system shared binary rm /usr/bin/acs # Remove systemctl entry rm /etc/systemd/system/acs.service Example Usage # Here are some examples of how acs can be used.\n# Monitor mode acs monitor # Run as root sudo acs run # Get all speeds acs get speeds # Select gov from dmenu sudo acs set gov $(acs get available-govs --raw | dmenu) Detailed usage # Detailed usage can be found on our wiki\nHelp # Automatic CPU frequency scaler and power saver USAGE: acs \u0026lt;SUBCOMMAND\u0026gt; FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: daemon Controls interaction with a running daemon get Get a specific value or status help Prints this message or the help of the given subcommand(s) initconfig Initialize config interactive Interactive mode for auto clock speed commands monitor Monitor each cpu, it\u0026#39;s min, max, and current speed, along with the governor run Run the daemon, this checks and edit your cpu\u0026#39;s speed set Set a specific value showconfig Show the current config in use ","date":"7 July 2021","permalink":"/projects/auto-clock-speed/","section":"Projects","summary":"ACS Upstream - autoclockspeed.","title":"auto-clock-speed"},{"content":" This is the repository for the QME source code and assets.\nQME # Keybinds # w, a, s, and d for movement i zooms the camera in o zooms the camera out Requirements # The project is built using the latest version of open Java Development Pack java-latest-openjdk-devel\nThe current version the program supports Java version 15\nMaximum backwards compatibility with JDK version 11\nNote: If you are using MacOS and do not have a working Maven installation, please run the script ./scripts/classpath_manager.py and select the ./osx_classpath\n","date":"5 July 2021","permalink":"/projects/qme/","section":"Projects","summary":"This is the repository for the QME source code and assets.","title":"qme"},{"content":" cproc # A small C preprocessor, written in C.\n#define A 4 // New code int main() { int main() { int x = A; -\u0026gt; int x = 4; } } build # ./scripts/build.sh run # ./build/cproc ","date":"4 July 2021","permalink":"/projects/cproc/","section":"Projects","summary":"cproc # A small C preprocessor, written in C.","title":"cproc"},{"content":" Clifford Attractors # ","date":"30 June 2021","permalink":"/projects/clifford/","section":"Projects","summary":" Clifford Attractors # ","title":"Clifford"},{"content":"","date":"30 June 2021","permalink":"/tags/html/","section":"Tags","summary":"","title":"HTML"},{"content":" my-readme-image # ","date":"30 June 2021","permalink":"/projects/my-readme-image/","section":"Projects","summary":" my-readme-image # ","title":"my-readme-image"},{"content":" novus-chart # A modern looking, accessible medical chart application based on a MERN stack.\n","date":"30 June 2021","permalink":"/projects/novus-chart/","section":"Projects","summary":"novus-chart # A modern looking, accessible medical chart application based on a MERN stack.","title":"novus-chart"},{"content":" sorting-algorithms # This repository contains a variety of sorting algorithms written in several programming languages. Currently, it contains the selection and insertion sort algorithms implemented using Java and Python. More languages and algorithms will be added in the future.\n","date":"29 June 2021","permalink":"/projects/sorting-algorithms/","section":"Projects","summary":"sorting-algorithms # This repository contains a variety of sorting algorithms written in several programming languages.","title":"sorting-algorithms"},{"content":" flowerknight # Overview # This is a game we are making over the summer as a demo for our club. In the game, you are a knight who needs to protect and transport a flower.\nDetails # We are using Java 16 and OpenGL\nContributing # All PRs must have two reviews before merge, and changes to the code (things in src) must have a corresponding issue attached. All PRs must have one test at least, PRs are subject to added review specs, for example, \u0026ldquo;test on mac\u0026rdquo; or \u0026ldquo;test on all platforms\u0026rdquo;. You cannot mark as reviewed, or mark as tested if you have worked on a specific PR. Whoever you are encurraged to review PRs you added to, and are expected to test.\nIt\u0026rsquo;s always welcome to make new issues, and review PRs. We will assign issues to people, but you can also ask on the issue if you want to be assigned that issue.\nRequirements # The project is built using the latest version of Open Java Development Kit (Open JDK) java-latest-openjdk-devel\nThe current version the program supports Java version 16\nMaximum backwards compatibility with JDK version 11\n","date":"1 June 2021","permalink":"/projects/flowerknight/","section":"Projects","summary":"flowerknight # Overview # This is a game we are making over the summer as a demo for our club.","title":"flowerknight"},{"content":" link # Description # A public mirror of the private development link repository, without the .env and Rocket.toml hosted on heroku\nSetup # .env needs the following\nDATABASE_URL=url or uri to database AUTH_KEY=an auth key Rocket.toml\n# Database [global.databases.redirect-api] url = \u0026#34;postgres://\u0026lt;user\u0026gt;:\u0026lt;password\u0026gt;@localhost/redirect-api\u0026#34; ","date":"26 May 2021","permalink":"/projects/link/","section":"Projects","summary":"link # Description # A public mirror of the private development link repository, without the .","title":"link"},{"content":" mahou # Mahou is my AP Computer Science create task for the 2020-21 year. Mahou is a source-to-source compiler, from a custom language to python.\nMahou uses a lexer to find all of the tokens in the original source, then uses a parser to convert the source code into python.\nFirst language # set a = 0; a += 1; print a; Tokens parsed # Type Part Line ---------------------------- Set set 1:1 Identifier a 1:5 Assign = 1:7 Numeric 0 1:9 Semi ; 1:10 Identifier a 2:12 Plus + 2:14 Assign = 2:15 Numeric 1 2:17 Semi ; 2:18 Print print 3:20 Identifier a 3:26 Semi ; 3:27 Outputted python # a = 0 a+=1 print(a) ","date":"20 May 2021","permalink":"/projects/mahou/","section":"Projects","summary":"mahou # Mahou is my AP Computer Science create task for the 2020-21 year.","title":"mahou"},{"content":" Rock # A simple command line utility for manipulating standard in and out, similar to awk\nInstall # git clone https://github.com/JakeRoggenbuck/rock cargo install --path rock Usage # short long description example -r --replace replace a certain string with another rock --replace \u0026quot;~\u0026quot; \u0026quot;/home/jake\u0026quot; -s --split split into multiple lines after a character rock --split , Examples # echo $PATH | rock -s : | rock -r \u0026#34;/home/jake\u0026#34; \u0026#34;~\u0026#34; echo \u0026#34;~/Downloads,~/Documents,~/Repos/rock\u0026#34; | rock --replace \u0026#34;~\u0026#34; \u0026#34;/home/jake\u0026#34; echo \u0026#34;~/Downloads,~/Documents,~/Repos/rock\u0026#34; | rock --split , | rock --replace \u0026#34;~\u0026#34; \u0026#34;/home/jake\u0026#34; TODO Feature # -f --filter will filter out something -b --block will completely block out something -o --only will only show something -p --prepend something to the begining -a --append something the end ","date":"17 May 2021","permalink":"/projects/rock/","section":"Projects","summary":"Rock # A simple command line utility for manipulating standard in and out, similar to awk","title":"rock"},{"content":" mongoc-rs # Quickly view your mongodb databases, their collections, and their documents from the command line\nInstalling # git clone https://github.com/JakeRoggenbuck/mongoc-rs cargo install --path mongoc-rs ","date":"9 May 2021","permalink":"/projects/mongoc-rs/","section":"Projects","summary":"mongoc-rs # Quickly view your mongodb databases, their collections, and their documents from the command line","title":"mongoc-rs"},{"content":" Pathfinder # A simple utility to view, edit, and search your $PATH\nUsage # Shorthand Flag Description h --help Display this page and exit v --version Display the version and exit l --list List all locations in $PATH (same as find with no keyword) f --find Find locations in $PATH including search keyword a --add Add location to the $PATH, and open a new bash prompt p --purge Unsets everything in $PATH, it will be empty n --number List locations in $PATH with numbers on side Install # git clone https://github.com/JakeRoggenbuck/pathfinder cargo install --path pathfinder ","date":"6 May 2021","permalink":"/projects/pathfinder/","section":"Projects","summary":"Pathfinder # A simple utility to view, edit, and search your $PATH","title":"pathfinder"},{"content":" hexviewer-rs # Hexviewer allows you to quickly view the hex representation of a file with vim style keybinds\nInstall # git clone https://github.com/JakeRoggenbuck/hexviewer-rs cargo install --path hexviewer-rs ","date":"4 May 2021","permalink":"/projects/hexviewer-rs/","section":"Projects","summary":"hexviewer-rs # Hexviewer allows you to quickly view the hex representation of a file with vim style keybinds","title":"hexviewer-rs"},{"content":" colorgradient-rs # Python version colorgradient Rust version colorgradient-rs Julia version colorgradient-julia C version colorgradient-c Go version colorgradient-go Clojure version colorgradient-clj Install # git clone https://github.com/JakeRoggenbuck/colorgradient-rs cargo install --path colorgradient-rs Test # cargo test Code # fn calculate_gradient(num: i64, original_colors: Vec\u0026lt;RGB\u0026gt;) -\u0026gt; Vec\u0026lt;RGB\u0026gt; { // Get the needed step value to fit the num of iterations in the original_colors length let step: f32 = (original_colors.len() as f32 - 1.0) / num as f32; let channels = get_channels(original_colors); let mut colors = Vec::\u0026lt;RGB\u0026gt;::new(); for i in 0..num { // Use step to count up with index let x: f32 = i as f32 * step; let mut color = Vec::new(); // Get each channel as a reference, use it as the known x values for channel in [\u0026amp;channels.red, \u0026amp;channels.green, \u0026amp;channels.blue].iter() { // Add the y values found from the x value to the color color.push(find_y(x as f32, channel).abs()); } // Change the vector of colors to RGB structure, and add it to all the colors colors.push(vec_to_rgb!(color)); } return colors; } ","date":"1 May 2021","permalink":"/projects/colorgradient-rs/","section":"Projects","summary":"colorgradient-rs # Python version colorgradient Rust version colorgradient-rs Julia version colorgradient-julia C version colorgradient-c Go version colorgradient-go Clojure version colorgradient-clj Install # git clone https://github.","title":"colorgradient-rs"},{"content":" rocket-[not]-pinball # A game that started as pinball, but really isn\u0026rsquo;t, more fun\nSetup # pip3 install -r requirements.txt Running # # Start the HTTP server python3 app.py # Start the score REST API uvicorn app:APP --reload --port 8081 Current progress # Here are the decisions that have been made about the pinball game. The game won\u0026rsquo;t be the standard pinball game. It will have the mechanic of hitting the ball, but the objective is to have it travel upwards without hitting obstacles. It will be space themed, and the player is trying to send the ball to space, avoiding planets, meteors, and other space debris. It will be made using p5.js and a nodejs backend to host it. Basic game mechanics have been made, like ball and obstacle movement, along with obstacle creation and destruction.\n","date":"27 April 2021","permalink":"/projects/rocket-pinball/","section":"Projects","summary":"rocket-[not]-pinball # A game that started as pinball, but really isn\u0026rsquo;t, more fun","title":"rocket-pinball"},{"content":" f09f-rs # pronounced fonf; is a statically typed language with simple but powerful syntax, and an interpreter written in rust\nInstall # git clone https://github.com/JakeRoggenbuck/f09f-rs cargo install --path f09f-rs Usage # Shorthand Flag Description -v Verbose output of tokens Syntax # Vars # int fact = 0; prec dec = 0.2; string this = \u0026#34;a string\u0026#34;; bool is_it = true; Functions # fun square(int n) returns int { ~ Return the square of n ~ return n ^ n; } Control flow # if (n == 0) { n = 1; } if (this and not that) { x = 1; } if (this or that) { x = 1; } if (this) { that = false; } else { that = true; } ","date":"22 April 2021","permalink":"/projects/f09f-rs/","section":"Projects","summary":"f09f-rs # pronounced fonf; is a statically typed language with simple but powerful syntax, and an interpreter written in rust","title":"f09f-rs"},{"content":" Argos Translate # Docs | Website | Video intro\nOpen-source offline translation library written in Python. Uses OpenNMT for translations, SentencePiece for tokenization, Stanza for sentence boundary detection, and PyQt for GUI. Designed to be used as either a Python library, command-line, or GUI application. LibreTranslate is an API and web-app built on top of Argos Translate.\nArgos Translate supports installing model files which are a zip archive with an \u0026ldquo;.argosmodel\u0026rdquo; extension that contains an OpenNMT CTranslate2 model, a SentencePiece tokenization model, a Stanza tokenizer model for sentence boundary detection, and metadata about the model. Pretrained models can be downloaded here.\nArgos Translate also manages automatically pivoting through intermediate languages to translate between languages that don\u0026rsquo;t have a direct translation between them installed. For example, if you have a es ➔ en and en ➔ fr translation installed you are able to translate from es ➔ fr as if you had that translation installed. This allows for translating between a wide variety of languages at the cost of some loss of translation quality.\nModels # Browse models P2P download (IPFS and BitTorrent) Training script Google Drive download Supported languages # Arabic Chinese English French German Hindi Italian Japanese Polish Portuguese Turkish Russian Spanish Examples # GUI # Python # \u0026gt;\u0026gt;\u0026gt; from argostranslate import package, translate \u0026gt;\u0026gt;\u0026gt; package.install_from_path(\u0026#39;en_es.argosmodel\u0026#39;) \u0026gt;\u0026gt;\u0026gt; installed_languages = translate.get_installed_languages() \u0026gt;\u0026gt;\u0026gt; [str(lang) for lang in installed_languages] [\u0026#39;English\u0026#39;, \u0026#39;Spanish\u0026#39;] \u0026gt;\u0026gt;\u0026gt; translation_en_es = installed_languages[0].get_translation(installed_languages[1]) \u0026gt;\u0026gt;\u0026gt; translation_en_es.translate(\u0026#34;Hello World!\u0026#34;) \u0026#39;¡Hola Mundo!\u0026#39; Command Line # $ argos-translate --from-lang en --to-lang es \u0026#34;Hello World\u0026#34; Hola Mundo $ echo \u0026#34;Text to translate\u0026#34; | argos-translate --from-lang en --to-lang es Texto para traducir LibreTranslate Web App ( Demo) # LibreTranslate API # const res = await fetch(\u0026#34;https://libretranslate.com/translate\u0026#34;, { method: \u0026#34;POST\u0026#34;, body: JSON.stringify({ q: \u0026#34;Hello!\u0026#34;, source: \u0026#34;en\u0026#34;, target: \u0026#34;es\u0026#34; }), headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} }); console.log(await res.json()); { \u0026#34;translatedText\u0026#34;: \u0026#34;¡Hola!\u0026#34; } Installation # Install from PyPI # Argos Translate is available from PyPI and can be installed with pip.\npython3 -m pip install --upgrade pip python3 -m pip install argostranslate Installation for Windows # CTranslate2, the inference engine for Argos Translate, currently only distributes binaries for Linux and MacOS so to install Argos Translate on Windows you will need to build CTranslate2 from source.\nInstall from Snap Store # Argos Translate is available from the Snap Store and auto installs a content snap to support translation between Arabic, Chinese, English, French, Russian, and Spanish. Additional languages can be installed from supplementary content snaps.\nWith snapd installed:\nsudo snap install argos-translate Automatically installs and connects to argos-translate-base-langs snap to support translations between Arabic, Chinese, English, French, Russian, and Spanish.\nAdditional languages can be installed from *.argosmodel files or from supplementary content snaps:\nargos-translate-de-en - German - English argos-translate-en-it - English - Italian argos-translate-en-pt - English - Portuguese To connect automatically: sudo snap connect argos-translate:argos-packages argos-translate-en-it:argos-packages\nTo run command line interface on Snapcraft:\nargos-translate.cli --help Python source installation # Dependencies # Requires Python3, pip (which should come with Python3), and optionally virtualenv to keep Argos Translate\u0026rsquo;s dependencies separate from other Python programs you have installed.\nPython Installation Instructions\nOn Ubuntu:\nsudo apt-get update sudo apt-get install -y python3 Install # Download a copy of this repo (this requires either installing git or downloading a zip from GitHub): git clone https://github.com/argosopentech/argos-translate.git cd argos-translate Make a virtual environment to install into (optional): python3 -m pip install --upgrade virtualenv # If virtualenv not already installed virtualenv env source env/bin/activate Install this package with pip: python3 -m pip install --upgrade pip python3 -m pip install . Build and install snap package # Install snapd if it isn\u0026rsquo;t already installed. Using snapd install snapcraft and its dependency multipass: sudo snap install multipass sudo snap install snapcraft Clone this repo: git clone https://github.com/argosopentech/argos-translate.git cd argos-translate From the root directory of this project build the snap package: SNAPCRAFT_BUILD_ENVIRONMENT_MEMORY=4G snapcraft Any unzipped package files in package/ will be automatically included in the snap archive (and won\u0026rsquo;t be able to be deleted by users of the snap).\nNote, the build won\u0026rsquo;t run with Snapcraft\u0026rsquo;s default build memory of 2GB so you need to set the SNAPCRAFT_BUILD_ENVIRONMENT environment variable. More on Snapcraft forum.\nInstall the snap package: sudo snap install --devmode argos-translate_\u0026lt;version information\u0026gt;.snap Run Argos Translate! # argos-translate argos-translate-gui When installing with snap a .desktop file should also be installed which will make Argos Translate available from the desktop menu.\nContributing # Contributions are welcome! Available issues are on the GitHub issues page.\nSupport # For support use the GitHub issues page or forum.\nDonations # If you find this software useful donations are appreciated.\nGitHub Sponsor PayPal Ethereum: 0x4E9017d8e275cA54C91E056381DAb9fe6ECC1AF6 Paid supporters receive priority support.\nLicense # Dual licensed under either the MIT License or CC0.\n","date":"21 April 2021","permalink":"/projects/argos-translate/","section":"Projects","summary":"Argos Translate # Docs | Website | Video intro","title":"argos-translate"},{"content":" Markov chain # this is a simple markov chain implementation inspired by a project @nathansolomon1678 made\nGenerate chain # The text used to make the chain should be in ./all_stuff.txt, or change the string in markov.py from markov import MakeMarkov mark = MakeMarkov() # Only needed on first run mark.setup_db() # Pull all the words from the text file mark.read_file() # Make the sql database of unique words, only needed on first run mark.find_unique()\t# This will output to a database called `markov.db` # Finally build the markov chain! mark.train()\t# This will output to a file called `markov.json` Generate text (in code) # from markov import generate # Some text will be generated with 100 words text = generate() # Some text will start with the word \u0026#34;The\u0026#34; text = generate(start=\u0026#34;The\u0026#34;) # Some text with the length of 1000 words text = generate(length=1000) # Some text with the length of 1000 words and starting with \u0026#34;The\u0026#34; text = generate(length=1000, start=\u0026#34;The\u0026#34;) Generate text (with api) # uvicorn fast:app --reload Some text will be generated with 100 words # http://127.0.0.1:8000 Some text will be generated with 1000 words # http://127.0.0.1:8000?length=1000 Some text will start with the word \u0026ldquo;The\u0026rdquo; # http://127.0.0.1:8000?start=The Some text will start with the word \u0026ldquo;The\u0026rdquo; and with the length of 1000 # http://127.0.0.1:8000?start=The\u0026amp;length=1000 ","date":"12 April 2021","permalink":"/projects/markov-chain/","section":"Projects","summary":"Markov chain # this is a simple markov chain implementation inspired by a project @nathansolomon1678 made","title":"markov-chain"},{"content":" Jake Roggenbuck # Hello! 👋 My name is Jake Roggenbuck and I\u0026rsquo;m studying Computer Science. Currently doing research in the intersection of programming languages and security. My favorite languages are Rust 🦀, Go and C. I also frequently use TypeScript, C++ and Python.\n🌐 Website: jr0.org\n🇱 LinkedIn: Jake Roggenbuck\n📦 Crates.io: jakeroggenbuck\n🔑 GPG Key: 309BBC\nOpen Source 💻 # I\u0026rsquo;ve contributed to the following open source projecrs:\nonlook-dev/onlook (YC W25), pretzelai/pretzelai (YC W24), microsoft/RD-Agent, rocky-linux/rocky, rust-lang/miri, grafana/pyroscope, fastly/fastly-py, pathwaycom/pathway, argosopentech/argos-translate, python-mechanize/mechanize, and more\nI also created a fork of OpenAI's gpt-2 at https://github.com/jakeroggenbuck/gpt-2 in 2019 but didn't push any changes. - I still cannot believe how implactful this one project has been on the world as a whole and on software engineering especially\n- You can verify the creation date with GitHub's api at https://api.github.com/repos/jakeroggenbuck/gpt-2 and look at the `created_at` field ","date":"1 April 2021","permalink":"/projects/jakeroggenbuck/","section":"Projects","summary":"Jake Roggenbuck # Hello!","title":"jakeroggenbuck"},{"content":" Journal.vim # What it does # This plugin lets you organize your daily journal.\nIt lets you open a day specific journal and manages the files all behind the scenes.\nSetup and Config # let g:journals_directory = \u0026#34;/path/to/journals_directory\u0026#34; \u0026#34; Set by default let g:journals_date_format = \u0026#39;%b-%d-%Y\u0026#39; \u0026#34; Set by default let g:journals_title_template = \u0026#39;Journal entry {date}\u0026#39; Commands # Command Description Journal Open the journal for today JournalSearch Keyword search for journals Future features # A search by date or relative date (ex. \u0026ldquo;last friday\u0026rdquo;) ","date":"31 March 2021","permalink":"/projects/journal.vim/","section":"Projects","summary":"Journal.","title":"Journal.vim"},{"content":" nitrogen-css # A very epic CSS front end framework that is slightly not bad. ","date":"31 March 2021","permalink":"/projects/nitrogen-css/","section":"Projects","summary":"nitrogen-css # A very epic CSS front end framework that is slightly not bad.","title":"nitrogen-css"},{"content":" ursina-platform-game # ","date":"29 March 2021","permalink":"/projects/ursina-platform-game/","section":"Projects","summary":" ursina-platform-game # ","title":"ursina-platform-game"},{"content":" spicetify community themes # This is a collection of themes for spicetify, a command-line tool to customize Spotify; you can add your own theme simply by opening a Pull Requests (more info in the Contributions section).\nYou can find a preview of all the themes in the wiki. # Installation and usage # (If you use Arch Linux you can find this project on the AUR)\nOnce you cloned the repository you\u0026rsquo;ll need to put the files into the Themes folder. This varies between operating systems. The example shows the Themes directory for Linux. For other operating systems, see the Themes folder location here.\ncd spicetify-themes cp -r * ~/.config/spicetify/Themes NOTE: to install Dribbblish and DribbblishDynamic follow the instructions in its README.\nAfter that you can choose which theme to apply just by running spicetify config current_theme THEME_NAME. Some themes have 2 or more different color schemes. You can switch between them, once selected the theme, with spicetify config color_scheme SCHEME_NAME.\nContributions # If you want to add your theme:\nFork this repository Create another folder with your theme name. The theme name should consist of one word starting with an uppercase letter and shouldn\u0026rsquo;t contain spicetify or any whitespace in it; if a \u0026ldquo;-\u0026rdquo; is present in the name it must be followed by an uppercase letter. Copy color.ini and user.css into it Create a README.md in it with the following structure # THEME_NAME ## Screenshots [Put at least one image per color scheme here] ## More [Specify any needed font; (optionally) author name and/or any other info about the theme] Open a Pull Request Thanks to all the contributors.\nTroubleshooting # If you find problems when using or installing these themes, or you need help in modifying a theme use the Spectrum chat.\nFor bugs and requesting new features use the GitHub issues.\nIf you are unsure about which channel to use, go for Spectrum.\nNOTE: Spotify ad-blocked version is not supported.\n","date":"28 March 2021","permalink":"/projects/spicetify-themes/","section":"Projects","summary":"spicetify community themes # This is a collection of themes for spicetify, a command-line tool to customize Spotify; you can add your own theme simply by opening a Pull Requests (more info in the Contributions section).","title":"spicetify-themes"},{"content":" ursina ʕ •ᴥ•ʔゝ□ # An easy to use game engine/framework for python.\nGetting Started # Install Python 3.6 or newer. https://www.python.org/downloads/\nOpen cmd/terminal and type:\npip install ursina If you want to install the newest version from git, you can install like this:\npip install git+https://github.com/pokepetter/ursina.git If you want to easily edit the source, it\u0026rsquo;s recommended to clone the git repo and install as develop like this. Make sure you have git installed. https://git-scm.com/\ngit clone https://github.com/pokepetter/ursina.git python setup.py develop Also install any of the optional dependencies you want from the list below, or install them all with:\npip install ursina[extras] On some systems you might have to use pip3 instead of pip in order to use Python 3 and not the old Python 2.\nDependencies # python 3.6+ panda3d screeninfo, for detecting screen resolution hurry.filesize, for converting bytes to megabytes pillow, for texture manipulation psd-tools, for converting .psd files blender, for converting .blend files pyperclip, for copy/pasting Examples # from ursina import * # this will import everything we need from ursina with just one line. app = Ursina() ground = Entity( model = \u0026#39;cube\u0026#39;, color = color.magenta, z = -.1, y = -3, origin = (0, .5), scale = (50, 1, 10), collider = \u0026#39;box\u0026#39;, ) app.run() # opens a window and starts the game. Minecraft Clone\nPlatformer Game\nHow do I make a game? # Ursina games are made by writing Python code. You can use any text editor you want, but personally I like to use Atom.\nCreate an empty .py file called \u0026lsquo;ursina_game.py\u0026rsquo; Copy this text into your new file: from ursina import * # this will import everything we need from ursina with just one line. app = Ursina() player = Entity( model = \u0026#39;cube\u0026#39; , # finds a 3d model by name color = color.orange, scale_y = 2 ) def update(): # update gets automatically called by the engine. player.x += held_keys[\u0026#39;d\u0026#39;] * .1 player.x -= held_keys[\u0026#39;a\u0026#39;] * .1 app.run() # opens a window and starts the game. Type this in the terminal to start the game:\npython ursina_game.py If you use Atom, I recommend installing the package atom-python-run to run your scripts with the press of a button.\nYou can now move the orange box around with \u0026lsquo;a\u0026rsquo; and \u0026rsquo;d\u0026rsquo;!\nTo close the window, you can by default, press shift+q or press the red x. to disable this, write \u0026lsquo;window.exit_button.enabled = False\u0026rsquo; somewhere in your code.\n","date":"28 March 2021","permalink":"/projects/ursina/","section":"Projects","summary":"ursina ʕ •ᴥ•ʔゝ□ # An easy to use game engine/framework for python.","title":"ursina"},{"content":" Z-Flat # Overview # Z-flat is a compiled, statically-typed programming language. Its design goals are intuitive syntax, type-safety, and easy library management.\nRepository Overview # std - standard library folder zfc - the Z-flat compiler docs - all documentation\nSyntax highlighting # Vim # JakeRoggenbuck/zflat.vim Others # Please help us grow the zflat community and add support for your favorite tools Building # run ./scripts/build.sh to start the build process Running # run the compiler at ./cmake/zfc see instructions at docs/compiler.md Compiler Test # ./cmake/zfc input/file.zf compiles to input/file.o ","date":"25 March 2021","permalink":"/projects/zflat/","section":"Projects","summary":"Z-Flat # Overview # Z-flat is a compiled, statically-typed programming language.","title":"zflat"},{"content":" zflat.vim # A syntax highlighting for zflat lang files\nInstall # Plug \u0026#39;jakeroggenbuck/zflat.vim\u0026#39; ","date":"25 March 2021","permalink":"/projects/zflat.vim/","section":"Projects","summary":"zflat.","title":"zflat.vim"},{"content":" f09f # pronounced fonf; is a simple statically typed language with simple but powerful syntax # Include # include standio; Byte # byte name = 10000000; Byte array # byte[] name; byte name = [10000000, 10000001]; Char # char letter = \u0026#39;A\u0026#39;; Char array # char[] letter; char letter = [\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;]; Int # int number = 4; Int array # int[] numbers; int numbers = [2, 5, 4]; Prec # prec percent = 3.4; Prec array # prec[] precise_numbers; prec precise_numbers = [3.14, 2.71828, 4.222]; Ptr # ptr my_variable = 0x494949; String # string name = \u0026#34;Jake\u0026#34;; Bool # isRunning = True; If # if (num \u0026gt; 0) { ~ do thing ~ } Else if # if (name == \u0026#34;Jake\u0026#34;) { ~ do thing ~ } else { ~ do other things ~ } If # if (val \u0026lt; physics.gravity) { ~ val is less than gravity ~ } else if (val \u0026lt; physics.gravity / 2) { ~ val is less than half gravity ~ } else { ~ val is greater than gravity ~ } Function # function foo (int age, int height) returns string { return(age ^ height); } Return # return(4); Entry # while () { ~ Looped code ~ } Exit # do { ~ Looped code ~ } while still (); For # loop for item in array { ~ Looped code ~ } Classes # class Lang (string name) { ~ constructor ~ self.name = name; } : Main { ~ main body ~ function hello_lang () returns string { return(\u0026#34;Hello \u0026#34; + self.name()); } } Static # static byte name = 10101010; static char letter = \u0026#39;A\u0026#39;; static ptr = 0x494949; Burn # burn string name = \u0026#34;Jake\u0026#34;; ","date":"11 March 2021","permalink":"/projects/f09f/","section":"Projects","summary":"f09f # pronounced fonf; is a simple statically typed language with simple but powerful syntax # Include # include standio; Byte # byte name = 10000000; Byte array # byte[] name; byte name = [10000000, 10000001]; Char # char letter = \u0026#39;A\u0026#39;; Char array # char[] letter; char letter = [\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;]; Int # int number = 4; Int array # int[] numbers; int numbers = [2, 5, 4]; Prec # prec percent = 3.","title":"f09f"},{"content":"","date":"24 February 2021","permalink":"/tags/glsl/","section":"Tags","summary":"","title":"GLSL"},{"content":" minecraft_texture_packs # ","date":"24 February 2021","permalink":"/projects/minecraft_texture_packs/","section":"Projects","summary":" minecraft_texture_packs # ","title":"minecraft_texture_packs"},{"content":" Bookmark search # A simple thing to allow you to search your bookmarks better than any browser default bookmark search\nInstall meilisearch REST API from https://github.com/meilisearch/MeiliSearch and meilisearch with pip\n","date":"22 February 2021","permalink":"/projects/bookmark-search/","section":"Projects","summary":"Bookmark search # A simple thing to allow you to search your bookmarks better than any browser default bookmark search","title":"bookmark-search"},{"content":" colorgradient # Implementations # Python version colorgradient Rust version colorgradient-rs Julia version colorgradient-julia C version colorgradient-c Go version colorgradient-go Clojure version colorgradient-clj Diagram # TODOS # Make docs for usage, output, running, and testing\n","date":"20 February 2021","permalink":"/projects/colorgradient/","section":"Projects","summary":"colorgradient # Implementations # Python version colorgradient Rust version colorgradient-rs Julia version colorgradient-julia C version colorgradient-c Go version colorgradient-go Clojure version colorgradient-clj Diagram # TODOS # Make docs for usage, output, running, and testing","title":"colorgradient"},{"content":"","date":"15 February 2021","permalink":"/tags/kotlin/","section":"Tags","summary":"","title":"Kotlin"},{"content":" kotlin-todo-app # First view # Second view # ","date":"15 February 2021","permalink":"/projects/kotlin-todo-app/","section":"Projects","summary":" kotlin-todo-app # First view # Second view # ","title":"kotlin-todo-app"},{"content":" Scry # Scry is a web application that allows users to quickly check open ports, logins, and critical system information about their server from any web browser, anywhere.\nScry consists of both a local server and a client. The server is responsible for gathering data about the system it\u0026rsquo;s monitoring, then making it accessible to the client. The clients job is to present the data from the server, in an easy to use graphical interface in the browser.\nServer # The server includes two main parts, the updater_deamon and the Scry Fastapi. First, the updater_deamon will run commands in the background to gather crucial information about the system and update it in the local database. The Scry Fastapi will allow the client to pull data using an internal web API to present the data in the browser.\nRequirements # Python # The server code is located in ./server/, go to this directory before the next step\nInstall using pip3 install -r requirements.txt Non-Python # We use netstat and who for getting system information and Mongodb for our database\nMost systems come pre-installed with who, and possibly netstat but they may have to be installed separately\nRunning # Run ./start.sh to run the updater_deamon.py in the background then it will run the scry.py Fastapi app Client # The client is a Node.js React app the pulls data from the local Scry web API and displays it.\nRequirements # Install the requirements using npm install in the ./client/scry directory. Running # Then run it using npm start. Video demonstration # https://www.youtube.com/watch?v=D87Blido5tQ\nScreenshots # Design process artifacts # Original ideas # Original current designs # ","date":"15 February 2021","permalink":"/projects/scry/","section":"Projects","summary":"Scry # Scry is a web application that allows users to quickly check open ports, logins, and critical system information about their server from any web browser, anywhere.","title":"Scry"},{"content":" AgeInMinutes # A basic Kotlin app to test out android development with Kotlin.\nFirst view # Select a date # Confirm the date # Get the date in standard format # ","date":"14 February 2021","permalink":"/projects/ageinminutes/","section":"Projects","summary":"AgeInMinutes # A basic Kotlin app to test out android development with Kotlin.","title":"AgeInMinutes"},{"content":" Game Design Scoring Regression Model # Data # The data that this model used to train is the csv ./point-weighting-combined-new.csv and was made in a spreedsheet\nBuilding the model # Run the script ./generate_model.py to build the model\nUsage # To run the model that you built, this is all the code needed\nfrom tensorflow import keras import numpy as np import csv from generate_model import get_data_Y, get_data_X # Import the model made by ./generate_model.py model = keras.models.load_model(\u0026#39;neural_network.model\u0026#39;) # Gets the data from the model generator file \u0026#34;./generate_model.py\u0026#34; X = get_data_X() Y = get_data_Y() def test_each_match(): # Get the result actual win value, and the one predicted by the model for x, y in zip(X, Y): # Predicts the Y value from a given X result = model.predict(np.array([x])) # Prints the values in the (actual, predict) format print(y[0], result[0][0]) ","date":"6 February 2021","permalink":"/projects/game-design-scoring/","section":"Projects","summary":"Game Design Scoring Regression Model # Data # The data that this model used to train is the csv .","title":"game-design-scoring"},{"content":" Installing QME # 1. Get Java 15 # Windows # If you are on windows, get the MSI version Java 15.0.2 MSI\nMac OS # Mac OS will use the zip version Java 15.0.2 ZIP\n2. Get the latest launcher # 2.1 Navigate to latest and click the qme-launcher-0.1.1.jar under assets # 2.2 Run the launcher jar with Java (By clicking it) and you will see the QME launcher open # 2.3 In the launcher, click Launch Game, it will have a popup that says Version not installed, would you like to install?. Click yes and watch the game download # 2.4 The game will say successfully installed when ready # 2.5 When it does say successfully installed click launch and the game will open! # ","date":"2 February 2021","permalink":"/projects/qme-launcher/","section":"Projects","summary":"Installing QME # 1.","title":"qme-launcher"},{"content":" QME Releases # How to install QME unsing the QME Installer (Recommended) # Download the QME launcher from the releases section in this github repository and run the executable file. If you are having issues follow the instructions below.\nIf you are not using the latest version of Java runtime (15) then QME Installer will not work. Please install it here, or use the JRE 11-14 compatible release without the launcher.\nJRE versions lower than 11 are not supported.\nHow to install QME from a release # If your operating system is not supported or you just want to make things hard then you can manually install QME. To do this follow these steps:\nDownload your prefered release from the releases section of this repository. Create a directory and move the jar file into it. Create a directory named qdata inside of the directory you just created. Launch the game by running java -jar qme.jar. There are two versions of QME release, the offical version is compatible with JRE 15, but there is also an alternative version compatible with JRE 11-14.\nPlease note that older JRE compatibility may come with performance, stability, and other issues.\n","date":"1 February 2021","permalink":"/projects/qme-releases/","section":"Projects","summary":"QME Releases # How to install QME unsing the QME Installer (Recommended) # Download the QME launcher from the releases section in this github repository and run the executable file.","title":"qme-releases"},{"content":"","date":"30 January 2021","permalink":"/tags/crystal/","section":"Tags","summary":"","title":"Crystal"},{"content":" crystal-colord # A library for colored text in crystal\nInstallation # Add the dependency to your shard.yml:\ndependencies: crystal-colord: github: jakeroggenbuck/crystal-colord Run shards install\nUsage # require \u0026#34;crystal-colord\u0026#34; message = CrystalColord::Colord.new \u0026#34;Hey\u0026#34; message.cyan(background=true) message.black puts message.bold Contributing # Fork it ( https://github.com/jakeroggenbuck/crystal-colord/fork) Create your feature branch (git checkout -b my-new-feature) Commit your changes (git commit) Push to the branch (git push origin my-new-feature) Create a new Pull Request Contributors # jakeroggenbuck - creator and maintainer ","date":"30 January 2021","permalink":"/projects/crystal-colord/","section":"Projects","summary":"crystal-colord # A library for colored text in crystal","title":"crystal-colord"},{"content":" bash_startup # A program that runs every time I open a terminal\nMore complex things I want to run when my bashrc runs (external scripts)\nbash_startup # Description # There are two scripts, the first bash_startup are written in crystal and c++. This script will search through my bashrc and find aliases\nExample # start_alias_show () { bash_startup_cpp } # Checks if bash_startup_cpp is installed, then run it if [ 2\u0026gt;/dev/null 1\u0026gt;/dev/null $(which bash_startup_cpp) ]; then start_alias_show else echo \u0026#34;Install bash_startup_cpp and add it to your PATH\u0026#34; fi cheat_sheet_startup # Description # The second script is cheat_sheet_startup and will pull cheat sheets from cht.sh. This script is written in python.\nExample # # It will print the last cheat sheet located at `~/.cht_sheet_entry` # then it will pull the next one in the background so the perceived time # it takes to run is just the time it takes to read the cht_sheet_entry start_cheat_sheet () { # Print the cheat sheet pulled last run cheat_sheet_startup print # Pulling the cheat sheet (takes a few seconds) (cheat_sheet_startup pull \u0026amp;) } # Checks if the script is installed, then runs it if [ 2\u0026gt;/dev/null 1\u0026gt;/dev/null $(which cheat_sheet_startup) ]; then start_cheat_sheet else echo \u0026#34;Install cheat_sheet_startup and add it to your PATH\u0026#34; fi ","date":"29 January 2021","permalink":"/projects/bash-startup/","section":"Projects","summary":"bash_startup # A program that runs every time I open a terminal","title":"bash-startup"},{"content":" QME5 File Server # A dynamic website to host jar files for the game qme5\nRun # python3 app.py\n","date":"21 January 2021","permalink":"/projects/qme5-server/","section":"Projects","summary":"QME5 File Server # A dynamic website to host jar files for the game qme5","title":"qme5-server"},{"content":" Paper # print \u0026#34;Hello World!!\u0026#34; string .name = input \u0026#34;What\u0026#39;s your name? \u0026#34; int .age = input \u0026#34;What\u0026#39;s you age? \u0026#34; float .a_float = input \u0026#34;Pick a number greater than 1 and less than 2: \u0026#34; bool .ask_again = True string .name = input \u0026#34;What\u0026#39;s your name? \u0026#34; int .age = input \u0026#34;What\u0026#39;s you age? \u0026#34; stop ","date":"19 January 2021","permalink":"/projects/paper/","section":"Projects","summary":"Paper # print \u0026#34;Hello World!","title":"Paper"},{"content":"\nSpicetify Custom Apps and Extensions A repository to help users find custom apps and extensions for spicetify-cli\nSummary # 🔥 A central location for custom apps and extensions for spicetify-cli 👀 More features 🎉 Open source How to add Custom Apps or Extensions to Spicetify # Install spicetify-cli using the instructions found here Click on the green Clone or download button at the top right this repo and choose Download ZIP Unzip the .zip file Choose the app that you want and drag it out of the folder Open the spicetify-cli CustomApps folder (paths can be found below) Drag the custom app into the CustomApps folder Open config.ini and add the name of the custom app to the custom_apps line separated by the | character [AdditionalOptions] ... custom_apps = reddit|genius|yourownapp OR add the respective apps / extension by typing: spicetify config extensions CustomAppOrExtensionName.js Run spicetify backup apply in the command line and Spicetify will install the app You\u0026rsquo;re set! 🎉 CustomApps Folder Paths # Platform Path macOS ~/spicetify_data/Themes OR\n$SPICETIFY_CONFIG/CustomApps Windows %userprofile%.spicetify\\CustomApps\\ Linux ~/.config/spicetify/CustomApps OR\n$XDG_CONFIG_HOME/.config/spicetify/CustomApps/ Want to contribute your custom app(s) to this repo? # Follow the instructions below or read the CONTRIBUTING.md file!\nFork this repository Create another folder with your custom app name in all lower case with no symbols (dashes and underscores are okay) or spaces. Ensure the files and folders don\u0026rsquo;t contain the word spicetify Copy the necessary files into the folder Create a README.md in it with the following structure # App Name ## Screenshots [Put at least one image of the app working here] ## More [Specify any dependencies, author name, and any other info about the custom app] (Optional) Add your name and custom apps or extensions to the AUTHORS.md file If you decide to add your name, please use the following format # Authors ... ### [Your name](link to website, GitHub profile, donation page, etc.) - [Your custom app](link to repo) - Your custom app description and other info Open a Pull Request Support # If you run into any issues or need help troubleshooting any custom apps or extensions reach out to the Spicetify community on Spectrum\nIf there are bugs or you\u0026rsquo;d like to request a feature, open an issue using the respective template.\nLicense # Licensed under the MIT License by Braxton Huff\nIf there is any other problem, please refer to the spicetify-cli wiki to help troubleshoot your problem. # Hopefully this helps improve your use of Spotifty and treats you well! Cheers! # ","date":"18 January 2021","permalink":"/projects/spicetify-customapps/","section":"Projects","summary":"","title":"spicetify-customapps"},{"content":" reminder-bot # A Discord Bot designed to remind users of custom events\n","date":"2 January 2021","permalink":"/projects/reminder-bot/","section":"Projects","summary":"reminder-bot # A Discord Bot designed to remind users of custom events","title":"reminder-bot"},{"content":" Draft.vim # ✏ Quickly write up and save drafts for messaging apps in your favorite editor vimawesome.com/plugin/draft-vim\nWhy use Draft.vim # I often write important messages in a vim buffer before I send it.\nThe main reason for this is because it is simply faster. The second reason is you might want syntax highlighting or auto formatting. Also, sometimes you want to write a message without worrying about accidentally sending it. Requirements # Pandoc wkhtmltopdf Dragon ( https://github.com/mwh/dragon) Features # Quickly open a new, well named file in a consistent directory Each file automatically contains attributes like the date and title that can be searchable Setup and Config # Make a drafts directory # \u0026#34; Add the command to setup a drafts directory let g:drafts_directory = \u0026#34;/path/to/drafts/\u0026#34; Optional, create a keybind for the commands # NewDraft Keybind # nnoremap \u0026lt;Leader\u0026gt;nd :call NewDraft()\u0026lt;CR\u0026gt; ListDrafts Keybind # nnoremap \u0026lt;Leader\u0026gt;ld :call ListDrafts()\u0026lt;CR\u0026gt; OpenDrafts Keybind # nnoremap \u0026lt;Leader\u0026gt;z :call OpenDrafts()\u0026lt;CR\u0026gt; Usage # Commands # Command Description Draft Open a blanck new draft Draft \u0026quot;title\u0026quot; Open a draft with a title DraftExt \u0026quot;extension\u0026quot; Change the file extension of a draft Drafts Open the draft directory in a buffer DraftCopy Copy the contents of the draft to the clipboard DraftSearch Search through drafts by keyword More info # New draft: run :Draft or :Draft \u0026quot;\u0026lt;Title\u0026gt;\u0026quot; to auto name with the date and time Edit the file extension: run :DraftExt .md to change the file to markdown Open the drafts directory: run :Drafts Copy the contents of the current draft :DraftCopy Draft will open a new file in a specific directory, with a unique name The file will be based on a template with stuff like the title and datetime Install # Vim-Plug # Plug \u0026#39;jakeroggenbuck/draft.vim\u0026#39; Vundle # Plugin \u0026#39;jakeroggenbuck/draft.vim\u0026#39; Changelog # 0.1 draft.vim - not fully functional, just a concept # Open a new draft with a name List the draft but no reopening them 0.2 draft.vim - first complete version # Add OpenDrafts() Add new command aliases Draft, DraftExt 0.3 draft.vim - more features # Add ClipDraft() for DraftCopy Add Buffer reload for DraftExt 0.4 draft.vim - convert features # Add ConvertMDToHTML() for DraftToHTML Add ConvertMDToPDF() for DraftToPDF Add ConvertHTMLToPDF and ConvertToPDFFromTemplate() for DraftToTemplatePDF Add template for html conversion Change readme format a little Add vimawesome link! 0.5 draft.vim - more features # Add DraftDragonPDF for DraftDragonPDF Add DraftOpenPDF for OpenPDF Add requirements 0.6 draft.vim - added search # Add DraftSearch by word :DraftSearch \u0026lt;term\u0026gt; :DraftSearch school 0.7 draft.vim - search fixes # Rank searches Fix parenthesis in filename bug 0.8 draft.vim - date format fix # Change default date format to m/d/y Documentation fixes and additions Fix rename symbols issue Add testing for python Testing # pip install -r requirements.txt cd python pytest TODO # Make md to template pdf correctly do syntax highlight Maybe TODO # Make a draft file type with metadata and parse out the metadata when opened in vim, then use this data to search for notes better and stuff, like have raw data for python to search better with ","date":"31 December 2020","permalink":"/projects/draft.vim/","section":"Projects","summary":"Draft.","title":"draft.vim"},{"content":" \"Thinking back to early CentOS days... My cofounder was Rocky McGaugh. He is no longer with us, so as a H/T to him, who never got to see the success that CentOS came to be, I introduce to you...Rocky Linux\"\n— Gregory Kurtzer, Founder About # Rocky Linux is a community enterprise Operating System designed to be 100% bug-for-bug compatible with Enterprise Linux, now that CentOS has shifted direction.\nFrequently Asked Questions # Q: What do you mean, \u0026ldquo;CentOS has shifted direction?\u0026rdquo;\nThe CentOS project recently announced a shift in strategy for CentOS. Whereas previously CentOS existed as a downstream build of its upstream vendor (it receives patches and updates after the upstream vendor does), it will be shifting to an upstream build (testing patches and updates before inclusion in the upstream vendor).\nAdditionally, support for CentOS Linux 8 has been cut short, from May 31, 2029 to December 31, 2021.\nQ: So where does Rocky Linux come in?\nRocky Linux aims to function as a downstream build as CentOS had done previously, building releases after they have been added to the upstream vendor, not before.\nQ: When will it be released?\nThere is not currently an ETA for release.\nQ: What is the vision for Rocky Linux?\nA solid, stable, and transparent alternative for production environments, developed by the community for the community.\nQ: Who drives Rocky Linux?\nWe all do, Rocky Linux is a community-driven project and always will be. Rocky Linux will not be sold or driven by corporate interest.\nQ: How can I get involved?\nPlease view the contributing section below.\nContact # Team Contact Press outreach@rockylinux.org Development development@rockylinux.org Infrastructure infrastructure@rockylinux.org Security security@rockylinux.org Web and Branding web@rockylinux.org, brand@rockylinux.com For all other questions: hello@rockylinux.org\nStay Informed # Slack Forum GitHub Twitter IRC Reddit Matrix Contributing # Thank you for your interest in contributing to the project.\nIf you are a developer, architect, engineer, or otherwise looking to contribute your time and expertise, please consider joining the Slack and jumping into the most relevant channel to your interests.\nIf you are interested in donating or sponsoring the project, please email hello@rockylinux.org.\nNOTE: We do not currently have any official crowdsourcing established.\nSpecial Thanks # We would like to thank the following groups for their support thus far on the project:\nCtrl IQ, Inc. The OSU Open Source Lab Clouvider SpryServers FMI Groupe ","date":"14 December 2020","permalink":"/projects/rocky/","section":"Projects","summary":"\"","title":"rocky"},{"content":" door-lock # Smart home door lock using a raspberry pi\nTODOS # Add an example and running info\n","date":"11 December 2020","permalink":"/projects/door-lock/","section":"Projects","summary":"door-lock # Smart home door lock using a raspberry pi","title":"door-lock"},{"content":" advent-of-code # My advent of code throughout the years https://adventofcode.com/\n","date":"1 December 2020","permalink":"/projects/advent-of-code/","section":"Projects","summary":"advent-of-code # My advent of code throughout the years https://adventofcode.","title":"advent-of-code"},{"content":" PLL (Planck Lang Library) # Plank Lang Standard Library\nio.plk list.plk math.plk struct.plk ","date":"1 December 2020","permalink":"/projects/pll/","section":"Projects","summary":"PLL (Planck Lang Library) # Plank Lang Standard Library","title":"pll"},{"content":" planck.vim # A syntax highlighting for planck lang files\nInstall # Plug jakeroggenbuck/planck.vim\n","date":"30 November 2020","permalink":"/projects/planck.vim/","section":"Projects","summary":"planck.","title":"planck.vim"},{"content":" HexViewer # Read the binary data of a file in hex\nUpdated Rust Version # Better rust version: https://github.com/JakeRoggenbuck/hexviewer-rs\nUsage # python3 main.py -i video.mp4 ","date":"29 November 2020","permalink":"/projects/hexviewer/","section":"Projects","summary":"HexViewer # Read the binary data of a file in hex","title":"HexViewer"},{"content":"","date":"26 November 2020","permalink":"/tags/css/","section":"Tags","summary":"","title":"CSS"},{"content":" spicetify-cli-extensions # This is a place for an assortment of small extensions for spicetify\nautoSkipSadSongs.js # Skip sad songs by block listing the artist or the title.\nAdd autoSkipSadSongs.js to your spicetify config under extensions.\n","date":"26 November 2020","permalink":"/projects/spicetify-cli-extensions/","section":"Projects","summary":"spicetify-cli-extensions # This is a place for an assortment of small extensions for spicetify","title":"spicetify-cli-extensions"},{"content":" spicetify-cli-themes # A collection of tweaked and custom themes for spicetify-cli\nTree-Green # A nice calming tree green based on Pop-Dark\n","date":"26 November 2020","permalink":"/projects/spicetify-cli-themes/","section":"Projects","summary":"spicetify-cli-themes # A collection of tweaked and custom themes for spicetify-cli","title":"spicetify-cli-themes"},{"content":"404: Not Found\n","date":"24 November 2020","permalink":"/projects/learn-planck/","section":"Projects","summary":"404: Not Found","title":"learn-planck"},{"content":" Pinter # This repository houses the codebase for pinter (Planck interpreter), an interpreter for Planck bytecodes.\nRequirements # GNU make clang Build # make\nRun # ./bin/pinter [file] to execute a file ./bin/pinter --help for help\n","date":"24 November 2020","permalink":"/projects/pinter/","section":"Projects","summary":"Pinter # This repository houses the codebase for pinter (Planck interpreter), an interpreter for Planck bytecodes.","title":"pinter"},{"content":" ImportLint # Check and fix your module and package imports in python files\nUse # Check # importlint check test.py\nfix # importlint fix test.py\nInstall # pip install importlint\nTODO # Make command line interface for # Audit file (Done) Correct file in place (Done) Make features: a = can audit, c = can correct # [c] Sort imports alphabetically [a] Check for use of * in imports [] Check for unused imports [] Check for no imports for used module [] Consolidate multiple similar imports into one line Package for pypi # Make a setup.py Publish package! ","date":"8 November 2020","permalink":"/projects/importlint/","section":"Projects","summary":"ImportLint # Check and fix your module and package imports in python files","title":"importlint"},{"content":" jakesutils # A collection of utils that I use all the time in my projects\n","date":"7 November 2020","permalink":"/projects/jakesutils/","section":"Projects","summary":"jakesutils # A collection of utils that I use all the time in my projects","title":"jakesutils"},{"content":" Themes # These are some great themes. I only use gruvbox right now but the rest of these themes are great. morhetz/gruvbox # https://github.com/morhetz/gruvbox baskerville/bubblegum # https://github.com/baskerville/bubblegum mhartington/oceanic-next # https://github.com/mhartington/oceanic-next cocopon/iceberg.vim # https://github.com/cocopon/iceberg.vim drewtempelmeyer/palenight.vim # https://github.com/drewtempelmeyer/palenight.vim roosta/srcery # https://github.com/roosta/srcery joshdick/onedark.vim # https://github.com/joshdick/onedark.vim ajh17/spacegray.vim # https://github.com/ajh17/spacegray.vim Appearance # These make vim look really nice, the goyo is great for taking screenshots and just having a distraction free work time.\nvim-airline/vim-airline # https://github.com/ajh17/spacegray.vim vim-airline/vim-airline-themes # https://github.com/vim-airline/vim-airline-themes junegunn/goyo.vim # https://github.com/junegunn/goyo.vim Movement # These are great for making editing speedy\ntacahiroy/ctrlp-funky # justinmk/vim-sneak # t9md/vim-choosewin # Efficiency # Coc is the best for completion\nneoclide/coc.nvim # junegunn/fzf.vim # kien/ctrlp.vim # Git # These help with checking files for diffs and making git easy inside of vim\ntpope/vim-fugitive # airblade/vim-gitgutter # Language # Vimtex helps make vim way better for latex, flake8 is great for python and python-mode also is great\nlervag/vimtex # nvie/vim-flake8 # python-mode/python-mode # Organization # Helps me take notes in class and remember stuff\nxolox/vim-misc # xolox/vim-notes # Fun-Misc # Rainbow parentheses is great for code golf and list comprehensions, and vim table mode is great for writing docs\njunegunn/rainbow_parentheses.vim # dhruvasagar/vim-table-mode # ","date":"26 October 2020","permalink":"/projects/bestvimplugins/","section":"Projects","summary":"Themes # These are some great themes.","title":"BestVimPlugins"},{"content":" yes-no-neural-network-classifier # ","date":"24 October 2020","permalink":"/projects/yes-no-neural-network-classifier/","section":"Projects","summary":" yes-no-neural-network-classifier # ","title":"yes-no-neural-network-classifier"},{"content":" neural_network # ","date":"20 October 2020","permalink":"/projects/neural_network/","section":"Projects","summary":" neural_network # ","title":"neural_network"},{"content":"404: Not Found\n","date":"18 October 2020","permalink":"/projects/strategy_presentation_numpy/","section":"Projects","summary":"404: Not Found","title":"strategy_presentation_numpy"},{"content":" yaml_serialize # A simple way to store python objects in yaml\nUse # from src import yaml_serialize class MyObject: def __init__(self): self.name = \u0026#34;Jake\u0026#34; self.age = 16 self.favorite_color = \u0026#34;Green\u0026#34; self.mylist = [\u0026#34;hey\u0026#34;, \u0026#34;this\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;cool\u0026#34;] my_object = MyObject() serializer = yaml_serialize.Serialize(my_object) # Get serialized object as string my_serialized_object = serializer.get() # Write serialized object with class name \u0026#34;MyObject.yml\u0026#34; serializer.write() # Write serialized object with custom name \u0026#34;MyCoolObject.yml\u0026#34; serializer.write(\u0026#34;MyCoolObject.yml\u0026#34;) des = yaml_serialize.Deserialize() new = des.read(my_serialized_object) print(new) Output # _object: !!python/object:__main__.MyObject age: 16 favorite_color: Green mylist: - hey - this - is - cool name: Jake ","date":"10 October 2020","permalink":"/projects/yaml_serialize/","section":"Projects","summary":"yaml_serialize # A simple way to store python objects in yaml","title":"yaml_serialize"},{"content":" Fastly Python Client # fastly-py is available through pip as the fastly package\nThere are three simple scripts provided in /bin that can be used for various stand-alone purge operations.\nA Note About Authentication # Authenticating with an API Token is shown in the example below. For more information on API Tokens, please see Fastly\u0026rsquo;s API Token documentation. For more information about authenticating to our API, please see our Authentication section.\nUsage # import fastly api = fastly.API() api.authenticate_by_key(\u0026#39;MYKEY\u0026#39;) api.purge_url(\u0026#39;www.example.com\u0026#39;, \u0026#39;/some/path\u0026#39;) TODO: # Doc files Docstrings Config file create requirements.txt file\nRunning Tests # $ python -m test.api_test Set up environment configuration # We use Python\u0026rsquo;s os and a .env file to manage environment variables in development and test environments. See the list of required environment variables in the .env.example file in the root directory.\nTo optionally set values for testing, make a copy of .env.example and name it .env.\ncp .env.example .env # Note: # If using direnv, you can just use it as a `.envrc` file and not have to `source` it manually. # Using your favorite editor, update the values of the environment variables in `.env` and then source .env Distributing a package # Create a .pypirc file:\n$ cat \u0026gt; .pypirc [distutils] index-servers = pypi [pypi] repository: https://upload.python.org/legacy/ Install twine:\n$ python3 -m pip install --user --upgrade twine Create a build:\n$ python3 setup.py sdist bdist_wheel Use twine to publish to Pypi:\n$ twine upload dist/* Uploading distributions to https://upload.pypi.org/legacy/ Enter your username: [YOUR_USERNAME] Enter your password: Uploading fastly-0.2.3-py3-none-any.whl 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13.2k/13.2k [00:02\u0026lt;00:00, 6.40kB/s] Uploading fastly-0.1.3-py2.7.egg 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17.8k/17.8k [00:01\u0026lt;00:00, 12.8kB/s] Uploading fastly-0.1.3-py3.6.egg 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18.3k/18.3k [00:01\u0026lt;00:00, 14.2kB/s] Uploading fastly-0.2.3.tar.gz 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.25k/9.25k [00:01\u0026lt;00:00, 9.09kB/s] Builds and uploads to PyPi. More info on this at the python site. You will need to be granted access to the fastly package in order to push.\n","date":"6 October 2020","permalink":"/projects/fastly-py/","section":"Projects","summary":"Fastly Python Client # fastly-py is available through pip as the fastly package","title":"fastly-py"},{"content":" Ice # Programming language and interpreter\nNAME ice SYNOPSIS ice GROUP | COMMAND GROUPS GROUP is one of the following: ice COMMANDS COMMAND is one of the following: run step print \u0026#34;this\u0026#34; input \u0026#34;that: \u0026#34; print \u0026#34;this and that\u0026#34; int \u0026#34;856\u0026#34; str \u0026#34;hey\u0026#34; str $0 print $1 calc $1 + $1 dump ","date":"6 October 2020","permalink":"/projects/ice/","section":"Projects","summary":"Ice # Programming language and interpreter","title":"ice"},{"content":" HEASARC and NEOSSat Data Viewer # nasa-spaceapps-2020\nBy George Berdovskiy and Shuzheng (Tom) Zhang\nSet up instructions:\nInstall pip, see how to do it at https://pip.pypa.io/en/stable/installing/ Install flask using pip install flask Install packages using pip, you can see the full list in requirements.txt You can install them manually, or navigate to this directory in terminal/command prompt and type in: pip install -r requirements.txt Navigate to directory nasa-spaceapps-2020/app_directory in terminal/command prompt Run the webapp using python viewer.py If you see line Running on http://127.0.0.1:5000/ in the terminal, this indicates that the app is working If not, check for missing packages and dependencies Now go to your browser (We have only tested Firefox), and type in http://127.0.0.1:5000/ in the address bar The webapp should now be up and running ","date":"6 October 2020","permalink":"/projects/nasa-spaceapps-2020/","section":"Projects","summary":"HEASARC and NEOSSat Data Viewer # nasa-spaceapps-2020","title":"nasa-spaceapps-2020"},{"content":" Blue alliance data # Blue alliance data pull utility.\n","date":"1 October 2020","permalink":"/projects/blue-alliance-data/","section":"Projects","summary":"Blue alliance data # Blue alliance data pull utility.","title":"blue-alliance-data"},{"content":" ImgurApi # Simple Imgur library to interface with the Imgur web API\nfrom main import Imgur # Create object imgur = Imgur() # Upload image imgur.upload(\u0026#34;test.jpg\u0026#34;) # Get viewable images (Not Blocked Not Deleted) imgur.get_viewable_images() # Block image by hash (Know as \u0026#34;id\u0026#34;) imgur.block(\u0026#34;\u0026lt;image hash\u0026gt;\u0026#34;) # Delete image by hash (Know as \u0026#34;id\u0026#34;) imgur.delete(\u0026#34;\u0026#34;) # View all things in the uploaded collection imgur.view() ","date":"29 September 2020","permalink":"/projects/imgurapi/","section":"Projects","summary":"ImgurApi # Simple Imgur library to interface with the Imgur web API","title":"ImgurApi"},{"content":" Logger # A simple and fask logging library that uses the power of mongodb to save and query logs (with built in cli)\nCli # # search search \u0026lt;tag\u0026gt; \u0026lt;logger_name\u0026gt; mongofastlogger search Info # clear clear \u0026lt;logger_name\u0026gt; mongofastlogger clear # log log \u0026lt;tag\u0026gt; \u0026lt;message\u0026gt; \u0026lt;logger_name\u0026gt; mongofastlogger log Info \u0026#34;This is a log message\u0026#34; # view view \u0026lt;logger_name\u0026gt; mongofastlogger view # export export \u0026lt;filename\u0026gt; \u0026lt;logger_name\u0026gt; mongofastlogger export filename.log # last last \u0026lt;metric\u0026gt; \u0026lt;amount\u0026gt; \u0026lt;logger_name\u0026gt; mongofastlogger last hours 3 # help find commands python3 mongofastlogger help with specific commands mongofastlogger command --help # other info \u0026lt;logger_name\u0026gt; is optional and is \u0026#39;logs\u0026#39; by default Library # from mongofastlogger import LogViewer, Logger # Make logger logger = Logger() # Log message with tag of \u0026#34;Something\u0026#34; logger.log(\u0026#34;Something\u0026#34;, \u0026#34;This is bad as well i guess but i dont actually know\u0026#34;) # Log message with tag of \u0026#34;Something\u0026#34; and display log in console logger.log(\u0026#34;Something\u0026#34;, \u0026#34;This is a message\u0026#34;, display=True) # Make Viewer viewer = LogViewer() # Print all logs viewer.view_log() # Search logs that have the tag \u0026#34;Something\u0026#34; viewer.search_logs_by_tag(\u0026#34;Something\u0026#34;) # Search logs in the last 3 days viewer.check_by_time(\u0026#34;days\u0026#34;, 3) # Export logs to example.log viewer.export_log(\u0026#34;example.log\u0026#34;) print(\u0026#34;Production\u0026#34;) # Make logger with name production_logger = Logger(\u0026#34;Production\u0026#34;) production_logger.log(\u0026#34;Error\u0026#34;, \u0026#34;Critical error in production\u0026#34;) # Make viewer with name production_viewer = LogViewer(\u0026#34;Production\u0026#34;) production_viewer.view_log() Todo # Make readme more easy to read, especially comments in code and command line instructions\nMake last use \u0026ldquo;amount metric\u0026rdquo; format because it is more intuitive\nMake last have more time metrics, like month and year\nMake actual docs for how to use this and its commands, and cli\n","date":"27 September 2020","permalink":"/projects/mongofastlogger/","section":"Projects","summary":"Logger # A simple and fask logging library that uses the power of mongodb to save and query logs (with built in cli)","title":"mongofastlogger"},{"content":" fumble_api # A simple web API to return critical charts for the game dnd 5e\nRunning # python3 main.py\nData manipulation (setup) # import data # Clear all the data in the db data.clear_data() # Upload the new data in the list (fumble_data) data.upload_data(data.fumble_data) Routes # Find table names at /\n/ -\u0026gt; shooting, melee\n/shooting/12 -\u0026gt; \u0026ldquo;Ooops!\u0026rdquo; + \u0026ldquo;You hit an unintended random target.\u0026quot;\n\u0026lt;table name\u0026gt;/\u0026lt;number 1-20\u0026gt;\n","date":"26 September 2020","permalink":"/projects/fumble_api/","section":"Projects","summary":"fumble_api # A simple web API to return critical charts for the game dnd 5e","title":"fumble_api"},{"content":" SoftwareGeneralFinalTests # ","date":"25 September 2020","permalink":"/projects/softwaregeneralfinaltests/","section":"Projects","summary":" SoftwareGeneralFinalTests # ","title":"SoftwareGeneralFinalTests"},{"content":" mongoc # A fast way to view databases, collections and documents from mongodb in the command line\nUsage # mongoc\nInstall # pip install mongoc\n","date":"24 September 2020","permalink":"/projects/mongoc/","section":"Projects","summary":"mongoc # A fast way to view databases, collections and documents from mongodb in the command line","title":"mongoc"},{"content":" mongodb_schema_check # from pymongo import MongoClient from typeschemalib import typeschemalib class Database: def __init__(self, location: str = \u0026#34;localhost\u0026#34;, port: int = 27017): \u0026#34;\u0026#34;\u0026#34;Set defaults and connect\u0026#34;\u0026#34;\u0026#34; self.location = location self.port = port self.connect() def connect(self): \u0026#34;\u0026#34;\u0026#34;Create client, database and collections\u0026#34;\u0026#34;\u0026#34; self.client = MongoClient(self.location, self.port) self.db = self.client.my_database self.profiles = self.db.profiles self.messages = self.db.messages def write_profile(self, document): \u0026#34;\u0026#34;\u0026#34;Check schema of document using schema file\u0026#34;\u0026#34;\u0026#34; schema = \u0026#34;schema/profile.stml\u0026#34; valid = typeschemalib.schema_check(schema, document) if valid is not True: # Not valid, print error print(valid) else: # All good schema, insert and print print(document) self.profiles.insert_one(document) if __name__ == \u0026#34;__main__\u0026#34;: # Get data to insert data = { \u0026#34;name\u0026#34;: input(\u0026#34;Name: \u0026#34;), \u0026#34;age\u0026#34;: int(input(\u0026#34;Age: \u0026#34;)), \u0026#34;lang\u0026#34;: input(\u0026#34;Lang: \u0026#34;), \u0026#34;percent\u0026#34;: float(input(\u0026#34;Percent: \u0026#34;)) } db = Database() db.write_profile(data) ","date":"23 September 2020","permalink":"/projects/mongodb_schema_check/","section":"Projects","summary":"mongodb_schema_check # from pymongo import MongoClient from typeschemalib import typeschemalib class Database: def __init__(self, location: str = \u0026#34;localhost\u0026#34;, port: int = 27017): \u0026#34;\u0026#34;\u0026#34;Set defaults and connect\u0026#34;\u0026#34;\u0026#34; self.","title":"mongodb_schema_check"},{"content":" typeschemalib # A yaml like schema that can be used to check dictionaries for correct schema\nSchema file # schema example # point: Int my_string: Str grade: Float data example # {\u0026#34;point\u0026#34;: 45, \u0026#34;my_string\u0026#34;: \u0026#34;Hey\u0026#34;, \u0026#34;grade\u0026#34;: 4.5} Checking data for correct schema # Test parse with stml file # from typeschemalib import typeschemalib if __name__ == \u0026#34;__main__\u0026#34;: data = {\u0026#34;point\u0026#34;: 45, \u0026#34;my_string\u0026#34;: \u0026#34;Hey\u0026#34;, \u0026#34;grade\u0026#34;: 4.5} # Validate data from schema file schema = \u0026#34;test.stml\u0026#34; valid = typeschemalib.schema_check(schema, data) print(valid) # Validate data from list of schema schema = [\u0026#34;point: Int\u0026#34;, \u0026#34;my_string: Str\u0026#34;, \u0026#34;grade: Int\u0026#34;] valid = typeschemalib.schema_check(schema, data) print(valid) # Validate data from dict of values schema = {\u0026#34;point\u0026#34;: \u0026#34;Int\u0026#34;, \u0026#34;my_string\u0026#34;: \u0026#34;Str\u0026#34;, \u0026#34;grade\u0026#34;: \u0026#34;Int\u0026#34;} valid = typeschemalib.schema_check(schema, data) print(valid) Todo # Make schema have regex\nMake documentation for stml writer\nAdd object type and class checker, isinstance issubclass time: DateTimeObject\n","date":"20 September 2020","permalink":"/projects/typeschemalib/","section":"Projects","summary":"typeschemalib # A yaml like schema that can be used to check dictionaries for correct schema","title":"typeschemalib"},{"content":" GitHub CLI # gh is GitHub on the command line. It brings pull requests, issues, and other GitHub concepts to the terminal next to where you are already working with git and your code.\nAvailability # GitHub CLI is available for repositories hosted on GitHub.com and GitHub Enterprise Server 2.20+, and to install on macOS, Windows, and Linux.\nDocumentation # Read the official docs for usage and more information.\nWe want your feedback # We\u0026rsquo;d love to hear your feedback about gh. If you spot bugs or have features that you\u0026rsquo;d really like to see in gh, please check out the contributing page.\nInstallation # macOS # gh is available via Homebrew and MacPorts.\nHomebrew # Install: Upgrade: brew install gh brew upgrade gh MacPorts # Install: Upgrade: sudo port install gh sudo port selfupdate \u0026amp;\u0026amp; sudo port upgrade gh Linux # See Linux installation docs.\nWindows # gh is available via scoop, Chocolatey, and as downloadable MSI.\nscoop # Install:\nscoop bucket add github-gh https://github.com/cli/scoop-gh.git scoop install gh Upgrade:\nscoop update gh Chocolatey # Install: Upgrade: choco install gh choco upgrade gh Signed MSI # MSI installers are available for download on the releases page.\nOther platforms # Download packaged binaries from the releases page.\nBuild from source # See here on how to build GitHub CLI from source.\nComparison with hub # For many years, hub was the unofficial GitHub CLI tool. gh is a new project that helps us explore what an official GitHub CLI tool can look like with a fundamentally different design. While both tools bring GitHub to the terminal, hub behaves as a proxy to git, and gh is a standalone tool. Check out our more detailed explanation to learn more.\n","date":"19 September 2020","permalink":"/projects/cli/","section":"Projects","summary":"GitHub CLI # gh is GitHub on the command line.","title":"cli"},{"content":"404: Not Found\n","date":"17 September 2020","permalink":"/projects/mechanize/","section":"Projects","summary":"404: Not Found","title":"mechanize"},{"content":" mongo_passkeep # A command line utility for storing passwords\nCli # Write # write\nView # view\nDelete # delete\nEdit # edit \u0026lt;id\u0026gt;\nRead # read \u0026lt;id\u0026gt;\n","date":"16 September 2020","permalink":"/projects/mongo_passkeep/","section":"Projects","summary":"mongo_passkeep # A command line utility for storing passwords","title":"mongo_passkeep"},{"content":" MongoShortener # ","date":"14 September 2020","permalink":"/projects/mongoshortener/","section":"Projects","summary":" MongoShortener # ","title":"MongoShortener"},{"content":"404: Not Found\n","date":"14 September 2020","permalink":"/projects/sars-cov-2_peplomer_structure_analysis/","section":"Projects","summary":"404: Not Found","title":"SARS-CoV-2_Peplomer_Structure_Analysis"},{"content":" dotfiles # A collection of my dotfiles that I use on my GNU/Linux machine Both Arch (current Laptop and Desktop) and Ubuntu (Old Desktop). I also used some of these dotfiles on MacOS, and exclusivly used yabai and skhd on that machine Tools # I use a tool I made called stow-squid to manage the updating and deplying of my dotfiles The config for that can be found also in the repo here Configs # Nvim, Tmux, Alacritty # Bspwm, Sxhkd, Skhd, Yabai (Window managers and keybinds) # ","date":"12 September 2020","permalink":"/projects/dotfiles/","section":"Projects","summary":"dotfiles # A collection of my dotfiles that I use on my GNU/Linux machine Both Arch (current Laptop and Desktop) and Ubuntu (Old Desktop).","title":"dotfiles"},{"content":" PySwitch # pyswitch # ","date":"11 September 2020","permalink":"/projects/pyswitch/","section":"Projects","summary":" PySwitch # pyswitch # ","title":"pyswitch"},{"content":" mongo-notes # ","date":"10 September 2020","permalink":"/projects/mongo-notes/","section":"Projects","summary":" mongo-notes # ","title":"mongo-notes"},{"content":" Shredder # Instead of sredding files normally, replace all of the data with Random Star Wars Quotes.\n","date":"7 September 2020","permalink":"/projects/shredder/","section":"Projects","summary":"Shredder # Instead of sredding files normally, replace all of the data with Random Star Wars Quotes.","title":"Shredder"},{"content":" mongodb-login # ","date":"5 September 2020","permalink":"/projects/mongodb-login/","section":"Projects","summary":" mongodb-login # ","title":"mongodb-login"},{"content":" Tap # Quickly tap basic files into existence\nHelp # # create an empty python file tap it py # create an empty python file with argparse boilerplate tap it pyarg Usage # tap-it 0.1.0 USAGE: tap it [FLAGS] \u0026lt;given\u0026gt; FLAGS: -f, --force -h, --help Prints help information -V, --version Prints version information ARGS: \u0026lt;given\u0026gt; TODO # Make a tap install\n","date":"29 August 2020","permalink":"/projects/tap/","section":"Projects","summary":"Tap # Quickly tap basic files into existence","title":"tap"},{"content":" sfetch # A very simple fetch\n","date":"22 August 2020","permalink":"/projects/sfetch/","section":"Projects","summary":"sfetch # A very simple fetch","title":"sfetch"},{"content":" flask-sql-login # A super simple login page with flask and sqlite3\n","date":"21 August 2020","permalink":"/projects/flask-sql-login/","section":"Projects","summary":"flask-sql-login # A super simple login page with flask and sqlite3","title":"flask-sql-login"},{"content":" player_data_finder # Install # pip3 install -r requirements.txt\nSetup # Make a file called config.py Add this class and your ftp credentials\nclass Config: host = \u0026#34;ftp.example.com\u0026#34; username = \u0026#34;user\u0026#34; password = \u0026#34;password\u0026#34; port = 254 Running # python3 main.py\n","date":"19 August 2020","permalink":"/projects/player_data_finder/","section":"Projects","summary":"player_data_finder # Install # pip3 install -r requirements.","title":"player_data_finder"},{"content":" GPGExchange # Website for posting encrypted files, encrypted meme exchange.\n","date":"18 August 2020","permalink":"/projects/gpgexchange/","section":"Projects","summary":"GPGExchange # Website for posting encrypted files, encrypted meme exchange.","title":"GPGExchange"},{"content":" js-snake # Snake game written in p5 JS Requires a local node server. install http-server by running \u0026ldquo;npm install http-server\u0026rdquo;, then while in the js-snake folder run \u0026ldquo;http-server\u0026rdquo;, finally connect to \u0026ldquo;http://localhost:8080/\u0026rdquo; and open js-snake.html.\n","date":"18 August 2020","permalink":"/projects/js-snake/","section":"Projects","summary":"js-snake # Snake game written in p5 JS Requires a local node server.","title":"js-snake"},{"content":" ChatApp # Realtime chat app using pusher\nRunning # cd src/ python3 app.py then open http://127.0.0.1:5000/\n","date":"17 August 2020","permalink":"/projects/chatapp/","section":"Projects","summary":"ChatApp # Realtime chat app using pusher","title":"ChatApp"},{"content":" position_reader # Running # python3 main.py nbtdatafile.dat python3 -m venv .venv source .venv/bin/activate pip install nbtlib ","date":"17 August 2020","permalink":"/projects/position_reader/","section":"Projects","summary":"position_reader # Running # python3 main.","title":"position_reader"},{"content":" Deepai # Use the deepdream API and other APIs from deepai to convert, upscale, and change images in interesting ways\nUse # Use as CLI # python3 main.py -i ~/Downloads/input_image.jpg -o image.png -n NeuralStyle -s ~/Downloads/vangohg.png python3 main.py -i ~/Downloads/input_image.jpg -o image.png -n DeepDream Use as library # from main import DeepImage # NeuralStyle style_name = \u0026#34;NeuralStyle\u0026#34; input_file = \u0026#34;~/Downloads/input_image.png\u0026#34; output_file = \u0026#34;~/Downloads/output_image.png\u0026#34; style_image = \u0026#34;~/Downloads/style_image.png\u0026#34; deep = Deep(style_name, input_file, output_file, style_image) # DeepDream style_name = \u0026#34;DeepDream\u0026#34; input_file = \u0026#34;~/Downloads/input_image.png\u0026#34; output_file = \u0026#34;~/Downloads/output_image.png\u0026#34; deep = Deep(style_name, input_file, output_file) deep.download() Config # Make the config and get an api key at deepai.org config.py\nKEY = \u0026#34;API-KEY\u0026#34; urls: - DeepDream: \u0026#34;https://api.deepai.org/api/deepdream\u0026#34; - Colorizer: \u0026#34;https://api.deepai.org/api/colorizer\u0026#34; - NeuralStyle: \u0026#34;https://api.deepai.org/api/neural-style\u0026#34; Example # ","date":"11 August 2020","permalink":"/projects/deepai/","section":"Projects","summary":"Deepai # Use the deepdream API and other APIs from deepai to convert, upscale, and change images in interesting ways","title":"Deepai"},{"content":" Reddit_Downloader # Get page # Because Reddit requires authentication, it\u0026rsquo;s easier to go to the website and get the page source to parse the image urls\nConfig # element = \u0026#34;classname or elecment id\u0026#34; filename = \u0026#34;example.txt\u0026#34; directory = \u0026#34;example\u0026#34; Running # # Get filename and directory from config FILENAME = config.filename DIRECTORY = config.directory # Make GetUrls object and give it filename geturls = GetUrls(FILENAME) # Parse and Extract urls geturls.parse_urls() geturls.extract_url() # Gets urls list and give to download obejct urls = geturls.image_urls # Setup directory _setup = Setup(DIRECTORY) _setup.setup() # Make download object and download list of urls _dowload = DownloadUrls(urls, DIRECTORY) _dowload.download() ","date":"11 August 2020","permalink":"/projects/reddit_downloader/","section":"Projects","summary":"Reddit_Downloader # Get page # Because Reddit requires authentication, it\u0026rsquo;s easier to go to the website and get the page source to parse the image urls","title":"Reddit_Downloader"},{"content":" CError # Run cerror on a source code file of a compiled language to get the length of the outputted error compared to the filesize. Some people try to get the largest error with the smallest file, this script gives you a score for this.\nUse (in general) # #include __FILE__ credit: @adamhutchings for finding this\nThe sample file is above, this is a small amount of source but produces a large error. This would get a really good score because of that ratio of code to error.\nUse (the script) # ./cerror -f main.c -c g++ ./cerror -f \u0026lt;filename\u0026gt; -c \u0026lt;compiler name\u0026gt; ","date":"10 August 2020","permalink":"/projects/cerror/","section":"Projects","summary":"CError # Run cerror on a source code file of a compiled language to get the length of the outputted error compared to the filesize.","title":"CError"},{"content":" SQLShortener # Create short url with New, Use short url with Use\nStart # python3 app.py\nUse # localhost:5000/ex -\u0026gt; https://example.com\nNew # localhost:5000/new/example.com!ex\n","date":"10 August 2020","permalink":"/projects/sqlshortener/","section":"Projects","summary":"SQLShortener # Create short url with New, Use short url with Use","title":"SQLShortener"},{"content":" ModPackMaker # Use # run python3 main.py\nmod = Mods() mod.run_command(mod.command) Folder # Add minecraft mods to mods/ folder\n","date":"9 August 2020","permalink":"/projects/modpackmaker/","section":"Projects","summary":"ModPackMaker # Use # run python3 main.","title":"ModPackMaker"},{"content":" discord_keep_save # Running # python3 main.py\nUse # Type ;s message to save\nSetup # DB # On startup main.py does checks and setups for critical components\nDiscord # Add bot token to a file called token.txt\n","date":"5 August 2020","permalink":"/projects/discord_keep_save/","section":"Projects","summary":"discord_keep_save # Running # python3 main.","title":"discord_keep_save"},{"content":"404: Not Found\n","date":"4 August 2020","permalink":"/projects/dotfiles-1/","section":"Projects","summary":"404: Not Found","title":"dotfiles-1"},{"content":" learning_sql # Connecting # connection = sqlite3.connect(\u0026#34;myTable.db\u0026#34;) Creating a table # sql_command = \u0026#34;\u0026#34;\u0026#34; CREATE TABLE employee ( staff_number INTEGER PRIMARY KEY, fname VARCHAR(20), lname VARCHAR(30), gender CHAR(1), joining DATE);\u0026#34;\u0026#34;\u0026#34; Writing the empty table to the db # crsr = connection.cursor() crsr.execute(sql_command) connection.commit() Writing values to db # Getting data # First = input(\u0026#34;First: \u0026#34;) Last = input(\u0026#34;Last: \u0026#34;) Gender = input(\u0026#34;Gender: \u0026#34;) Joining = input(\u0026#34;Joining: \u0026#34;) Adding data to the correct syntax # sql_command = \u0026#34;INSERT INTO employee\u0026#34; sql_fields = \u0026#34;(staff_number, fname, lname, gender, joining)\u0026#34; sql_data = f\u0026#34;VALUES (NULL, \u0026#39;{First}\u0026#39;, \u0026#39;{Last}\u0026#39;, \u0026#39;{Gender}\u0026#39;, \u0026#39;{Joining}\u0026#39;);\u0026#34; Excecuting the command # cursor.execute(sql_command + sql_fields + sql_data) Make sure to commit and close # connection.commit() connection.close() ","date":"4 August 2020","permalink":"/projects/learning_sql/","section":"Projects","summary":"learning_sql # Connecting # connection = sqlite3.","title":"learning_sql"},{"content":" gpg_passkeep # Store passwords in a GPG encrypted vault\nTODO: # Add config file Add save directory ","date":"3 August 2020","permalink":"/projects/gpg_passkeep/","section":"Projects","summary":"gpg_passkeep # Store passwords in a GPG encrypted vault","title":"gpg_passkeep"},{"content":" VimGameSnake # version 0.1\nHow to play # :VimGameSnake to Start :echo g:VimSnakeScore to view score\nh j k l c q ← ↓ ↑ → end game quit Installation # VimPlug # Place this in your .vimrc:\nPlug \u0026lsquo;johngrib/vim-game-snake\u0026rsquo;\nThen run the following in Vim:\n:source %\n:PlugInstall\n","date":"2 August 2020","permalink":"/projects/vim-game-snake/","section":"Projects","summary":"VimGameSnake # version 0.","title":"vim-game-snake"},{"content":" vim-impulse-syntax # vim-impulse-syntax.vim - Impulse build syntax highlighting\n","date":"2 August 2020","permalink":"/projects/vim-impulse-syntax/","section":"Projects","summary":"vim-impulse-syntax # vim-impulse-syntax.","title":"vim-impulse-syntax"},{"content":" xdotool_python_window_util # Use # Size # from main import Size a = Size(40, 60) a.size_win() Move # from main import Move a = Move(True, 20, 30) a.move_win() Test # PYTHONPATH=./src pytest Future Features # Something to name windows\nSomething to use move and size with window by name\nSomething to get the size and position of a window by name\nSomething that uses the size and position of two windows by name and calculates distance between\n","date":"29 July 2020","permalink":"/projects/xdotool_python_window_util/","section":"Projects","summary":"xdotool_python_window_util # Use # Size # from main import Size a = Size(40, 60) a.","title":"xdotool_python_window_util"},{"content":" dot_dropper # Copy dotfiles to a new system\nUpdate # Deprecated for my new stow-squid\n","date":"26 July 2020","permalink":"/projects/dot_dropper/","section":"Projects","summary":"dot_dropper # Copy dotfiles to a new system","title":"dot_dropper"},{"content":" vimage.vim # vimage.vim - Open image paths in vim\nInstall # Plug jakeroggenbuck/vimage.vim\nUse # Set image viewer let g:image_viewer = \u0026quot;feh\u0026quot;\nSet key binding nmap \u0026lt;leader\u0026gt;mg :call Vimage()\u0026lt;CR\u0026gt;\n","date":"26 July 2020","permalink":"/projects/vimage.vim/","section":"Projects","summary":"vimage.","title":"vimage.vim"},{"content":" whats-next # A reworked utility to either guide an Arch Linux install, or a tool to help you learn the commands in order.\nModes # Easy Mode # This gives you the option to either (R)un the command or (E)dit the command.\nHard Mode # You need to type in the whole command to build up that memory\nImage # Install # ## Official Mirror curl -LO https://jr0.org/cdn/whats-next.py # or wget https://jr0.org/cdn/whats-next.py ## Github Source curl -LO https://raw.githubusercontent.com/JakeRoggenbuck/arch-installer-whats-next/main/whats-next.py # or wget https://raw.githubusercontent.com/JakeRoggenbuck/arch-installer-whats-next/main/whats-next.py ","date":"23 July 2020","permalink":"/projects/arch-installer-whats-next/","section":"Projects","summary":"whats-next # A reworked utility to either guide an Arch Linux install, or a tool to help you learn the commands in order.","title":"arch-installer-whats-next"},{"content":" zerOS # an os? based on arch\n","date":"21 July 2020","permalink":"/projects/zeros/","section":"Projects","summary":"zerOS # an os?","title":"zerOS"},{"content":"404: Not Found\n","date":"20 July 2020","permalink":"/projects/dwm/","section":"Projects","summary":"404: Not Found","title":"dwm"},{"content":"","date":"20 July 2020","permalink":"/tags/ruby/","section":"Tags","summary":"","title":"Ruby"},{"content":" A simple git package manager written in ruby\nInstall # run sh impulse.build\nHelp # run sapphire -h\n","date":"20 July 2020","permalink":"/projects/sapphire/","section":"Projects","summary":"A simple git package manager written in ruby","title":"sapphire"},{"content":" A simple package manager written in ruby\nInstall # run sh impulse.build\nHelp # run amethyst -h\n","date":"14 July 2020","permalink":"/projects/amethyst/","section":"Projects","summary":"A simple package manager written in ruby","title":"amethyst"},{"content":" nxyso_api # A definition and search API for a language called Nysjomon made using FastApi\nRequirements: # fastapi uvicorn Running: # uvicorn main:app --reload\n","date":"12 July 2020","permalink":"/projects/nxyso_api/","section":"Projects","summary":"nxyso_api # A definition and search API for a language called Nysjomon made using FastApi","title":"nxyso_api"},{"content":" public-pgp-keys # ","date":"10 July 2020","permalink":"/projects/public-pgp-keys/","section":"Projects","summary":" public-pgp-keys # ","title":"public-pgp-keys"},{"content":" vim-selection-test # vim-selection-test.vim - A test of calling a function with selected text\n","date":"9 July 2020","permalink":"/projects/vim-selection-test/","section":"Projects","summary":"vim-selection-test # vim-selection-test.","title":"vim-selection-test"},{"content":" vim-snow-script-syntax # ","date":"9 July 2020","permalink":"/projects/vim-snow-script-syntax/","section":"Projects","summary":" vim-snow-script-syntax # ","title":"vim-snow-script-syntax"},{"content":" vim-character-creator # character-creator only has dice rolling functionality as of 0.2 with lots more features coming soon. # Role: role a dice in vim # Example: run \u0026lsquo;:echo Character_Creator_Roll(\u0026ldquo;1d8\u0026rdquo;)\u0026rsquo; # Options: have the number of dice before the \u0026rsquo;d\u0026rsquo; and the dice value after # ","date":"8 July 2020","permalink":"/projects/vim-character-creator/","section":"Projects","summary":"vim-character-creator # character-creator only has dice rolling functionality as of 0.","title":"vim-character-creator"},{"content":" valueourminds_website # ","date":"7 July 2020","permalink":"/projects/valueourminds_website/","section":"Projects","summary":" valueourminds_website # ","title":"valueourminds_website"},{"content":" bspswallow # Adds functionality provided by the dwm \u0026ldquo;swallow\u0026rdquo; patch to bspwm.\nDependencies # bspwm (obviously) xprop Installation # Add two files to ~/.config/bspwm\nnoswallow - list of classes of windows that you don\u0026rsquo;t want to swallow the terminal\nterminals - list of classes of terminals that you want to be swallowed\nIf a class isn\u0026rsquo;t available (such as with xev) then the command of origin can be used.\n(example files are included in \u0026ldquo;examples\u0026rdquo;)\nPlace bspswallow into your PATH and add the following line to your bspwmrc.\n(ps x | grep bspswallow | grep -v grep) || bspswallow Now just restart bspwm and you\u0026rsquo;re good to go.\nKnown Issues # Incompatability with LibreOffice due to it having a splash screen and spawning multiple windows, use \u0026ndash;no-logo when launching and turn off \u0026ldquo;Tip of the day\u0026rdquo; in order to avoid this issue. ","date":"1 July 2020","permalink":"/projects/bspswallow/","section":"Projects","summary":"bspswallow # Adds functionality provided by the dwm \u0026ldquo;swallow\u0026rdquo; patch to bspwm.","title":"bspswallow"},{"content":" break # When automating your workflow, build systems and ci are some of the most important parts. Break is analogous to a build system, and test suite. Break, will insure your tests fail by irreversibly destroying your executables and source code according to your breakfile. It will also prepare the source code by breaking the files before tests. Break is blazing fast, and it\u0026rsquo;s unique functionality allows for a streamlined workflow.\nAbout # break is a Unix command-line utility for destruction of programs.\nThe break utility requires a file in the current directory called GNObreakfile, breakfile, or Breakfile.\nUse # If you have a breakfile, simply type ./break or python3 break into command-line to start the irreversible, randomized destruction of your hard-written programs.\nBreakfile # Breakfiles have the keyword break and a name of a file\nbreak filename.c There is another example at ./example/Breakfile\n","date":"24 June 2020","permalink":"/projects/break/","section":"Projects","summary":"break # When automating your workflow, build systems and ci are some of the most important parts.","title":"break"},{"content":" nxyso_definition_bot # ","date":"24 June 2020","permalink":"/projects/nxyso_definition_bot/","section":"Projects","summary":" nxyso_definition_bot # ","title":"nxyso_definition_bot"},{"content":" snow_script # A macro language with low level macros like if statements, jumps, and memory control and high level futures like memory dump, type, type casting, push, and calc\nDependencies # pip3 install -r requirements.txt\nInstall # sudo ./impulse.build\nRunning # ssc filename.ssc\nTesting # Configure # python3 conftest.py\nTest # pytest\nSyntax # view syntax.txt for macros and uses\n","date":"19 June 2020","permalink":"/projects/snow_script/","section":"Projects","summary":"snow_script # A macro language with low level macros like if statements, jumps, and memory control and high level futures like memory dump, type, type casting, push, and calc","title":"snow_script"},{"content":" new-website # ","date":"15 June 2020","permalink":"/projects/new-website/","section":"Projects","summary":" new-website # ","title":"new-website"},{"content":" is_254_or_larger # About # An npm package to determine whether a number is 254 or larger\nView the official npm package # https://www.npmjs.com/package/is_254_or_larger\n","date":"14 June 2020","permalink":"/projects/is_254_or_larger/","section":"Projects","summary":"is_254_or_larger # About # An npm package to determine whether a number is 254 or larger","title":"is_254_or_larger"},{"content":" wallpaper_downloader # ","date":"3 June 2020","permalink":"/projects/wallpaper_downloader/","section":"Projects","summary":" wallpaper_downloader # ","title":"wallpaper_downloader"},{"content":" lightBot # A Discord bot with control of GPIO pins. These pins were connected to a few relays that controlled lights in my room.\n","date":"27 May 2020","permalink":"/projects/lightbot/","section":"Projects","summary":"lightBot # A Discord bot with control of GPIO pins.","title":"lightBot"},{"content":" Galrux # A Discord bot designed for adding music to a queue. This was specifically requested for a server.\n","date":"27 May 2020","permalink":"/projects/vorahk/","section":"Projects","summary":"Galrux # A Discord bot designed for adding music to a queue.","title":"Vorahk"},{"content":" claculator # adds numbers together\nA joke if you couldn\u0026rsquo;t tell\n","date":"23 May 2020","permalink":"/projects/claculator/","section":"Projects","summary":"claculator # adds numbers together","title":"claculator"},{"content":" Landing Page # Clone the library to your computer\ngit clone git@github.com:Camerooooon/landing-page.git Now set the homepage of your browser to the location of the index.html file on your computer. You may need to use an extention for chrome. Inorder to control which search engine you want to use you must use the left and right arrow keys.\nThis idea was inspired by JakeRoggenbuck\u0026rsquo;s landing page repo.\n","date":"20 May 2020","permalink":"/projects/firefox-landing-page-1/","section":"Projects","summary":"Landing Page # Clone the library to your computer","title":"firefox-landing-page-1"},{"content":" calcLex # A simple lexer using a yacc.\n","date":"24 April 2020","permalink":"/projects/calclex/","section":"Projects","summary":"calcLex # A simple lexer using a yacc.","title":"calcLex"},{"content":" landing-page # The most simple landing page possible\n","date":"23 April 2020","permalink":"/projects/firefox-landing-page/","section":"Projects","summary":"landing-page # The most simple landing page possible","title":"firefox-landing-page"},{"content":" hash-colorer # View hashes as colors # Use # Pipe something to hashc\ne.g. sha256sum file | hashc\nInstall # Run ./install.sh as root\n","date":"17 April 2020","permalink":"/projects/hash-colorer/","section":"Projects","summary":"hash-colorer # View hashes as colors # Use # Pipe something to hashc","title":"hash-colorer"},{"content":" impulse # Install # Run ./impulse.build with root permissions\nUse # Read the man page\n","date":"17 April 2020","permalink":"/projects/impulse/","section":"Projects","summary":"impulse # Install # Run .","title":"impulse"},{"content":" n-rus-bot # Discord bot for the e🅱️ic server\n","date":"16 April 2020","permalink":"/projects/n-rus-bot/","section":"Projects","summary":"n-rus-bot # Discord bot for the e🅱️ic server","title":"n-rus-bot"},{"content":" 1678 Server 2019-2020 # Run setup_environment.py when you clone the repository.\nThis will install a virtual python environment in the main project directory. It will then install the external dependencies into this environment from PyPI using pip. (This will NOT install any non-python dependencies such as MongoDB, as the process for that depends on your distribution. You will have to do that manually).\nWhen testing from the command line, remember to activate the virtual environment (source .venv/bin/activate on bash/zsh). Instructions for other shells, along with more in-depth information about Python virtual environments, can be found here.\n","date":"15 April 2020","permalink":"/projects/server-public/","section":"Projects","summary":"1678 Server 2019-2020 # Run setup_environment.","title":"server-public"},{"content":" ned-bot # A discord bot that reacts with 🇩 when any users react with 🇳 and 🇪. It likes spelling its name. # Requirements\nRequires discord.py https://pypi.org/project/discord.py/ Installation\ninstall discord.py through pip via \u0026ldquo;pip install discord.py\u0026rdquo; To start the bot run \u0026ldquo;python3 ned-bot.py\u0026rdquo;\n","date":"7 April 2020","permalink":"/projects/ned-bot/","section":"Projects","summary":"ned-bot # A discord bot that reacts with 🇩 when any users react with 🇳 and 🇪.","title":"ned-bot"},{"content":" Polybot-2 # ","date":"4 April 2020","permalink":"/projects/polybot-2/","section":"Projects","summary":" Polybot-2 # ","title":"Polybot-2"},{"content":" sort-vis # Visualizes sorting alogithims using p5 js # Requirements\nRequires a local node server Install/Setup\nInstall node.js by running \u0026ldquo;npm install http-server\u0026rdquo; Then while in the sort-vis folder run \u0026ldquo;http-server\u0026rdquo; Finally connect to \u0026ldquo;http://localhost:8080/\u0026rdquo; and open sort-vis.html ","date":"29 March 2020","permalink":"/projects/sort-vis/","section":"Projects","summary":"sort-vis # Visualizes sorting alogithims using p5 js # Requirements","title":"sort-vis"},{"content":" Cruzlang # Cruz Lang is a simple explicit static typed language. The compiler written in python is called shark and uses ply.lex for lexical analysis. It has 5 primitive data types:\nData types # byte one byte of data char one ASCII character int a 32 bit integer prec a number with a decimal point of 4 bytes ptr a 4 byte pointer address Other key words # fun the function key word while a while loop write output as stdout read input as stdin with uses external files if do something if condition true elif else do something if condition true else else do something takes place for parameters and types return a return returns tells function what type to return Here is the syntax:\nSyntax\n","date":"26 March 2020","permalink":"/projects/cruz-lang/","section":"Projects","summary":"Cruzlang # Cruz Lang is a simple explicit static typed language.","title":"cruz-lang"},{"content":" Terbot-2 # ","date":"25 March 2020","permalink":"/projects/terbot-2/","section":"Projects","summary":" Terbot-2 # ","title":"Terbot-2"},{"content":"Source code for an RPG game To run the game, type \u0026ldquo;java -jar asmura.jar\u0026rdquo; in terminal The application opens with a black background (menu screen). To enter \u0026ldquo;main game\u0026rdquo;, press enter (white background), and press q to toggle between main game and escaped screen (gray background).\n","date":"24 March 2020","permalink":"/projects/asmura/","section":"Projects","summary":"Source code for an RPG game To run the game, type \u0026ldquo;java -jar asmura.","title":"asmura"},{"content":" Route-Finding # ","date":"24 March 2020","permalink":"/projects/route-finding/","section":"Projects","summary":" Route-Finding # ","title":"Route-Finding"},{"content":" fork-craft # ","date":"21 March 2020","permalink":"/projects/fork-craft/","section":"Projects","summary":" fork-craft # ","title":"fork-craft"},{"content":" Covid19-cases-tui # Simple tui for seeing Covid-19 cases # This script uses herokuapp\u0026rsquo;s api for data https://covid2019-api.herokuapp.com/\n","date":"20 March 2020","permalink":"/projects/covid19-cases-tui/","section":"Projects","summary":"Covid19-cases-tui # Simple tui for seeing Covid-19 cases # This script uses herokuapp\u0026rsquo;s api for data https://covid2019-api.","title":"Covid19-cases-tui"},{"content":" Dictionary-tui # A tui for dictionaryapi.com to search for Synonyms and Antonyms\nApi Key # First go to https://dictionaryapi.com/ Then follow their instructions to get an api key\nSetup # Create a file named api_key.py and in it make a var called key, set it to \u0026ldquo;your api key\u0026rdquo;\nOutput example # Word: fast fast Synonyms apace briskly chop-chop double-quick fleetly full tilt hastily hell-for-leather hot lickety-split posthaste presto pronto quick quickly rapidly snappily soon speedily swift swiftly Antonyms slow slowly ","date":"19 March 2020","permalink":"/projects/dictionary-tui/","section":"Projects","summary":"Dictionary-tui # A tui for dictionaryapi.","title":"Dictionary-tui"},{"content":" dmenuSpecialCharacterCopier # Copy emoji from dmenu by searching their name\n","date":"19 March 2020","permalink":"/projects/dmenuspecialcharactercopier/","section":"Projects","summary":"dmenuSpecialCharacterCopier # Copy emoji from dmenu by searching their name","title":"dmenuSpecialCharacterCopier"},{"content":" learningMachineLearning # ","date":"19 March 2020","permalink":"/projects/learningmachinelearning/","section":"Projects","summary":" learningMachineLearning # ","title":"learningMachineLearning"},{"content":" firefoxThemes # ","date":"17 March 2020","permalink":"/projects/firefoxthemes/","section":"Projects","summary":" firefoxThemes # ","title":"firefoxThemes"},{"content":" Lan-Website # A simple website for my local network to share files\n","date":"17 March 2020","permalink":"/projects/lan-website/","section":"Projects","summary":"Lan-Website # A simple website for my local network to share files","title":"Lan-Website"},{"content":"","date":"17 March 2020","permalink":"/tags/php/","section":"Tags","summary":"","title":"PHP"},{"content":"","date":"16 March 2020","permalink":"/tags/brainf/","section":"Tags","summary":"","title":"Brainf"},{"content":" BrainF # Turn jaker into hello with\n\u0026gt;,--.\u0026gt;,++++.\u0026gt;,+.\u0026gt;,+++++++.\u0026gt;,---. Caeser cipher with\n\u0026gt;+[\u0026gt;,+++.] Print a lot of weird stuff with a loop\n\u0026gt;+[+.] ","date":"16 March 2020","permalink":"/projects/brainf/","section":"Projects","summary":"BrainF # Turn jaker into hello with","title":"BrainF"},{"content":"404: Not Found\n","date":"16 March 2020","permalink":"/projects/javapong/","section":"Projects","summary":"404: Not Found","title":"JavaPong"},{"content":" tbaTestScripts # TBA API key # Create an api key API docs Make a api_key.py file Add request_headers = {\u0026lsquo;X-TBA-Auth-Key\u0026rsquo;: \u0026lsquo;Your Key Here\u0026rsquo;} Uses # averageBumperColor.py # In averageBumperColor.py the url event/{event_code}/matches/simple is used. Example of the event_code could be 2020caln. The feilds used are [\u0026quot;alliances\u0026quot;][\u0026quot;blue\u0026quot;][\u0026quot;team_keys\u0026quot;] and [\u0026quot;alliances\u0026quot;][\u0026quot;red\u0026quot;][\u0026quot;team_keys\u0026quot;] The data it returns is a json of the average bumper/alliance color used\n{ \u0026#39;frc6560\u0026#39;: -4, \u0026#39;frc115\u0026#39;: 0, \u0026#39;frc114\u0026#39;: -6, \u0026#39;frc1678\u0026#39;: -6 } matchColorMaker.py # In matchColorMaker.py the url event/{event_key}/matches/simple is used. Example of the event_key could be 2020caln. The feilds used are [\u0026quot;alliances\u0026quot;][\u0026quot;blue\u0026quot;][\u0026quot;team_keys\u0026quot;] and [\u0026quot;alliances\u0026quot;][\u0026quot;red\u0026quot;][\u0026quot;team_keys\u0026quot;] The data it prints is two lists with the red team and then the blue team in match\n[\u0026#39;frc1678\u0026#39;, \u0026#39;frc973\u0026#39;, \u0026#39;frc4\u0026#39;] [\u0026#39;frc6560\u0026#39;, \u0026#39;frc115\u0026#39;, \u0026#39;frc114\u0026#39;] [\u0026#39;frc1678\u0026#39;, \u0026#39;frc973\u0026#39;, \u0026#39;frc4\u0026#39;] [\u0026#39;frc6560\u0026#39;, \u0026#39;frc115\u0026#39;, \u0026#39;frc114\u0026#39;] [\u0026#39;frc1678\u0026#39;, \u0026#39;frc973\u0026#39;, \u0026#39;frc5089\u0026#39;] [\u0026#39;frc6560\u0026#39;, \u0026#39;frc115\u0026#39;, \u0026#39;frc114\u0026#39;] [\u0026#39;frc1678\u0026#39;, \u0026#39;frc973\u0026#39;, \u0026#39;frc4\u0026#39;] [\u0026#39;frc359\u0026#39;, \u0026#39;frc1388\u0026#39;, \u0026#39;frc3309\u0026#39;] teamsMapView.py # In teamsMapView.py the url event/{tba_key}/teams is used. Example of the event_code could be 2020mosl. The feilds used are team[\u0026quot;nickname\u0026quot;] and team[\u0026quot;state_prov\u0026quot;] The data it prints are the teams and their state/province\nMetool Brigade, Illinois RAVEN Robotics, Missouri Tech Heads, Missouri Citrus Circuits, California teamsViewer.py # In teamsViewer.py the url event/{tba_key}/teams is used. Example of the event_code could be 2020mosl. The feilds used are team[\u0026quot;key\u0026quot;], team[\u0026quot;nickname\u0026quot;], team[\u0026quot;city\u0026quot;], team[\u0026quot;state_prov\u0026quot;], and team[\u0026quot;rookie_year\u0026quot;] The data it prints are the teams and a few facts about them\nTeam frc2978 Cavaliers is from Saint Louis, Missouri and their rookie year was 2009 Team frc302 The Dragons is from Lake Orion, Michigan and their rookie year was 1999 Team frc3330 System of the Corn is from Saint Charles, Missouri and their rookie year was 2010 Team frc3397 Robolions is from Saint Louis, Missouri and their rookie year was 2010 teamsSpecificInfo.py # In teamsSpecificInfo.py the url team/{team_key} is used. Example of the team_key could be frc253. The data it prints all data about the given team\n{\u0026#39;address\u0026#39;: None, \u0026#39;city\u0026#39;: \u0026#39;Millbrae\u0026#39;, \u0026#39;country\u0026#39;: \u0026#39;USA\u0026#39;, \u0026#39;gmaps_place_id\u0026#39;: None, \u0026#39;gmaps_url\u0026#39;: None, \u0026#39;home_championship\u0026#39;: {\u0026#39;2020\u0026#39;: \u0026#39;Houston\u0026#39;}, \u0026#39;key\u0026#39;: \u0026#39;frc253\u0026#39;, \u0026#39;lat\u0026#39;: None, \u0026#39;lng\u0026#39;: None, \u0026#39;location_name\u0026#39;: None, \u0026#39;motto\u0026#39;: None, \u0026#39;name\u0026#39;: \u0026#39;Caccia Plumbing Inc/Millbrae Lions Club/Bishop Wisecarver/San Mateo Union High School District/Google/FIRST NorCal/Lockheed Martin/Santosh Abraham/SolidWorks/Comcast NBCUniversal/GATE/Intuitive Foundation/Millbrae Leos Club/San Carlos Kiwanis Club/Upward Credit Union/AGC Acupuncture Clinic/Rotary Club of Millbrae/Millbrae T4U/Tea Link/San Mateo Credit Union/Numis International Inc\u0026amp;Mills High School\u0026#39;, \u0026#39;nickname\u0026#39;: \u0026#39;Boba Bots\u0026#39;, \u0026#39;postal_code\u0026#39;: \u0026#39;94030\u0026#39;, \u0026#39;rookie_year\u0026#39;: 1999, \u0026#39;school_name\u0026#39;: \u0026#39;Mills High School\u0026#39;, \u0026#39;state_prov\u0026#39;: \u0026#39;California\u0026#39;, \u0026#39;team_number\u0026#39;: 253, \u0026#39;website\u0026#39;: \u0026#39;https://millsroboticsteam253.com\u0026#39;} ","date":"16 March 2020","permalink":"/projects/tbatestscripts/","section":"Projects","summary":"tbaTestScripts # TBA API key # Create an api key API docs Make a api_key.","title":"tbaTestScripts"},{"content":" Table of Contents # What is openpilot? Integration with Stock Features Supported Hardware Supported Cars Community Maintained Cars and Features Installation Instructions Limitations of openpilot ALC and LDW Limitations of openpilot ACC and FCW Limitations of openpilot DM User Data and comma Account Safety and Testing Testing on PC Community and Contributing Directory Structure Licensing What is openpilot? # openpilot is an open source driver assistance system. Currently, openpilot performs the functions of Adaptive Cruise Control (ACC), Automated Lane Centering (ALC), Forward Collision Warning (FCW) and Lane Departure Warning (LDW) for a growing variety of supported car makes, models and model years. In addition, while openpilot is engaged, a camera based Driver Monitoring (DM) feature alerts distracted and asleep drivers.\nIntegration with Stock Features # In all supported cars:\nStock Lane Keep Assist (LKA) and stock ALC are replaced by openpilot ALC, which only functions when openpilot is engaged by the user. Stock LDW is replaced by openpilot LDW. Additionally, on specific supported cars (see ACC column in supported cars):\nStock ACC is replaced by openpilot ACC. openpilot FCW operates in addition to stock FCW. openpilot should preserve all other vehicle\u0026rsquo;s stock features, including, but are not limited to: FCW, Automatic Emergency Braking (AEB), auto high-beam, blind spot warning, and side collision warning.\nSupported Hardware # At the moment, openpilot supports the EON DevKit and the comma two. A car harness is recommended to connect the EON or comma two to the car. In the future, we\u0026rsquo;d like to support other platforms as well, like gaming PCs.\nSupported Cars # Make Model (US Market Reference) Supported Package ACC No ACC accel below No ALC below Acura ILX 2016-18 AcuraWatch Plus openpilot 25mph6 25mph Acura RDX 2016-18 AcuraWatch Plus openpilot 25mph6 12mph Chrysler Pacifica 2017-18 Adaptive Cruise Stock 0mph 9mph Chrysler Pacifica Hybrid 2017-18 Adaptive Cruise Stock 0mph 9mph Chrysler Pacifica Hybrid 2019-20 Adaptive Cruise Stock 0mph 39mph Honda Accord 2018-19 All Stock 0mph 3mph Honda Accord Hybrid 2018-19 All Stock 0mph 3mph Honda Civic Hatchback 2017-19 Honda Sensing Stock 0mph 12mph Honda Civic Sedan/Coupe 2016-18 Honda Sensing openpilot 0mph 12mph Honda Civic Sedan/Coupe 2019 Honda Sensing Stock 0mph 2mph4 Honda CR-V 2015-16 Touring openpilot 25mph6 12mph Honda CR-V 2017-19 Honda Sensing Stock 0mph 12mph Honda CR-V Hybrid 2017-2019 Honda Sensing Stock 0mph 12mph Honda Fit 2018-19 Honda Sensing openpilot 25mph6 12mph Honda Odyssey 2018-20 Honda Sensing openpilot 25mph6 0mph Honda Passport 2019 All openpilot 25mph6 12mph Honda Pilot 2016-18 Honda Sensing openpilot 25mph6 12mph Honda Pilot 2019 All openpilot 25mph6 12mph Honda Ridgeline 2017-19 Honda Sensing openpilot 25mph6 12mph Hyundai Elantra 2017-191 SCC + LKAS Stock 19mph 34mph Hyundai Genesis 20181 All Stock 19mph 34mph Hyundai Santa Fe 20191 All Stock 0mph 0mph Jeep Grand Cherokee 2016-18 Adaptive Cruise Stock 0mph 9mph Jeep Grand Cherokee 2019 Adaptive Cruise Stock 0mph 39mph Kia Optima 20191 SCC + LKAS Stock 0mph 0mph Kia Sorento 20181 All Stock 0mph 0mph Kia Stinger 20181 SCC + LKAS Stock 0mph 0mph Lexus CT Hybrid 2017-18 All Stock5 0mph 0mph Lexus ES 2019 All openpilot 0mph 0mph Lexus ES Hybrid 2019 All openpilot 0mph 0mph Lexus IS 2017-2019 All Stock 22mph 0mph Lexus IS Hybrid 2017 All Stock 0mph 0mph Lexus NX Hybrid 2018 All Stock5 0mph 0mph Lexus RX 2016-17 All Stock5 0mph 0mph Lexus RX 2020 All openpilot 0mph 0mph Lexus RX Hybrid 2016-19 All Stock5 0mph 0mph Subaru Crosstrek 2018-19 EyeSight Stock 0mph 0mph Subaru Impreza 2019-20 EyeSight Stock 0mph 0mph Toyota Avalon 2016 TSS-P Stock5 20mph6 0mph Toyota Avalon 2017-18 All Stock5 20mph6 0mph Toyota Camry 2018-19 All Stock 0mph2 0mph Toyota Camry Hybrid 2018-19 All Stock 0mph2 0mph Toyota C-HR 2017-19 All Stock 0mph 0mph Toyota C-HR Hybrid 2017-19 All Stock 0mph 0mph Toyota Corolla 2017-19 All Stock5 20mph6 0mph Toyota Corolla 2020 All openpilot 0mph 0mph Toyota Corolla Hatchback 2019-20 All openpilot 0mph 0mph Toyota Corolla Hybrid 2020 All openpilot 0mph 0mph Toyota Highlander 2017-19 All Stock5 0mph 0mph Toyota Highlander Hybrid 2017-19 All Stock5 0mph 0mph Toyota Highlander 2020 All openpilot 0mph 0mph Toyota Prius 2016 TSS-P Stock5 0mph 0mph Toyota Prius 2017-19 All Stock5 0mph 0mph Toyota Prius Prime 2017-20 All Stock5 0mph 0mph Toyota Rav4 2016 TSS-P Stock5 20mph6 0mph Toyota Rav4 2017-18 All Stock5 20mph6 0mph Toyota Rav4 2019 All openpilot 0mph 0mph Toyota Rav4 Hybrid 2016 TSS-P Stock5 0mph 0mph Toyota Rav4 Hybrid 2017-18 All Stock5 0mph 0mph Toyota Rav4 Hybrid 2019-20 All openpilot 0mph 0mph Toyota Sienna 2018 All Stock5 0mph 0mph Volkswagen Golf 2016-193 Driver Assistance Stock 0mph 0mph 1Requires a panda and open sourced Hyundai giraffe, designed for the 2019 Sante Fe; pinout may differ for other Hyundai and Kia models. 228mph for Camry 4CYL L, 4CYL LE and 4CYL SE which don\u0026rsquo;t have Full-Speed Range Dynamic Radar Cruise Control. 3Requires a custom connector for the car harness 42019 Honda Civic 1.6L Diesel Sedan does not have ALC below 12mph. Community Maintained Cars and Features # Make Model (US Market Reference) Supported Package ACC No ACC accel below No ALC below Buick Regal 20187 Adaptive Cruise openpilot 0mph 7mph Cadillac ATS 20187 Adaptive Cruise openpilot 0mph 7mph Chevrolet Malibu 20177 Adaptive Cruise openpilot 0mph 7mph Chevrolet Volt 2017-187 Adaptive Cruise openpilot 0mph 7mph GMC Acadia Denali 20187 Adaptive Cruise openpilot 0mph 7mph Holden Astra 20177 Adaptive Cruise openpilot 0mph 7mph 5When disconnecting the Driver Support Unit (DSU), openpilot ACC will replace stock ACC. For DSU locations, see Toyota Wiki page. NOTE: disconnecting the DSU disables Automatic Emergency Braking (AEB). 6 Comma Pedal is used to provide stop-and-go capability to some of the openpilot-supported cars that don\u0026rsquo;t currently support stop-and-go. Here is how to build a Comma Pedal. NOTE: The Comma Pedal is not officially supported by comma. 7Requires a panda and community built giraffe. NOTE: disconnecting the ASCM disables Automatic Emergency Braking (AEB). Community Maintained Cars and Features are not verified by comma to meet our safety model. Be extra cautious using them. They are only available after enabling the toggle in Settings-\u0026gt;Developer-\u0026gt;Enable Community Features.\nInstallation Instructions # Install openpilot on a EON by entering https://openpilot.comma.ai during the installer setup.\nFollow this video instructions to properly mount the EON on the windshield. Note: openpilot features an automatic pose calibration routine and openpilot performance should not be affected by small pitch and yaw misalignments caused by imprecise EON mounting.\nBefore placing the device on your windshield, check the state and local laws and ordinances where you drive. Some state laws prohibit or restrict the placement of objects on the windshield of a motor vehicle.\nYou will be able to engage openpilot after reviewing the onboarding screens and finishing the calibration procedure.\nLimitations of openpilot ALC and LDW # openpilot ALC and openpilot LDW do not automatically drive the vehicle or reduce the amount of attention that must be paid to operate your vehicle. The driver must always keep control of the steering wheel and be ready to correct the openpilot ALC action at all times.\nWhile changing lanes, openpilot is not capable of looking next to you or checking your blind spot. Only nudge the wheel to initiate a lane change after you have confirmed it\u0026rsquo;s safe to do so.\nMany factors can impact the performance of openpilot ALC and openpilot LDW, causing them to be unable to function as intended. These include, but are not limited to:\nPoor visibility (heavy rain, snow, fog, etc.) or weather conditions that may interfere with sensor operation. The road facing camera is obstructed, covered or damaged by mud, ice, snow, etc. Obstruction caused by applying excessive paint or adhesive products (such as wraps, stickers, rubber coating, etc.) onto the vehicle. The EON is mounted incorrectly. When in sharp curves, like on-off ramps, intersections etc\u0026hellip;; openpilot is designed to be limited in the amount of steering torque it can produce. In the presence of restricted lanes or construction zones. When driving on highly banked roads or in presence of strong cross-wind. Extremely hot or cold temperatures. Bright light (due to oncoming headlights, direct sunlight, etc.). Driving on hills, narrow, or winding roads. The list above does not represent an exhaustive list of situations that may interfere with proper operation of openpilot components. It is the driver\u0026rsquo;s responsibility to be in control of the vehicle at all times.\nLimitations of openpilot ACC and FCW # openpilot ACC and openpilot FCW are not systems that allow careless or inattentive driving. It is still necessary for the driver to pay close attention to the vehicle’s surroundings and to be ready to re-take control of the gas and the brake at all times.\nMany factors can impact the performance of openpilot ACC and openpilot FCW, causing them to be unable to function as intended. These include, but are not limited to:\nPoor visibility (heavy rain, snow, fog, etc.) or weather conditions that may interfere with sensor operation. The road facing camera or radar are obstructed, covered, or damaged by mud, ice, snow, etc. Obstruction caused by applying excessive paint or adhesive products (such as wraps, stickers, rubber coating, etc.) onto the vehicle. The EON is mounted incorrectly. Approaching a toll booth, a bridge or a large metal plate. When driving on roads with pedestrians, cyclists, etc\u0026hellip; In presence of traffic signs or stop lights, which are not detected by openpilot at this time. When the posted speed limit is below the user selected set speed. openpilot does not detect speed limits at this time. In presence of vehicles in the same lane that are not moving. When abrupt braking maneuvers are required. openpilot is designed to be limited in the amount of deceleration and acceleration that it can produce. When surrounding vehicles perform close cut-ins from neighbor lanes. Driving on hills, narrow, or winding roads. Extremely hot or cold temperatures. Bright light (due to oncoming headlights, direct sunlight, etc.). Interference from other equipment that generates radar waves. The list above does not represent an exhaustive list of situations that may interfere with proper operation of openpilot components. It is the driver\u0026rsquo;s responsibility to be in control of the vehicle at all times.\nLimitations of openpilot DM # openpilot DM should not be considered an exact measurements of the status of alertness of the driver.\nMany factors can impact the performance of openpilot DM, causing it to be unable to function as intended. These include, but are not limited to:\nLow light conditions, such as driving at night or in dark tunnels. Bright light (due to oncoming headlights, direct sunlight, etc.). The driver face is partially or completely outside field of view of the driver facing camera. Right hand driving vehicles. The driver facing camera is obstructed, covered, or damaged. The list above does not represent an exhaustive list of situations that may interfere with proper operation of openpilot components. A driver should not rely on openpilot DM to assess their level of attention.\nUser Data and comma Account # By default, openpilot uploads the driving data to our servers. You can also access your data by pairing with the comma connect app ( iOS, Android). We use your data to train better models and improve openpilot for everyone.\nopenpilot is open source software: the user is free to disable data collection if they wish to do so.\nopenpilot logs the road facing camera, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs. The driver facing camera is only logged if you explicitly opt-in in settings. The microphone is not recorded.\nBy using openpilot, you agree to our Privacy Policy. You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma for the use of this data.\nSafety and Testing # openpilot observes ISO26262 guidelines, see SAFETY.md for more detail. openpilot has software in the loop tests that run on every commit. The safety model code lives in panda and is written in C, see code rigor for more details. panda has software in the loop safety tests. Internally, we have a hardware in the loop Jenkins test suite that builds and unit tests the various processes. panda has additional hardware in the loop tests. We run the latest openpilot in a testing closet containing 10 EONs continuously replaying routes. Testing on PC # Check out the tools directory in master: lots of tools you can use to replay driving data, test and develop openpilot from your pc.\nCommunity and Contributing # openpilot is developed by comma and by users like you. We welcome both pull requests and issues on GitHub. Bug fixes and new car ports are encouraged.\nYou can add support for your car by following guides we have written for Brand and Model ports. Generally, a car with adaptive cruise control and lane keep assist is a good candidate. Join our Discord to discuss car ports: most car makes have a dedicated channel.\nWant to get paid to work on openpilot? comma is hiring. We also have a bounty program.\nAnd follow us on Twitter.\nDirectory Structure # . ├── apk # The apk files used for the UI ├── cereal # The messaging spec and libs used for all logs on EON ├── common # Library like functionality we've developed here ├── installer/updater # Manages auto-updates of openpilot ├── opendbc # Files showing how to interpret data from cars ├── panda # Code used to communicate on CAN ├── phonelibs # Libraries used on EON ├── pyextra # Libraries used on EON └── selfdrive # Code needed to drive the car ├── assets # Fonts and images for UI ├── athena # Allows communication with the app ├── boardd # Daemon to talk to the board ├── camerad # Driver to capture images from the camera sensors ├── car # Car specific code to read states and control actuators ├── common # Shared C/C++ code for the daemons ├── controls # Perception, planning and controls ├── debug # Tools to help you debug and do car ports ├── locationd # Soon to be home of precise location ├── logcatd # Android logcat as a service ├── loggerd # Logger and uploader of car data ├── modeld # Driving and monitoring model runners ├── proclogd # Logs information from proc ├── sensord # IMU / GPS interface code ├── tests # Unit tests, system tests and a car simulator └── ui # The UI To understand how the services interact, see cereal/service_list.yaml.\nLicensing # openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.\nAny user of this software shall indemnify and hold harmless comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneys’ fees and costs) which arise out of, relate to or result from any use of this software by user.\nTHIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT. YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS. NO WARRANTY EXPRESSED OR IMPLIED.\n","date":"27 February 2020","permalink":"/projects/openpilot/","section":"Projects","summary":"Table of Contents # What is openpilot?","title":"openpilot"},{"content":" Apollo-11 # 🎌 Bahasa Indonesia, Català, Deutsch, English, Español, Français, Italiano, Nederlands, Polski, Português, Română, Tiếng Việt, Türkçe, Русский, العربية, فارسی, हिंदी, বাংলা, မြန်မာ, 日本, 正體中文, 简体中文, 한국어\nOriginal Apollo 11 guidance computer (AGC) source code for Command Module (Comanche055) and Lunar Module (Luminary099). Digitized by the folks at Virtual AGC and MIT Museum. The goal is to be a repo for the original Apollo 11 source code. As such, PRs are welcome for any issues identified between the transcriptions in this repository and the original source scans for Luminary 099 and Comanche 055, as well as any files I may have missed.\nContributing # Please read CONTRIBUTING.md before opening a pull request.\nCompiling # If you are interested in compiling the original source code, check out Virtual AGC.\nAttribution # Copyright Public domain Comanche055 Part of the source code for Colossus 2A, the Command Module\u0026rsquo;s (CM) Apollo Guidance Computer (AGC) for Apollo 11\nAssemble revision 055 of AGC program Comanche by NASA\n2021113-051. 10:28 APR. 1, 1969 Luminary099 Part of the source code for Luminary 1A, the Lunar Module\u0026rsquo;s (LM) Apollo Guidance Computer (AGC) for Apollo 11\nAssemble revision 001 of AGC program LYM99 by NASA\n2021112-061. 16:27 JUL. 14, 1969 Assembler yaYUL Contact Ron Burkey info@sandroid.org Website www.ibiblio.org/apollo Digitalization This source code has been transcribed or otherwise adapted from digitized images of a hardcopy from the MIT Museum. The digitization was performed by Paul Fjeld, and arranged for by Deborah Douglas of the Museum. Many thanks to both. Contract and Approvals # Derived from CONTRACT_AND_APPROVALS.agc\nThis AGC program shall also be referred to as Colossus 2A.\nThis program is intended for use in the CM as specified in report R-577. This program was prepared under DSR project 55-23870, sponsored by the Manned Spacecraft Center of The National Aeronautics and Space Administration through contract NAS 9-4065 with the Instrumentation Laboratory, Massachusetts Institute of Technology, Cambridge, Mass.\nSubmitted by Role Date Margaret H. Hamilton Colossus Programming Leader\nApollo Guidance and Navigation 28 Mar 69 Approved by Role Date Daniel J. Lickly Director, Mission Program Development\nApollo Guidance and Navigation Program 28 Mar 69 Fred H. Martin Colossus Project Manager\nApollo Guidance and Navigation Program 28 Mar 69 Norman E. Sears Director, Mission Development\nApollo Guidance and Navigation Program 28 Mar 69 Richard H. Battin Director, Mission Development\nApollo Guidance and Navigation Program 28 Mar 69 David G. Hoag Director\nApollo Guidance and Navigation Program 28 Mar 69 Ralph R. Ragan Deputy Director\nInstrumentation Laboratory 28 Mar 69 ","date":"20 February 2020","permalink":"/projects/apollo-11/","section":"Projects","summary":"Apollo-11 # 🎌 Bahasa Indonesia, Català, Deutsch, English, Español, Français, Italiano, Nederlands, Polski, Português, Română, Tiếng Việt, Türkçe, Русский, العربية, فارسی, हिंदी, বাংলা, မြန်မာ, 日本, 正體中文, 简体中文, 한국어","title":"Apollo-11"},{"content":"404: Not Found\n","date":"17 February 2020","permalink":"/projects/scouting-scripts/","section":"Projects","summary":"404: Not Found","title":"scouting-scripts"},{"content":" Crypto-Chat # Setup # To get started with cryptochat you first need to have firebase installed To do so just run pip install --user firebase\nUse # The chat will prompt you with Mode de/en: This is asking if you are decrypting a message or encrypting a message Then if you are encrypting it will ask you for your message then a key If you are decrypting it will ask for a key then it will print out all the messages decrypted by your key. This means that it if you decrypt messages with the wrong key they will look line nonsense.\nAfter encrypting the message, the program sends the encrypting message to the firebase database. This will allow anyone else to be able to view the encrypted message in their terminal with the version of the program. This makes it so you can have private group chats using very basic encryption.\n","date":"25 January 2020","permalink":"/projects/crypto-chat/","section":"Projects","summary":"Crypto-Chat # Setup # To get started with cryptochat you first need to have firebase installed To do so just run pip install --user firebase","title":"Crypto-Chat"},{"content":" pwntools - CTF toolkit # Pwntools is a CTF framework and exploit development library. Written in Python, it is designed for rapid prototyping and development, and intended to make exploit writing as simple as possible.\nfrom pwn import * context(arch = \u0026#39;i386\u0026#39;, os = \u0026#39;linux\u0026#39;) r = remote(\u0026#39;exploitme.example.com\u0026#39;, 31337) # EXPLOIT CODE GOES HERE r.send(asm(shellcraft.sh())) r.interactive() Try It Now! # You can now do a live demo of Pwntools, right in your browser.\nDocumentation # Our documentation is available at docs.pwntools.com\nTo get you started, we\u0026rsquo;ve provided some example solutions for past CTF challenges in our write-ups repository.\nInstallation # Pwntools is best supported on 64-bit Ubuntu LTE releases (12.04, 14.04, 16.04 and 18.04). Most functionality should work on any Posix-like distribution (Debian, Arch, FreeBSD, OSX, etc.). Python \u0026gt;= 2.7 is required (Python 3 suggested as best).\nMost of the functionality of pwntools is self-contained and Python-only. You should be able to get running quickly with\napt-get update apt-get install python3 python3-pip python3-dev git libssl-dev libffi-dev build-essential python3 -m pip install --upgrade pip python3 -m pip install --upgrade git+https://github.com/Gallopsled/pwntools.git@dev3 However, some of the features (assembling/disassembling foreign architectures) require non-Python dependencies. For more information, see the complete installation instructions here.\nContribution # See CONTRIBUTING.md\nContact # If you have any questions not worthy of a bug report, feel free to ping us at #pwntools on Freenode and ask away. Click here to connect. There is also a mailing list for higher latency discussion.\n","date":"12 January 2020","permalink":"/projects/pwntools/","section":"Projects","summary":"pwntools - CTF toolkit # Pwntools is a CTF framework and exploit development library.","title":"pwntools"},{"content":" Pogscript # Pogscript, Overtly, Great, Script # For syntax add @@PogScript to the top of the file\nFor each command use \u0026gt; in front\nCommands # v for value \u0026gt;v(2 This sets the value to 2 and writes it in memory to the first open location i.e. 0\no for output \u0026gt;o(0 This outputs memory location 0\ni for input \u0026gt;i(Prompt: This outputs memory location 0\nOperators # a for add; s for subtract m for multiply d for divide p for push For each operator the formate is consistent \u0026gt;a{0{1 This adds memory location 0 and 1 \u0026gt;m{0{1 This multiplies memory locations 0 and 1\n","date":"31 December 2019","permalink":"/projects/pogscript/","section":"Projects","summary":"Pogscript # Pogscript, Overtly, Great, Script # For syntax add @@PogScript to the top of the file","title":"pogscript"},{"content":" Sodium: Vim 2.0 # Sodium is an editor inspired by Vim (but not a clone). It aims to be efficient, fast, and productive.\nLibrary Requirements # Sodium requires the sdl2 library in order to build. To install on Ubuntu, use the following command: sudo apt-get install libsdl2-dev\nBuild # Use cargo run --features orbital in order to build the program.\n","date":"27 December 2019","permalink":"/projects/sodium/","section":"Projects","summary":"Sodium: Vim 2.","title":"sodium"},{"content":"codinggames is a repo dedicated to all (simple) open-source games. Feel free to add your own, and make sure to do bug tests and/or make suggestions for existing games!\n","date":"23 December 2019","permalink":"/projects/codinggames/","section":"Projects","summary":"codinggames is a repo dedicated to all (simple) open-source games.","title":"codinggames"},{"content":"404: Not Found\n","date":"18 December 2019","permalink":"/projects/polybot/","section":"Projects","summary":"404: Not Found","title":"Polybot"},{"content":" Termitebot # Copy and update termite configs.\n","date":"18 December 2019","permalink":"/projects/termitebot/","section":"Projects","summary":"Termitebot # Copy and update termite configs.","title":"Termitebot"},{"content":" tlib # Fork of Adam Hutchings\u0026rsquo; tlib\n","date":"13 December 2019","permalink":"/projects/tlib/","section":"Projects","summary":"tlib # Fork of Adam Hutchings\u0026rsquo; tlib","title":"tlib"},{"content":" RUSHNOTE # Take quick notes from command line when in a rush.\nInstall # git clone https://github.com/JakeRoggenbuck/RushNote.git cd RushNote sudo make \u0026amp;\u0026amp; sudo make install For more information # man rushnote\nSecurity # notes have very open privileges (777) all users can read, write and execute the save file\n","date":"23 November 2019","permalink":"/projects/rushnote/","section":"Projects","summary":"RUSHNOTE # Take quick notes from command line when in a rush.","title":"RushNote"},{"content":"404: Not Found\n","date":"15 November 2019","permalink":"/projects/inflector/","section":"Projects","summary":"404: Not Found","title":"inflector"},{"content":"Repository for 1678 Software General git lesson\n","date":"8 November 2019","permalink":"/projects/git-lesson/","section":"Projects","summary":"Repository for 1678 Software General git lesson","title":"git-lesson"},{"content":" Seamonsters-2605.github.io # Seamonsters team Github Pages site\n","date":"5 November 2019","permalink":"/projects/seamonsters-2605.github.io/","section":"Projects","summary":"Seamonsters-2605.","title":"Seamonsters-2605.github.io"},{"content":" Jake\u0026rsquo;s Auto Rice Bash Script # Configs for # bspwm\nsxhkd\npolybar\ndunst\nnvim\nvim\nrofi\ntermite\nalacritty\nbash aliases\nwallpapers\nand more\n","date":"3 November 2019","permalink":"/projects/jarbs/","section":"Projects","summary":"Jake\u0026rsquo;s Auto Rice Bash Script # Configs for # bspwm","title":"JARBS"},{"content":" LimeOS # Backgrounds images for LimeOS\n","date":"31 October 2019","permalink":"/projects/limeos/","section":"Projects","summary":"LimeOS # Backgrounds images for LimeOS","title":"LimeOS"},{"content":"404: Not Found\n","date":"3 September 2019","permalink":"/projects/skyometer/","section":"Projects","summary":"404: Not Found","title":"Skyometer"},{"content":"404: Not Found\n","date":"25 July 2019","permalink":"/projects/highlight/","section":"Projects","summary":"404: Not Found","title":"Highlight"},{"content":"404: Not Found\n","date":"3 July 2019","permalink":"/projects/gnome-clocks/","section":"Projects","summary":"404: Not Found","title":"gnome-clocks"},{"content":"","date":"3 July 2019","permalink":"/tags/vala/","section":"Tags","summary":"","title":"Vala"},{"content":" gpt-2 # Code from the paper \u0026ldquo;Language Models are Unsupervised Multitask Learners\u0026rdquo;.\nWe have currently released small (117M parameter) and medium (345M parameter) versions of GPT-2. While we have not released the larger models, we have released a dataset for researchers to study their behaviors.\nSee more details in our blog post.\nUsage # This repository is meant to be a starting point for researchers and engineers to experiment with GPT-2.\nSome caveats # GPT-2 models\u0026rsquo; robustness and worst case behaviors are not well-understood. As with any machine-learned model, carefully evaluate GPT-2 for your use case, especially if used without fine-tuning or in safety-critical applications where reliability is important. The dataset our GPT-2 models were trained on contains many texts with biases and factual inaccuracies, and thus GPT-2 models are likely to be biased and inaccurate as well. To avoid having samples mistaken as human-written, we recommend clearly labeling samples as synthetic before wide dissemination. Our models are often incoherent or inaccurate in subtle ways, which takes more than a quick read for a human to notice. Work with us # Please let us know if you’re doing interesting research with or working on applications of GPT-2! We’re especially interested in hearing from and potentially working with those who are studying\nPotential malicious use cases and defenses against them (e.g. the detectability of synthetic text) The extent of problematic content (e.g. bias) being baked into the models and effective mitigations Development # See DEVELOPERS.md\nContributors # See CONTRIBUTORS.md\nCitation # Please use the following bibtex entry:\n@article{radford2019language, title={Language Models are Unsupervised Multitask Learners}, author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya}, year={2019} } Future work # We may release code for evaluating the models on various benchmarks.\nWe are still considering release of the larger models.\nLicense # MIT\n","date":"25 June 2019","permalink":"/projects/gpt-2/","section":"Projects","summary":"gpt-2 # Code from the paper \u0026ldquo;Language Models are Unsupervised Multitask Learners\u0026rdquo;.","title":"gpt-2"},{"content":" spaceapp # Telescope Aiming tool ","date":"24 June 2019","permalink":"/projects/spaceapp/","section":"Projects","summary":" spaceapp # Telescope Aiming tool ","title":"spaceapp"},{"content":" jr0.org # Currently, Hugo to make this static website.\nUsage # Use deploy.sh in the root dir to deploy.\nThe script still works perfectly long after I wrote it!\nHistory # This website has been up since May 26, 2019 in some form or another.\nCurrent # Prior # Prior # Prior # First # ","date":"27 May 2019","permalink":"/projects/jakeroggenbuck.github.io/","section":"Projects","summary":"jr0.","title":"JakeRoggenbuck.github.io"},{"content":" SignInWebApp # ","date":"15 May 2019","permalink":"/projects/signinwebapp/","section":"Projects","summary":" SignInWebApp # ","title":"SignInWebApp"},{"content":" dcbankofcarter # ","date":"3 May 2019","permalink":"/projects/dcbankofcarter/","section":"Projects","summary":" dcbankofcarter # ","title":"dcbankofcarter"},{"content":" Best Next Step # Keep track of where you are on projects and what your Best Next Step is.\nWhy # Do you ever leave a project to focus on school, work, life, or other projects? The constant shifting from one project to another can lead to forgetting where to start up from. This app helps you keep track of your next steps and helps you plan ahead.\nHistory # This repository was created on April 28th in 2019 but never was pushed to and no code was written. It remained empty for over three years. I still have concept art from that time and want to continue this project with the skills I have now.\nCurrent update: Now this project has a full backend and frontend written with Gin Gonic and Svelte\nBackend (Daft) # For the backend, I am using Gin Frontend (Impulse) # For the frontend, I am using svelte Old Concept Design # ","date":"28 April 2019","permalink":"/projects/bestnextstep/","section":"Projects","summary":"Best Next Step # Keep track of where you are on projects and what your Best Next Step is.","title":"BestNextStep"},{"content":" GalapagosReefOctopus # ","date":"19 February 2019","permalink":"/projects/galapagosreefoctopus/","section":"Projects","summary":" GalapagosReefOctopus # ","title":"GalapagosReefOctopus"},{"content":" ChromeWebLauncher # ","date":"20 January 2019","permalink":"/projects/chromeweblauncher/","section":"Projects","summary":" ChromeWebLauncher # ","title":"ChromeWebLauncher"},{"content":" git-training # Git(hub) training for Scouting Programmers\n","date":"2 January 2019","permalink":"/projects/git-training/","section":"Projects","summary":"git-training # Git(hub) training for Scouting Programmers","title":"git-training"},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"1 January 0001","permalink":"/tags/code/","section":"Tags","summary":"","title":"code"},{"content":"","date":"1 January 0001","permalink":"/tags/github/","section":"Tags","summary":"","title":"github"},{"content":"","date":"1 January 0001","permalink":"/tags/jake/","section":"Tags","summary":"","title":"Jake"},{"content":"","date":"1 January 0001","permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"machine learning"},{"content":"","date":"1 January 0001","permalink":"/tags/manager/","section":"Tags","summary":"","title":"manager"},{"content":"","date":"1 January 0001","permalink":"/tags/product/","section":"Tags","summary":"","title":"product"},{"content":"","date":"1 January 0001","permalink":"/tags/projects/","section":"Tags","summary":"","title":"projects"},{"content":"","date":"1 January 0001","permalink":"/tags/roggenbuck/","section":"Tags","summary":"","title":"Roggenbuck"},{"content":"","date":"1 January 0001","permalink":"/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"1 January 0001","permalink":"/tags/software/","section":"Tags","summary":"","title":"software"}]